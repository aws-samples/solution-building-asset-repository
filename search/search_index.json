{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Solution Building Asset Repository","text":"<p>Welcome to the Solution Building Asset Repository! The Solution Building Asset Repository is part of the Solution Building Enablement for AWS Partners and provides a curated collection of technical resources to support solution building on AWS. This includes general and industry-specific assets, accelerators, blueprints, best practices, and reference architectures to guide and accelerate solution build and delivery. Learn more about the available asset categories here..</p> <p>Learn more about Solution Building Enablement (SBE) and the phases of Solution Building Enablement solution development below.</p>"},{"location":"index.html#solution-building-enablement","title":"Solution Building Enablement","text":"<p>Solution Building Enablement is designed to help all AWS Partners build and deliver solutions that address both current and future business outcome needs for customers. It simplifies and expedites delivery of industry-focused solutions on AWS as we continue to see a shift toward business outcome solutions driven by line of business buyers, with customers more likely to purchase solutions with business outcomes that impacts their top-line.</p> <p></p> <p>Our guidance is designed to help accelerate your solution building on AWS, starting with solution innovation and funding options, build assets that support discovery, design, build, and validation, all the way to publishing, go-to-market, and solution lifecycle management. This includes custom resources, such as templates and workshops, to help you document and operationalize the guidance that applies to you as part of your own organization\u2019s solution building on AWS.</p> <p>Solution Building Enablement is made up of four phases of solution development. These include Ideation, Design &amp; Build, GTM &amp; Sales, and Solution Lifecycle Management:</p> <p></p> <p>The Solution Building Asset Repository supports the Design &amp; Build phase by providing technical resources to minimize effort and time spent design and building a solution. The Design &amp; Build phase includes Design, Build, Validation, and Publishing.</p>"},{"location":"index.html#design","title":"Design","text":"<p>The Design phase is structured around translating the high-level customer and solution information into more tactical design decisions and documentation. The goal is to take the human-centered insights from the Ideate phase and transform them into an actionable design artifacts that guide the engineering build-out. It is expected that some elements of the architecture may change based on testing, but this establishing an initial technical strategy is crucial to providing a solid foundation for the development team to work from.</p> <p>Resources for supporting the Design phase, including reference architectures, UX design systems, and more are available in the Solution Building Asset Repository under Horizontal Technologies and Industries.</p>"},{"location":"index.html#build","title":"Build","text":"<p>The goal of the Build phase is to take the design established during the Design phase and transform it into working software leveraging robust engineering principles, testing, and automation to deliver a high-quality solution. The output is a tested, documented, production-ready initial release providing core value to users.</p> <p>Resources for supporting the Build phase, including assets, accelerators, blueprints and more are available in the Solution Building Asset Repository under Horizontal Technologies, Industries, and Tools.</p>"},{"location":"index.html#validation","title":"Validation","text":"<p>Validating your hardware, software, or services offering as an AWS Partner is an important factor in ensuring that a solution and its components meet a quality bar that delivers a consistent customer experience. It also enables additional benefits such as co-sell opportunities, funding options, and automatic listing in AWS Partner Solution Finder. Completion of validations also supports your journey in the AWS Partner Network. </p>"},{"location":"index.html#publishing","title":"Publishing","text":"<p>AWS Partner Solutions Finder provides AWS customers with a centralized place to search, discover, and connect with trusted AWS Partners based on customers\u2019 business needs. Customers can use the AWS Partner Solutions Finder to find partners and their validated offerings spanning from hardware, software, to services. Customers can l also look for partners based on a variety of criteria including industry, use case, location, and product areas. </p> <p>AWS Marketplace allows customers to find third-party software, data, and services that run on AWS and manage from a centralized location. AWS Marketplace includes thousands of software listings and simplifies software licensing and procurement with flexible pricing options and multiple deployment methods. AWS Marketplace allows Independent Software Vendors (ISVs), Data Providers, and Consulting Partners can sell their software, services, and data in AWS Marketplace to anyone with an AWS account can use. Jointly with the AWS Partner Network, AWS Marketplace helps ISVs and Consulting Partners to build, market, and sell their AWS offerings by providing valuable business, technical, and marketing support.</p>"},{"location":"glossary.html","title":"Glossary","text":""},{"location":"glossary.html#assets","title":"Assets","text":"<p>Assets are building blocks for solutions. Builders can use them as a starting point for building a solution or add them to an existing solution as a means of optimization. Assets can be put together to create accelerators but are not a complete solution without the inclusion of additional components.</p>"},{"location":"glossary.html#accelerators","title":"Accelerators","text":"<p>Accelerators are pre-built resources and tools that function end-to-end. Compared to assets, they are much closer to the final product that they might be used to build.</p>"},{"location":"glossary.html#blueprints","title":"Blueprints","text":"<p>Blueprints are used to raise the quality bar of a solution. They may focus on one of the pillars of the well-architected framework or target solution-building within a specific industry or technology field.</p>"},{"location":"glossary.html#solution-guidance","title":"Solution Guidance","text":"<p>Solution Guidance explains how to engage with and progress through the SBE Build Hub from Ideation to Delivery and Go-To-Market.</p>"},{"location":"glossary.html#reference-architecture","title":"Reference Architecture","text":"<p>Reference architecture diagrams provide examples of how solutions across a variety of use cases, industry, and technology fields are built using AWS services and SBE resources. They can be used as inspiration for building a new solution or to assist in improving the quality of an existing solution.</p>"},{"location":"questions.html","title":"Get in touch","text":"<p>For questions, concerns, and feedback on the SBE Build Hub or this website, start a new Discussion on our GitHub repo and someone will get back to you right away.</p>"},{"location":"accelerators/index.html","title":"Accelerators","text":"<p>Accelerators are pre-built resources and tools that function end-to-end. Compared to assets, they are much closer to the final product that they might be used to build.</p> <p>As accelerators are added to the SBE Design &amp; Build Hub, they will appear here with documentation, examples, and tutorials.</p>"},{"location":"accelerators/poc.html","title":"Proofs-of-Concept","text":"<p>Proofs of Concept (POC) are designed to help partners validate their technical architecture and business objective of a SBE solution. POCs provide access to AWS resources, tools, and expertise to accelerate solution building, deployment, and adoption.</p>"},{"location":"accelerators/genai/index.html","title":"GenAI Accelerator","text":"<p>GenAI Assets are building blocks for GenAI specific solutions. Builders can use them as a starting point for building a solution or add them to an existing solution as a means of optimization. Assets can be put together to create accelerators but are not a complete solution without the inclusion of additional components.</p> <p>As assets are added to the SBE Design &amp; Build Hub, they will appear here with documentation, examples, and tutorials.</p>"},{"location":"accelerators/genai/bedrock_karaoke.html","title":"BedRock AI Karaoke","text":"<p>Amazon Bedrock AI Karaoke is an interactive demo of prompting foundation models with a microphone and asking a human to pick their preferred response.</p>"},{"location":"accelerators/genai/bedrock_karaoke.html#architecure","title":"Architecure","text":"<p>Click here to access the library and start designing your solution.</p>"},{"location":"accelerators/genai/bedrock_prompt_examples.html","title":"BedRock Prompt Engineering","text":""},{"location":"accelerators/genai/bedrock_prompt_examples.html#amazon-bedrock-prompting-examples-tools","title":"Amazon Bedrock Prompting Examples &amp; Tools","text":"<p>This repo has a set of prompt engineering examples and prompting tools for working with Amazon Bedrock.</p>"},{"location":"accelerators/genai/bedrock_prompt_examples.html#content","title":"Content:","text":"<p>Notebooks with prompt examples per industry: Example notebooks</p> <p>Notebooks with examples for prompt misuse mitigation and evaluation: Example notebooks</p> <p>Useful tools for working with Generative AI prompts: Bedrock in Excel</p>"},{"location":"accelerators/genai/bedrock_prompt_examples.html#auto-prompting-assistant","title":"Auto-Prompting assistant","text":""},{"location":"accelerators/genai/bedrock_prompt_examples.html#prompt-examples-browser-creator","title":"Prompt Examples Browser &amp; Creator","text":"<p>Keymaker - Prompt translation across LLMs made easy</p> <p>Prompt templates for working Bedrock in LangChain</p> <p>Prompt Templates</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"accelerators/genai/building_with_bedrock_langchain.html","title":"BedRock & LangChain","text":""},{"location":"accelerators/genai/building_with_bedrock_langchain.html#building-with-amazon-bedrock-and-langchain","title":"Building with Amazon Bedrock and LangChain","text":""},{"location":"accelerators/genai/building_with_bedrock_langchain.html#amazon-bedrock","title":"Amazon Bedrock","text":"<p>is a fully managed service for using foundation models. It allows you to access models from Amazon and third parties with a single set of APIs for both text generation and image generation.</p> <p>In this workshop, we will use LangChain to build generative AI prototypes with Amazon Bedrock. We will proceed through a series of labs aimed at different skill levels from beginner to intermediate. We will learn how to use the capabilities of the libraries and models to build prototypes for a wide range of use cases.</p>"},{"location":"accelerators/genai/building_with_bedrock_langchain.html#langchain","title":"LangChain","text":"<p>provides convenient functions for interacting with Amazon Bedrock's models and related services like vector databases. LangChain offers Python and JavaScript libraries. For this workshop, we will use the Python version of LangChain.</p>"},{"location":"accelerators/genai/building_with_bedrock_langchain.html#streamlit","title":"Streamlit","text":"<p>allows us to quickly create web front ends for our Python code, without needing front-end development skills. Streamlit is great for creating proofs-of-concepts that can be presented to a wide audience of both technical and non-technical people.</p> <p>Once you have completed the Prerequisites sections, you can complete the labs in any order.</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"accelerators/genai/qa_with_llm_and_rag.html","title":"Q&A with LLM and RAG","text":""},{"location":"accelerators/genai/qa_with_llm_and_rag.html#qa-with-llm-and-rag","title":"Q&amp;A with LLM and RAG:","text":"<p>A question answer task on a corpus of enterprise specific data is a common use-case in an enterprise scenario. If the data to be used for this task is publicly available then chances are that a pre-trained foundation large language model (LLM) will be able to provide a reasonable response to the question but this approach suffers from the following problems: 1/ the LLM is trained with a point in time snapshot of the data so its response will not be current, 2/ the LLM could hallucinate i.e. provide convincing looking responses that are factually incorrect and 3/ most importantly, the model may never have seen the enterprise specific data and is therefore not able to provide a useful response.</p>"},{"location":"accelerators/genai/qa_with_llm_and_rag.html#architecure","title":"Architecure","text":"<p>Click here to access the library and start designing your solution.</p>"},{"location":"accelerators/genai/rag_using_bedrock.html","title":"RAG using BedRock","text":""},{"location":"accelerators/genai/rag_using_bedrock.html#rag-using-langchain-with-amazon-bedrock-titan-text-and-embedding-using-opensearch-vector-engine","title":"RAG using LangChain with Amazon Bedrock Titan text, and embedding, using OpenSearch vector engine:","text":"<p>This sample repository provides a sample code for using RAG (Retrieval augmented generation) method relaying on Amazon Bedrock Titan Embeddings Generation 1 (G1) LLM (Large Language Model), for creating text embedding that will be stored in Amazon OpenSearch with vector engine support for assisting with the prompt engineering task for more accurate response from LLMs.</p> <p>After we successfully loaded embeddings into OpenSearch, we will then start querying our LLM, by using LangChain. We will ask questions, retrieving similar embedding for a more accurate prompt.</p> <p>You can use --bedrock-model-id parameter, to seamlessly choose one of the available foundation model in Amazon Bedrock, that defaults to Anthropic Claude v2 and can be replaced to any other model from any other model provider to choose your best performing foundation model.</p>"},{"location":"accelerators/genai/rag_using_bedrock.html#anthropic","title":"Anthropic:","text":"<p>Claude v2 python ./ask-bedrock-with-rag.py --ask \"How will AI will change our every day life?\" Claude v1.3 python ./ask-bedrock-with-rag.py --bedrock-model-id anthropic.claude-v1 --ask \"How will AI will change our every day life?\" Claude Instance v1.2 python ./ask-bedrock-with-rag.py --bedrock-model-id anthropic.claude-instant-v1 --ask \"How will AI will change our every day life?\"</p>"},{"location":"accelerators/genai/rag_using_bedrock.html#ai21-labs","title":"AI21 Labs:","text":"<p>Jurassic-2 Ultra python ./ask-bedrock-with-rag.py --bedrock-model-id ai21.j2-ultra-v1 --ask \"How will AI will change our every day life?\" Jurassic-2 Mid python ./ask-bedrock-with-rag.py --bedrock-model-id ai21.j2-mid-v1 --ask \"How will AI will change our every day life?\"</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"accelerators/genai/sagemaker_protein_folding_workflows.html","title":"Sagemaker Protein Folding Workflows","text":""},{"location":"accelerators/genai/sagemaker_protein_folding_workflows.html#protein-folding-workflows-to-accelerate-drug-discovery-on-amazon-sagemaker","title":"Protein folding workflows to accelerate drug discovery on Amazon SageMaker:","text":"<p>Drug development is a complex and long process that involves screening thousands of drug candidates and using computational or experimental methods to evaluate leads. Drug discovery is the research component of this pipeline that generates candidate drugs with the highest likelihood of being effective with the least harm to patients. Machine learning (ML) methods can help identify suitable compounds at each stage in the drug discovery process, resulting in more streamlined drug prioritization and testing, saving billions in drug development costs.</p> <p>Amazon SageMaker is a fully managed service to prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML. It offers a fully-managed environment for ML, abstracting away the infrastructure, data management, and scalability requirements so users can focus on building, training, and testing ML models.</p>"},{"location":"accelerators/genai/sagemaker_protein_folding_workflows.html#architecure","title":"Architecure","text":"<p>Click here to access the library and start designing your solution.</p>"},{"location":"accelerators/iot/index.html","title":"IoT Accelerators","text":"<p>IoT Accelerators are building blocks for IoT-specific solutions. Builders can use them as a starting point for building a solution or add them to an existing solution as a means of optimization. </p> <p>As accelerators are added to the Solution Building Asset Repository, they will appear here with documentation, examples, and tutorials.</p>"},{"location":"accelerators/iot/energy_digitaltwins.html","title":"Energy Monitoring","text":""},{"location":"accelerators/iot/energy_digitaltwins.html#introduction","title":"Introduction:","text":"<p>Energy Monitoring for Digital Twins with AWS IoT is an accelerator consists of a AWS Cloud Formation template which creates and configures the AWS services required to deploy data ingestion, storage and 3D visualization components that are required for a energy monitoring solution in a digital twin. </p>"},{"location":"accelerators/iot/energy_digitaltwins.html#features","title":"Features:","text":"<p>AWS IoT Core ingests the uplink energy meter readings from the field device via MQTT Amazon Timestream stores the timeseries data AWS IoT TwinMaker provides the capability to build 3D model and entity hierarchy of the digital twin and bind the data stored in Timestream to these entities. Amazon Managed Grafana provides a flexible, no-code builder for the user to build a custom dashboard.</p>"},{"location":"accelerators/iot/energy_digitaltwins.html#architecure","title":"Architecure","text":"<p> Click here to access the library and start designing your solution.</p>"},{"location":"accelerators/iot/reference_architecture/IoT-ref-arch.html","title":"IoT Reference Architectures","text":""},{"location":"accelerators/iot/reference_architecture/IoT-ref-arch.html#introduction","title":"Introduction","text":"<p>The repo provides reference architecture diagrams and the code for IoT reference architectures that can be refered to in IoT presentations.</p>"},{"location":"accelerators/iot/reference_architecture/IoT-ref-arch.html#cdk-reference-architecture-list","title":"CDK reference architecture list","text":"<p>These reference architectures use CDK</p>"},{"location":"accelerators/iot/reference_architecture/IoT-ref-arch.html#dynamodb-api","title":"DynamoDB API","text":"<ul> <li>Java</li> </ul> <p>SQS to DynamoDB to IoT Core - An example project that shows how to take messages from SQS, move them DynamoDB, and then query the DynamoDB table with an IoT Core based API. This pattern is useful when multiple applications need access to messages from a device, the messages from a device come through a non-MQTT ingest mechanism and are stored in SQS, or when a device may send multiple messages that need to be processed in order.</p>"},{"location":"accelerators/iot/reference_architecture/IoT-ref-arch.html#binary-payloads","title":"Binary payloads","text":"<ul> <li>Java</li> </ul> <p>CBOR - An example project that shows how to convert between CBOR and JSON. This uses the rules engine base64 encoding support to work with binary payloads in AWS Lambda.</p> <ul> <li>Python</li> </ul> <p>Amazon Ion - An example project that shows how to convert between Amazon Ion and JSON. This uses the rules engine base64 encoding support to work with binary payloads in AWS Lambda.</p>"},{"location":"accelerators/iot/reference_architecture/IoT-ref-arch.html#jwt-authentication-for-aws-iot-core","title":"JWT authentication for AWS IoT Core","text":"<ul> <li>Java</li> </ul> <p>Java custom authentication demo with JWT - A stack that contains a serverless UI that shows how to use custom authentication with JWTs in AWS IoT Core</p> <ul> <li>Cross-account publish</li> </ul> <p>Certificate based stack - A stack that simplifies onboarding an AWS IoT data producer to an account using a certificate to allow cross-account publishing</p>"},{"location":"accelerators/iot/reference_architecture/IoT-ref-arch.html#vending-machine","title":"Vending machine","text":"<ul> <li>Java</li> </ul> <p>Disk image vending machine stack - A stack that has a web UI to create disk images. Currently supports Raspberry Pi devices and can pre-install and provision the AWS Systems Manager agent.</p> <p>Explore reference architecture diagrams for IoT solutions</p>"},{"location":"accelerators/security/edge_accelerator.html","title":"Edge Accelerator","text":""},{"location":"accelerators/security/edge_accelerator.html#aws-stm32-ml-at-edge-accelerator","title":"AWS STM32 ML at Edge Accelerator","text":"<p>This is an AWS STM32 example project that implements MLOps infrastructure using SageMaker pipeline to train and generate an audio classification model that will run on edge devices (stm32u5 series) with OTA updates using Freertos. Devices are connected to Iot Core and data is collected via mqtt.</p>"},{"location":"accelerators/security/edge_accelerator.html#architecure","title":"Architecure","text":"<p>Click here to access the library and start designing your solution.</p>"},{"location":"accelerators/security/secure_enviornment_accelerator.html","title":"Secure Environment Accelerator","text":""},{"location":"accelerators/security/secure_enviornment_accelerator.html#aws-secure-environment-accelerator","title":"AWS Secure Environment Accelerator","text":"<p>The AWS Accelerator is a tool designed to help deploy and operate secure multi-account, multi-region AWS environments on an ongoing basis. The power of the solution is the configuration file that drives the architecture deployed by the tool. This enables extensive flexibility and for the completely automated deployment of a customized architecture within AWS without changing a single line of code.</p> <p>While flexible, the AWS Accelerator is delivered with a sample configuration file which deploys an opinionated and prescriptive architecture designed to help meet the security and operational requirements of many governments around the world. Tuning the parameters within the configuration file allows for the deployment of customized architectures and enables the solution to help meet the multitude of requirements of a broad range of governments and public sector organizations.</p>"},{"location":"accelerators/security/secure_enviornment_accelerator.html#architecure","title":"Architecure","text":"<p>Click here to access the library and start designing your solution.</p>"},{"location":"accelerators/security/siem_on_opensearch.html","title":"SIEM on OpenSearch","text":""},{"location":"accelerators/security/siem_on_opensearch.html#siem-on-amazon-opensearch-service","title":"SIEM on Amazon OpenSearch Service","text":"<p>SIEM on Amazon OpenSearch Service is a solution for collecting multiple types of logs from multiple AWS accounts, correlating and visualizing the logs to help investigate security incidents. Deployment is easily done with the help of AWS CloudFormation or AWS Cloud Development Kit (AWS CDK), taking only about 30 minutes to complete. As soon as AWS services logs are put into a specified Amazon Simple Storage Service (Amazon S3) bucket, a purpose-built AWS Lambda function automatically loads those logs into SIEM on OpenSearch Service, enabling you to view visualized logs in the dashboard and correlate multiple logs to investigate security incidents.</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"accelerators/starter_projects/iot_app_kit.html","title":"IoT Application Kit","text":"<p>IoT Application Kit is a development library for building Industrial IoT web based applications.</p> <p>IoT App Kit is an open-source library consisting of front-end components and utilities. With IoT App Kit, you can build front-end applications and webpages to utilize IoT data. By default, IoT App Kit helps to retrieve data from AWS IoT SiteWise and AWS IoT TwinMaker You can also install plugins to retrieve data from your own sources. There\u2019s no charge for using IoT App Kit.</p> <p>For an example of a real world use case using the IoT App Kit, visit this tutorial on how to use IoT App Kit</p> <p></p>"},{"location":"accelerators/starter_projects/iot_app_kit.html#github-repository","title":"GitHub Repository","text":"<p>GitHub repo: IoT Application Kit</p>"},{"location":"accelerators/starter_projects/webui_starter_kit.html","title":"WebUI Starter Kit","text":"<p>WebUI Starter Kit is a build asset for Solution Building Enablement. It provides modular templates with low-code, configurable files for building web applications connected to AWS services. It is a self-service tool intended to reduce development effort for partners and accelerate completion of the SBE Design &amp; Build Phase.</p>"},{"location":"accelerators/starter_projects/webui_starter_kit.html#github-repository","title":"GitHub Repository","text":"<p>GitHub repo: Coming soon...</p> <p>Live website example: WebUI Starter Kit</p>"},{"location":"accelerators/starter_projects/webui_starter_kit.html#how-it-works","title":"How it works","text":"<p>WebUI Starter Kit leverages the Cloudscape Design System and AWS APIs to generate pre-built, configurable templates for web applications. Users define the application\u2019s structure by choosing from a set of page types (e.g., Homepage, Table view, Card view), and then Starter Kit generates a React application with the pages chosen. For each page, the Starter Kit generates an index file and a configuration file. The configuration file enables users with limited development experience to update static data (e.g., headers) and define API calls for sending and receiving dynamic application data (e.g., database query results). Because the templates are modular, users can include or omit page types according to the needs of the particular application.</p> <p>Starter Kit's ready-made web applications provide clean and consistent branding across SBE solutions, and give clear, prescriptive guidance on how to start developing and delivering product-centric solutions to customers. Because each configuration file defines a clear set of parameters for customization, the templates are accessible for teams with varying levels of technical depth and require minimal time for onboarding and enablement.</p>"},{"location":"accelerators/starter_projects/webui_starter_kit.html#user-flow","title":"User flow","text":"<p>1. User identifies the structure of app they want to build, for example:</p> <ul> <li>Homepage</li> <li>Table View</li> <li>Details View</li> </ul> <p>2. Template is generated with 2 files for each page:</p> <ul> <li><code>Page.jsx</code> \u2013 contains layout and components for the page</li> <li><code>page-config.jsx</code> \u2013 contains customizable configuration for <code>Page.jsx</code></li> </ul> <p>3. User updates <code>page-config.jsx</code> with:</p> <ul> <li>Static data (labels, headers, etc.) <pre><code>&lt;h1&gt; Solution Name &lt;/h1&gt;\n</code></pre></li> <li>Props and attributes (select options, links, etc.)</li> </ul> <pre><code>export const PRICE_CLASS_OPTIONS = [\n\n  { label: 'Use all locations', value: '0' },\n\n  { label: 'Use only US, Canada, and Europe', value: '1' },\n\n  { label: 'Use only US, Canada, Europe, and Asia', value: '2' }\n\n];\n</code></pre> <ul> <li>Dynamic data pulled via APIs</li> </ul> <p>4. User implements additional customizations as needed</p>"},{"location":"accelerators/starter_projects/webui_starter_kit.html#sample-pages","title":"Sample pages","text":""},{"location":"accelerators/starter_projects/webui_starter_kit.html#homepage","title":"Homepage","text":""},{"location":"accelerators/starter_projects/webui_starter_kit.html#table-view","title":"Table View","text":""},{"location":"accelerators/starter_projects/webui_starter_kit.html#details-view","title":"Details View","text":""},{"location":"accelerators/starter_projects/webui_starter_kit.html#cards-view","title":"Cards View","text":""},{"location":"assets/index.html","title":"Assets","text":"<p>Assets are building blocks for solutions. Builders can use them as a starting point for building a solution or add them to an existing solution as a means of optimization. Assets can be put together to create accelerators but are not a complete solution without the inclusion of additional components.</p> <p>As assets are added to the SBE Design &amp; Build Hub, they will appear here with documentation, examples, and tutorials.</p>"},{"location":"assets/code-frameworks/cloudscape.html","title":"Cloudscape Design System","text":"<p>Cloudscape offers user interface guidelines, front-end components, design resources, and development tools for building intuitive, engaging, and inclusive user experiences at scale.</p>"},{"location":"assets/code-frameworks/cloudscape.html#meet-cloudscape","title":"Meet Cloudscape","text":"<p>Cloudscape is an open source design system to create web applications. It was built for and is used by Amazon Web Services (AWS) products and services. We created it in 2016 to improve the user experience across web applications owned by AWS services, and also to help teams implement those applications faster. Since then, we have continued enhancing the system based on customer feedback and research. Learn more about the system.</p> <p></p>"},{"location":"assets/code-frameworks/cloudscape.html#get-familiar-with-the-system","title":"Get familiar with the system","text":"<p>Each component has a playground where designers and developers can see how the component behaves, along with sample code. To save you time and effort when building, we offer extensive guidance on accessibility options and design solutions. Head over to our demos for examples of Cloudscape in action.</p> <p></p> <p>Click here to see live demos, experiment with Cloudscape components, and learn more about the system.</p>"},{"location":"assets/code-frameworks/cloudscape.html#cloudscape-figma-component-library","title":"Cloudscape Figma Component Library","text":"<p>The Cloudscape Figma library consists of global styles and components that can be used to create your designs. The file can be used both as a sticker sheet (to copy and paste elements to your design files) and as a library (which you can publish to access all components from any design file).</p> <p></p> <p>This library enables you to design high-fidelity, interactive wireframes before investing time and resources into the graphical user interface for your solution. By producing wireframes prior to development, you can refine designs, conduct user testing, and validate user experience earlier on, accelerating development and reducing costs.</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"assets/code-frameworks/code-frameworks.html","title":"Code &amp; Frameworks","text":"<p>Code and Frameworks is a curated collection of building blocks for software developers and designers. This comprehensive toolbox includes security scanners, UX frameworks, and other resources to streamline development, boost productivity, enhance safety, and expedite product shipping. With these assets, teams can leverage pre-built components, libraries, and frameworks to save time, increase efficiency, and deliver secure, top-notch software products.</p> <p>As assets are built, they will be added here with complete documentation and examples.</p>"},{"location":"assets/code-frameworks/streamlit.html","title":"Streamlit","text":"<p>Streamlit is an open source app framework for the Python programming language. It helps you create web apps for data science and machine learning in a short time and makes it easy to create and share beautiful web apps. It is compatible with major Python libraries such as scikit-learn, Keras, PyTorch, SymPy(latex), NumPy, pandas, Matplotlib, and more.</p>"},{"location":"assets/code-frameworks/streamlit.html#streamlit-cli","title":"Streamlit CLI","text":"<p>When you install Streamlit, a command-line interface (CLI) tool is installed as well. This tool allows you to run Streamlit apps, change Streamlit configuration options, and assists in diagnosing and fixing issues.</p> <p>You can install streamlit using the following <code>pip</code> command:</p> <p><code>pip install streamlit</code></p>"},{"location":"assets/code-frameworks/streamlit.html#streamlit-starter-app","title":"Streamlit Starter App","text":"<p>You can create an app with Streamlit in just a few steps:</p> <ol> <li>Create a new Python script. Let's call it <code>home.py</code>.</li> <li> <p>Open <code>home.py</code> in your favorite IDE, then import these libraries (i.e., add the import statement)</p> <p><code>import streamlit as st</code> <code>import pandas as pd</code> <code>import numpy as np</code></p> <p><code>st.title(This is my Home)</code></p> </li> <li> <p>Save the file and run using the following command from command line</p> <p><code>streamlit run home.py</code></p> </li> </ol> <p>Here is a sample app mockup:</p> <p></p> <p></p>"},{"location":"assets/genai/index.html","title":"GenAI Assets","text":"<p>Assets are building blocks for solutions. Builders can use them as a starting point for building a solution or add them to an existing solution as a means of optimization. Assets can be put together to create accelerators but are not a complete solution without the inclusion of additional components.</p> <p>As assets are added to the SBE Design &amp; Build Hub, they will appear here with documentation, examples, and tutorials.</p>"},{"location":"assets/genai/deployment/deployllm.html","title":"LLM and Chatbot","text":""},{"location":"assets/genai/deployment/deployllm.html#deploying-a-multi-llm-and-multi-rag-powered-chatbot-using-aws-cdk-on-aws","title":"Deploying a Multi-LLM and Multi-RAG Powered Chatbot Using AWS CDK on AWS","text":"<p>Modular, comprehensive and ready to use solution, this solution provides ready-to-use code so you can start experimenting with a variety of Large Language Models, settings and prompts in your own AWS account.</p>"},{"location":"assets/genai/deployment/deployllm.html#supported-model-providers","title":"Supported model providers","text":""},{"location":"assets/genai/deployment/deployllm.html#amazon-bedrock","title":"Amazon Bedrock","text":""},{"location":"assets/genai/deployment/deployllm.html#amazon-sagemaker","title":"Amazon SageMaker","text":"<pre><code>self-hosted models from Foundation, Jumpstart and HuggingFace.\n</code></pre>"},{"location":"assets/genai/deployment/deployllm.html#third-party","title":"Third-party","text":"<pre><code>providers via API such as Anthropic, Cohere, AI21 Labs, OpenAI, etc. See available langchain integrations for a comprehensive list.\n</code></pre> <p>Click here to access the asset and source code.</p>"},{"location":"assets/iot/index.html","title":"IoT Assets","text":"<p>IoT assets  are building blocks for IoT-specific solutions. Builders can use them as a starting point for building a solution or add them to an existing solution as a means of optimization. </p> <p>As assets are added to the Solution Building Asset Repository, they will appear here with documentation, examples, and tutorials.</p>"},{"location":"assets/iot/device_provisioning.html","title":"Device Provisioning","text":""},{"location":"assets/iot/device_provisioning.html#introduction","title":"Introduction:","text":"<p>AWS IoT Device provisioning deep dive series provides different ways IoT devices can be provisioned for AWS IoT Core. This is an educational project, and the code samples and libraries should not be applied to a production environment without the appropriate development and understanding.</p>"},{"location":"assets/iot/device_provisioning.html#features","title":"Features:","text":"<p>Developing and manufacturing IoT device at scale comes with a multitude of challenges, being one of them, provision the devices with the necessary authentication elements. In AWS IoT Core authentication is handled by customers, following our shared responsibility model. In order to help customers to accomplish provisioning in the most efficient and secure way for their use case, AWS IoT Core supports a number of provisioning methods using X.509 certificates, those are: Just in time provisioning(JITP) Just in time registration(JITR) Multi-account registration Fleet provisioning - by trusted user Fleet provisioning - by claim Bulk registration Single thing provisioning Each one of those methods will be suitable to a specific use case, which is usually influenced by the device manufacturing supply chain. In the Device manufacturing and Provisioning with x.509 Certificates in AWS IoT Core white paper, you can learn more about how supply chains can influence you device provisioning strategy.</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"assets/iot/device_provisioning_mt.html","title":"Multi-tenancy Provisioning","text":""},{"location":"assets/iot/device_provisioning_mt.html#introduction","title":"Introduction:","text":"<p>This solution provides a method for QR code onboarding of devices to AWS IoT Core. It simplifies the provisioning and onboarding process of devices by removing the requirement that an end cloud account/region is known at the time of device provisioning in the factory such as in the use case for a multi-tenant IoT deployment.</p>"},{"location":"assets/iot/device_provisioning_mt.html#architecure","title":"Architecure","text":"<p>Click here to access the library and start designing your solution.</p>"},{"location":"assets/iot/real-time_dashboard.html","title":"Real-Time Dashboard","text":""},{"location":"assets/iot/real-time_dashboard.html#realtime-iot-dashboard-with-aws-appsync-and-amazon-location-service","title":"Realtime IoT Dashboard with AWS AppSync and Amazon Location Service","text":""},{"location":"assets/iot/real-time_dashboard.html#introduction","title":"Introduction:","text":"<p>This application demonstrates a web application dashboard receiving real-time updates from a series of IoT sensors. It depicts a fictitious set of pH sensors deployed around the San Francisco Bay. The solution is built with React, AWS AppSync, Amazon Location Service, AWS Amplify, and AWS IoT technologies.</p>"},{"location":"assets/iot/real-time_dashboard.html#architecure","title":"Architecure","text":"<p>Click here to access the library and start designing your solution.</p>"},{"location":"assets/security/automated_forensic_orchestrator.html","title":"Automated Forensics Orchestrator for Amazon EC2","text":"<p>The Automated Forensics Orchestrator for Amazon EC2 solution deploys a mechanism that uses AWS services to orchestrate and automate key digital forensics processes and activities for Amazon EC2 instances in the event of a potential security issue being detected.</p> <p>It helps to establish an automated workflow across data acquisition from disk and memory,  instance isolation, and invocation of third party forensics investigation, analysis, and reporting tools that can be easily integrated with the solution. The solution is intended to be used by organizations deploying and running workloads on EC2 instances, specifically aimed at helping their security operations and response functions.</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"assets/security/automated_security_response.html","title":"Automated Security Response on AWS","text":"<p>This AWS Solution is an add-on that works with AWS Security Hub and provides predefined response and remediation actions based on industry compliance standards and best practices for security threats. It helps Security Hub customers to resolve common security findings and to improve their security posture in AWS.</p> <p>This AWS Solution creates playbooks for customers to individually choose what they want to deploy in their Security Hub admin account. Each playbook contains the necessary actions to start the remediation workflow within the admin account or any member account.</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"assets/security/aws_incident_response_playbook.html","title":"AWS Incident Response Playbook","text":"<p>These playbooks are created to be used as templates only. They should be customized by administrators working with AWS to suit their particular needs, risks, available tools and work processes. These guides are not official AWS documentation and are provided as-is to customers using AWS products and who are looking to improve their incident response capability.</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"assets/security/container_runtime_security.html","title":"Guidance for Container Runtime Security with Amazon Inspector","text":"<p>This Guidance helps you safeguard and protect your cloud environment by monitoring the security posture of running containers. As a security best practice, you should scan container images for security vulnerabilities, such as common vulnerability exposures (CVEs), before deploying them on container orchestration platforms. This Guidance helps you detect vulnerabilities, such as containers that were initially deployed without proper scanning or containers using expired container images. You can then configure customized actions against such findings to remediate issues.</p> <p>Click here to access the library and start designing your solution. </p>"},{"location":"assets/security/dynamic_object_and_rule_extension_for_aws_network_firewall.html","title":"Dynamic Object and Rule Extensions for AWS Network Firewall","text":"<p>The Dynamic Object and Rule Extensions for AWS Network Firewall solution provides a mechanism to specify elastic and dynamic cloud resources as objects that can be easily referenced within AWS Network Firewall rules. The solution automatically synchronizes such rules to the current state of the referenced AWS resources, as they are scaled in or out, and updated. </p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"assets/security/security_automation_for_aws_waf.html","title":"Security Automations for AWS WAF","text":"<p>This solution automatically deploys a set of AWS WAF (web application firewall) rules that filter common web-based attacks. Users can select from preconfigured protective features that define the rules included in an AWS WAF web access control list (web ACL). Once deployed, AWS WAF protects your Amazon CloudFront distributions or Application Load Balancers by inspecting web requests.</p> <p>You can use AWS WAF to create custom, application-specific rules that block attack patterns to ensure application availability, secure resources, and prevent excessive resource consumption.</p> <p>The Security Automations for AWS WAF solution supports the latest version of AWS WAF (AWS WAFV2) service API.</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"assets/security/sensitive_information_scanning.html","title":"Guidance for Sensitive Information Scanning with Amazon Macie on AWS","text":"<p>This Guidance demonstrates how customer applications can scan artifacts for Personally Identifiable Information (PII), financial information or credentials, and other sensitive information with Amazon Macie.</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"blueprints/index.html","title":"Blueprints","text":"<p>Blueprint is pre-built, pre-validated implementation guide for building solutions. The customers aren't necessarily aware of a Blueprint used in a solution. The solutions implementing the Blueprints are either enhanced or modified to meet one or more Blueprints' implementation guidelines. Blueprints are not high-level reference architectures. Blueprints are tested against their expected outcomes. The output of a Blueprint could be an implementation guide or a deployable solution to enhance an existing solution.</p> <p>As blueprints are added to the SBE Design &amp; Build Hub, they will appear here with documentation, examples, and tutorials.</p>"},{"location":"blueprints/mvp.html","title":"Blueprint in Development","text":"<p>This iteration of the Build Hub site is an MVP - this blueprint will be added as it is published.</p>"},{"location":"blueprints/psf-index.html","title":"SBE Blueprints","text":"<p>Solution Building Enablement (SBE) Blueprints are a bar raising implementation guide to enhance the existing solution with a specific functionality. SBE Blueprint when implemented, it enhances the solution to meet a set of requirements at production scale. The SBE Build Hub team collaborates with the specializations like Data, AI &amp; ML and various other teams to create Blueprints. Partners building SBE solutions would implement the SBE Blueprints in their solution building process. Any solution building engagement that is following SBE can use one or more SBE Blueprints in their solution building journey. The output of a SBE Blueprint could include deployment templates built in CFN, terraform, etc or it could be a detailed implementation guideline that is pre-validate.</p>"},{"location":"blueprints/data_ai/data_ai_blueprints.html","title":"AWS Industry Blueprints for Data &amp; AI Blueprints","text":"<p>AWS Industry Blueprints for Data &amp; AI offers a collection of building <code>components</code>, including CDK code modules and solution accelerators to facilitate the configuration and deployment of tailored components for various industry verticals' data-to-insights needs. Users can swiftly set up foundational data ingestion pipeline or choose customized add-on components for specific use cases. Additionally, the Blueprints incorporate AWS technology partner ecosystem products with proprietary capabilities to meet specific requirements. This project simplifies the process of building industry solutions by providing pattern-based architectural definitions that developers can easily leverage.</p> <p>The add-on components provided by Industry Blueprints for Data &amp; AI are designed for industry-specific use cases, such as Churn Prediction, Campaign Management, and ID Resolution, while also addressing common tasks like Deploying BI Dashboards and Data Governance. These components can be easily combined with minimal adjustments to create comprehensive industry solutions, complete with business logic. Each component is pluggable yet generic enough to be reused for specific vertical solutions. The Blueprints components encapsulate various AWS Data &amp; AI services, including Amazon AppFlow, EMR, [Kinesis]https://aws.amazon.com/kinesis/, RDS, Redshift, DynamoDB, Glue, Athena, as well as purpose-built services for specific industries like Amazon HealthLake, [Amazon Security Lake]https://aws.amazon.com/security-lake/, AWS Supply Chain and etc. AWS Solutions Constructs serve as building blocks for the CDK code modules, offering well-architected patterns that allow developers to define solutions in code, resulting in predictable and repeatable infrastructure.</p>"},{"location":"blueprints/data_ai/data_ai_blueprints.html#why-use-aws-industry-blueprints-for-data-ai","title":"Why use AWS Industry Blueprints for Data &amp; AI?","text":"<p>The following diagram presented illustrates the complete process of Turn Data into Insights, which begins at the bottom and culminates at the top.</p> <p></p> <p>Regardless of the industry, the initial stage of the customers' data journey is similar. Data is gathered from multiple sources and funneled into the AWS Lake House. Subsequently, the data is processed and utilized according to specific use cases.</p> <p>As part of the Industry Blueprints for Data &amp; AI framework, we aim to break down this data journey into two main components categories: a foundational data ingestion pipeline and specialized use case modules that are tailored to each industry's unique needs. These modules concentrate on processing and utilizing data in industry-specific ways, adding a distinct flavor to the process. As we can see from the righthand side of the diagram, the framework also encompasses technology partner ecosystem products, which may provide customers with exclusive capabilities. As depicted on the top right side, AWS industry-specific services, built with a specific purpose in mind, can act as catalysts for accelerating solution building. On the bottom left are our partners who offer industry-specific solutions built on AWS data and analytics services.</p> <p>Click here to access the Quick Start.</p>"},{"location":"blueprints/databus/index.html","title":"Index","text":"<p>AWS | Solution Building Enablement | Blueprint - DataBus</p>"},{"location":"blueprints/databus/index.html#databus-blueprint","title":"DataBus Blueprint","text":"<p>DataBus Blueprint is a bar raising implementation guide for solutions that intends to implement data exchange pattern across multiple applications and/or services built in AWS cloud.</p>"},{"location":"blueprints/databus/conclusion.html","title":"Conclusion","text":""},{"location":"blueprints/databus/conclusion.html#conclusion","title":"Conclusion","text":"<p>By implementing DataBus Blueprint in the solution, the builders can avoid the undifferentiated glue code that are developed to integrate two different services or solutions in order to share the data and process in real-time. The efforts involved in R&amp;D and solution approaches evaluation can be avoided and can build solutions faster.</p> <p>Document Revisions</p> <p>Date Autor Change</p> <p>Mar       sanjevir@        Initial Release   2023 </p> <p>Notices</p> <p>Standard AWS service usage legal policy applies. There are no additional legal or licensing agreements required.</p>"},{"location":"blueprints/databus/cost.html","title":"POC Cost","text":"<p>Cost</p> <p>The cost details include only the DataBus costs and not the source and target cost as they could vary depending on the solution implementation. Also, the cost details do not cover any data transfer costs that could occur in a cross-region integration.</p> <p>The EventBridge Pipe cost includes only for the messages that are filtered and delivered to the pipe flow. The cost can be further optimized by designing the batch of events per request. Each 64KB chunk of payload is billed as one request. USD \\$0.40 is billed for every one million requests entering a pipe after filtering.</p> <p>POC Cost:</p> <p>Assuming 1000 water meters connected to the Digital Utility IoT platform and 100 meters failed to report the reads. The filtered meter events that enter the missing read pipe would be 100 per day.</p> <p>Implementation   Number of   Filtered   Filtered     Events   Monthly   Phase            Meter         Meter Events Meter Events   Per        Cost in                      Events      per Day      per Month    Batch    USD                                    (10%) </p> <p>POC                1000          100          3000           1          \\$0.0012</p> <p>Pilot              100K          10K          300K           1          \\$0.12</p> <p>Production         10M           100K         3M             1          \\$1.20</p> <p>Software licenses</p> <p>There are no third-party software or tools used in this Blueprint that would result in any software licenses.</p>"},{"location":"blueprints/databus/fanout.html","title":"Fanout","text":""},{"location":"blueprints/databus/fanout.html#organizational-constraints-and-concept-of-solution-fanout","title":"Organizational Constraints and Concept of Solution -- Fanout","text":"<p>A water utility organization has built a Digital Utility IoT platform in AWS cloud. The IoT platform process the telemetry data and generates water meter events. The water meter events need to be acted up by downstream services with different functional requirements and at a different level of priorities. There are alarm events that needs to be processed immediately, like tamper alarm in which the water meter wakes up the communication module and reports the alarm in real-time. There are non-real time alarms that needs to processed to for aggregation and reporting purposes. Events like negative water meter reads needs to be processed and validated by the billing services. There needs to be a solution that filters the water meter events and applies transformation for each type of events and integrates with its target service for downstream processing of the meter events data.</p>"},{"location":"blueprints/databus/fanout.html#business-architecture","title":"Business Architecture","text":"<p>Digital Utility IoT platform generates meter events. The meter events are streamed in the real-time and these data streams acts as a data source for the DataBus. The DataBus consumes the meter events and filters the events separately for each event like High Pressure Alarm, Empty Pipe Alarm, Tamper Alarm and Negative Water Meter Reads. The filtered events are then applied with transformations independently. The non-real-time events are gathered for the common target destination, the Alarm Processor service. The Tamper Alarm events are sent to the Billing Validator service after the transformation.</p> <p></p>"},{"location":"blueprints/databus/fanout.html#technology-architecture","title":"Technology Architecture","text":"<p>DataBus -- EventBridge Pipes are implemented as the DataBus that establishes a fan out from source to a single consumer integration for sharing data between the data source and the data target.</p> <p>DataBus Source -- The Digital Utility platform generates the meter events and publishes to Kinesis Data Streams which is the data source for the DataBus (EventBridge Pipe).</p> <p>DataBus Target -- The filtered and transformed data are sent to the API Destination which is the target of the DataBus. The API destination initiates the API request to its respective target services for the transformed events.</p> <p></p>"},{"location":"blueprints/databus/overview.html","title":"Blueprint Overview","text":""},{"location":"blueprints/databus/overview.html#solution-overview","title":"Solution Overview","text":"<p>DataBus is an implementation guideline for solutions in which data are shared by treating the data as events. The sharing mechanism is based on the ability to observe events rather than being directed commands.</p>"},{"location":"blueprints/databus/overview.html#solution-definition","title":"Solution Definition","text":"<p>DataBus shares data asynchronously between the data producers and consumers with an ability to share data to the predefined target consumers by filtering the data based on rule definition. DataBus supports the JSON format for data sharing. The DataBus attempts to retry the data delivery if the target consumer service is down and can scale horizontally to delivery millions of data events.</p>"},{"location":"blueprints/databus/overview.html#component-diagrams","title":"Component diagrams","text":"<p>The following diagram explains the solution components in the DataBus BP. Multiple solutions deployed in AWS cloud can either produce the data or consume the data. The data pipes establish the data inbound paths from the producer and the target destination path to the consumer based on the configurations. A same solution/service can be a prosumer, that is both producer and consumer. The filter rules determine the data events that matches the criteria to be processed and sent to a particular destination service/solution, thereby avoiding the additional compute need for filtering the messages. The enrichment allows the optional data enrichment from another AWS service. The DataBus also supports the data transformation before sending the data to the pre-defined target destination.</p> <p></p>"},{"location":"blueprints/databus/overview.html#reference-architecture-diagrams","title":"Reference architecture diagrams:","text":""},{"location":"blueprints/databus/overview.html#point-to-point-integration","title":"Point-to-Point Integration:","text":"<p>Solutions A, B, D are deployed within AWS cloud and Solution C could run outside AWS in a different cloud or on-premises. The following reference architecture explains how DataBus architecture can be implemented to achieve the consistent data sharing pattern between multiple solutions.</p> <p></p> <p>The DataBus pipes establishes point-to-point integration between data producers and data consumers. The data source produces the data and that can be filtered to match specific event to flow into the pipe. There is an optional enrichment step that can be used to enrich or transform the data before they reach the pre-defined targets. With DataBus, the solution that produces data for one pipe could be a data consumer of different pipe by implementing one of the supported target services in the solution.</p> <p>For the detailed implementation of the Point-to-Point integration with a specific use case, please refer here.</p> <p>Fanout From Single Data Source:</p> <p>In this architecture, the Solution A that supports multiple consumers acts as the data source and multiple EventBridge Pipes are created to independently filter and transform the data. Each Pipe can have its own target. Each Pipe is responsible for the transformations based on the specific filter and the data is delivered to its own target. Multiple Pipes can have a single target to achieve a scatter-gather pattern. In this reference architecture, the data from the source Solution A is scattered to Pipe-2 and Pipe-3 to apply different transformations before the data is gathered in Solution C.</p> <p></p> <p>For the detailed implementation of the fanout pattern, with a specific use case, please refer here.</p>"},{"location":"blueprints/databus/overview.html#research-methodology","title":"Research Methodology:","text":"<p>The DataBus BP is implemented using EventBridge Pipes that supports point-to-point integration. The DataBus can also be used to support multiple consumers for the same event stream. DataBus can be used for use cases that needs a fanout of events from a single source to multiple consumers. In fan out pattern, the DataBus bridges the single data source with multiple consumers by creating multiple pipes that can transform and enrich data without need for additional compute services.</p> <p>EventBridge Pipes supports batching from the source and to the targets that support it. Every event or batch of events received by the pipe that travel to an enrichment or target is a pipe execution. There can be up to 1000 pipes per account and region. All pipes in an account and region are limited to 1000 concurrent executions. Pipe executions are limited to a maximum of 5 minutes including the enrichment and target processing.</p> <p>EventBridge Pipes support optional input transformation when passing data to the enrichment and the target. The enrichment can be used to meet the needs of target service data requirements.</p>"},{"location":"blueprints/databus/overview.html#supported-sources","title":"Supported Sources:","text":"<ol> <li> <p>Amazon DynamoDB stream.</p> </li> <li> <p>Amazon Kinesis stream.</p> </li> <li> <p>Amazon MQ broker.</p> </li> <li> <p>Amazon MSK stream.</p> </li> <li> <p>Self-managed Apache Kafka stream.</p> </li> <li> <p>Amazon SQS queue.</p> </li> </ol> <p>For complete and updated list, please check the documentation.</p>"},{"location":"blueprints/databus/overview.html#supported-targets","title":"Supported Targets:","text":"<ol> <li> <p>API destination</p> </li> <li> <p>API Gateway</p> </li> <li> <p>Batch job queue</p> </li> <li> <p>CloudWatch log group</p> </li> <li> <p>ECS task</p> </li> <li> <p>Event bus in the same account and Region</p> </li> <li> <p>Firehose delivery stream</p> </li> <li> <p>Inspector assessment template</p> </li> <li> <p>Kinesis stream</p> </li> <li> <p>Lambda function (SYNC or ASYNC)</p> </li> <li> <p>Redshift cluster data API queries</p> </li> <li> <p>SageMaker Pipeline</p> </li> <li> <p>SNS topic</p> </li> <li> <p>SQS queue</p> </li> <li> <p>Step Functions state machine</p> </li> </ol> <p>i.  Express workflows (SYNC or ASYNC)</p> <p>ii. Standard workflows (ASYNC)</p> <p>For complete and updated list, please check the documentation.</p>"},{"location":"blueprints/databus/p2p.html","title":"Point to Point","text":""},{"location":"blueprints/databus/p2p.html#organizational-constraints-and-concept-of-solution","title":"Organizational Constraints and Concept of Solution","text":"<p>A water utility organization has built a Digital Utility IoT platform in AWS cloud. The water meters are managed by a Telco provided NB-IoT platform that is built in the Telco's private cloud. The Digital Utility IoT platform running in AWS cloud receives the water meter reads data through the telemetry data ingestion from the NB-IoT platform. The Digital Utility IoT platform process the data and derives insights and generates Meter Events based on the processed meter data. One of such meter events is, 'missing reads' where one or few of meters has failed to report the water meter reads for that day. For this use case, the NB-IoT platform has an on-demand read APIs where the cloud platforms can initiate an API request get the water meter reads for specific set of meters for the specified date range. The Digital utility IoT platform should extend the solution by building a scalable service that can identify the missing read events, create the API request payload and initiate the API request to the NB-IoT platform.</p>"},{"location":"blueprints/databus/p2p.html#_1","title":"Point to Point","text":""},{"location":"blueprints/databus/p2p.html#business-architecture","title":"Business Architecture","text":"<p>Digital Utility produces meter events and publishes to the meter events queue. The DataBus takes the meter events queue as the data source and applies filtering rules to filter only the 'missing reads' events. The missing reads event data JSON is transformed into the payload format of the on-demand read API of the NB-IoT platform. The DataBus is configured with the API destination to target the API URL of NB-IoT platform.</p> <p></p>"},{"location":"blueprints/databus/p2p.html#_2","title":"Point to Point","text":""},{"location":"blueprints/databus/p2p.html#technology-architecture","title":"Technology Architecture","text":"<p>DataBus -- EventBridge Pipes are implemented as the DataBus that establishes a point-to-point integration for sharing data between the data source and the data target.</p> <p>DataBus Source -- The Digital Utility platform generates the meter events and publishes to SQS which is the data source for the DataBus (EventBridge Pipe).</p> <p>DataBus Target -- The filtered and transformed data are sent to the API Destination which is the target of the DataBus. The API destination initiates the API request for the on-demand meter read API in the NB-IoT platform.</p> <p></p>"},{"location":"blueprints/databus/p2p.html#implementation-considerations","title":"Implementation considerations","text":"<p>Deployment options</p> <p>The DataBus BP can be implemented by following the step-by-step deployment instruction in this section.</p> <p>Before you deploy, review the cost, architecture, security, and other considerations discussed in this guide. Follow the step-by-step instructions in this section to configure and deploy into your account.</p> <p>Prerequisites</p> <p>The source and target for the DataBus has to be created before proceeding with the DataBus implementation.</p> <p>Deployment Overview</p> <p>The deployment creates the DataBus component, that is EventBridge Pipe and configures the source and target for the point-to-point integration.</p> <p>AWS CloudFormation template (outputs)</p> <p>NA\u00a0</p> <p>Deployment Instructions</p> <p>Go to EventBridge console and create a new EventBridge Pipe.</p> <p></p> <p>Select the DataBus source. In this implementation, the SQS is chosen as source.</p> <p></p> <p>Add the filtering as per the solution need. In this implementation, the meter events messages from SQS are filtered to process only the missing read meter events.</p> <p></p> <p>Add the optional Enrichment if required. In this implementation, the enrichment is not required. Proceed to creating the Target. This implementation has API destination as a target, which is configured with a connection to NB-IoT platform APIs.</p>"},{"location":"blueprints/databus/p2p.html#_3","title":"Point to Point","text":"<p>Add the transformer to convert the incoming message and verify whether the output is in the required format for the target. In this implementation, the meter IDs are filtered form the incoming message along with the unique message ID and a new JSON object with required keys are created as a payload body to the NB-IoT platform API.</p> <p></p> <p>Complete the EventBridge Pipe creation and test the end-to-to functionality to confirm the point-to-point integration between the DataBus source and the DataBus target are working fine as expected.</p>"},{"location":"blueprints/databus/prerequisites.html","title":"Prerequisites","text":"<p>DataBus BP needs the following prerequisites to be met.</p> <ol> <li> <p>The DataBus data source that ingests the data streams into the     DataBus.</p> </li> <li> <p>The DataBus data target that consumes the data processed by the     DataBus.</p> </li> <li> <p>The source and target must be one of the of the DataBus supported     sources and targets. Please refer supported sources and     targets.</p> </li> </ol>"},{"location":"blueprints/databus/supported-regions.html","title":"Supported Regions","text":""},{"location":"blueprints/databus/supported-regions.html#supported-aws-regions","title":"Supported AWS Regions:","text":"<p>Following are the regions in which EventBridge Pipe is available:</p>"},{"location":"blueprints/eks/eks_blueprints.html","title":"Amazon EKS Blueprints","text":"<p>EKS Blueprints helps you compose complete EKS clusters that are fully bootstrapped with the operational software that is needed to deploy and operate workloads. With EKS Blueprints, you describe the configuration for the desired state of your EKS environment, such as the control plane, worker nodes, and Kubernetes add-ons, as an IaC blueprint. Once a blueprint is configured, you can use it to stamp out consistent environments across multiple AWS accounts and Regions using continuous deployment automation.</p> <p>You can use EKS Blueprints to easily bootstrap an EKS cluster with Amazon EKS add-ons as well as a wide range of popular open-source add-ons, including Prometheus, Karpenter, Nginx, Traefik, AWS Load Balancer Controller, Fluent Bit, Keda, ArgoCD, and more. EKS Blueprints also helps you implement relevant security controls needed to operate workloads from multiple teams in the same cluster.</p>"},{"location":"blueprints/eks/eks_blueprints.html#what-can-i-do-with-this-blueprint","title":"What can I do with this Blueprint?","text":"<p>Use this Quick Start to easily architect and deploy a multi-tenant Blueprints built on EKS. Specifically, you can leverage the <code>eks-blueprints</code> module to:</p> <ul> <li> Deploy Well-Architected EKS clusters across any number of accounts and regions.</li> <li> Manage cluster configuration, including add-ons that run in each cluster, from a single Git repository.</li> <li> Define teams, namespaces, and their associated access permissions for your clusters.</li> <li> Create Continuous Delivery (CD) pipelines that are responsible for deploying your infrastructure.</li> <li> Leverage GitOps-based workflows for onboarding and managing workloads for your teams. </li> </ul> <p>Click here to access the Quick Start.</p>"},{"location":"blueprints/high_availability/index.html","title":"High Availability Blueprint","text":"<p>High availability is the ability of an application or service to remain accessible and operational without interruption for extended periods of time, achieved through features like regions, availability zones, and automated scaling and recovery mechanisms. It ensures that customers can rely on continuous and reliable access to their resources, minimizing downtime and maximizing uptime.</p>"},{"location":"blueprints/resiliency/index.html","title":"Resiliency Blueprint","text":"<p>Resiliency is the ability of a system to recover quickly and maintain operation in the face of disruptions or failures. SBE solutions are designed to automatically detect and mitigate issues, enabling applications to continue functioning with minimal downtime or impact.</p>"},{"location":"blueprints/scalability/index.html","title":"Scalability Blueprint","text":"<p>Scalability is the ability for systems \u2013 such as applications, storage, databases and networking \u2013 to continue to function properly when changed in size or volume or number of concurrent users. It often refers to increasing or decreasing resources as needed to meet the higher or lower demands of a business.</p>"},{"location":"blueprints/scalability/index.html#component-diagrams","title":"Component diagrams","text":"<p>Global Infrastructure includes regions and Availability Zones that should be considered as part of the services selection. This global footprint gives you the ability to deploy near your customers and migrate your workload as needed.</p> <p>Choose regions based on several criteria:</p> <ol> <li>Proximity to end user</li> <li>Data regulatory requirements</li> <li>Expansion strategy</li> <li>Cost</li> <li>Service availability</li> </ol> <p></p> <p>Horizontal scaling at the application layer, database layer, DB Read Replicas will allow system to scale pretty far. But for serving over ten thousand users, we highly recommend decouple the architecture.</p> <p>The AWS platform offers services in many flavors so that you can provision just enough performance, availability, and durability depending on your requirements. Your ability to be aware and leverage the entire palette of AWS services will be key to adding scalability to your application.</p>"},{"location":"blueprints/scalability/IoT-scaling.html","title":"IoT scaling","text":"<p>IoT Services Scaling (\\&lt;limits&gt;)</p> <p>AWS IoT Core</p> <p>AWS IoT Core can communicate with connected devices securely, with low latency and with low overhead. The communication can scale to as many devices as you want. AWS IoT Core supports standard communication protocols (HTTP, MQTT, and WebSockets and LoRaWAN are supported currently). Communication is secured using TLS.</p> <p>Device Gateway</p> <p>The Device Gateway forms the backbone of communication between connected devices and the cloud capabilities such as the Rules Engine, Device Shadow, and other AWS and 3rd-party services.</p> <p>AWS IoT FleetWise</p> <p>AWS IoT SiteWise</p> <p>AWS IoT TwinMaker</p> <p>Auto Scaling</p>"},{"location":"blueprints/scalability/application-scaling.html","title":"Application Scaling","text":"<p>The AWS Serverless platform allows you to scale very quickly in response to demand. Below is an example of a serverless design that is fully synchronous throughout the application. During periods of extremely high demand, Amazon API Gateway and AWS Lambda will scale in response to your incoming load. This design places extremely high load on your backend relational database because Lambda can easily scale from thousands to tens of thousands of concurrent requests.</p> <p>Consider decoupling your architecture and moving to an asynchronous model. In this architecture, you use an intermediary service to buffer incoming requests, such as Amazon Kinesis or Amazon Simple Queue Service (SQS). You can configure Kinesis or SQS as out of the box event sources for Lambda. In design below, AWS will automatically poll your Kinesis stream or SQS resource for new records and deliver them to your Lambda functions. You can control the batch size per delivery and further place throttles on a per Lambda function basis.</p>"},{"location":"blueprints/scalability/application-scaling.html#auto-scaling","title":"Auto Scaling","text":"<p>After you shift workload components to the appropriate AWS services and decouple your application, you can introduce Auto Scaling to squeeze more efficiency out of your infrastructure. \\&lt;link to reference doc&gt;</p> <p>Reference architecture diagrams:</p> <p>The reference diagram below illustrates setting up EC2 auto scaling instances</p> <p></p>"},{"location":"blueprints/scalability/database-scaling.html","title":"Database Scaling","text":"<p>Vertical scaling</p> <p>Vertical scaling is the most straightforward approach to adding more capacity in your database. Vertical scaling is suitable if you can't change your application and database connectivity configuration. You can vertically scale up your RDS instance with a click of a button. Several instance sizes are available, from general purpose to CPU and memory optimized, when resizing in Amazon RDS for MySQL, Amazon RDS for PostgreSQL, Amazon RDS for Maria DB, Amazon RDS for Oracle, or Amazon RDS for SQL Server. Instance types have combinations of CPU, memory, storage, and networking capacity, and give you the flexibility to choose the appropriate mix of resources for your database. In addition, each instance type includes several instance sizes, which allows you to scale your database to the requirements of your target workload.</p> <p>Horizontal scaling</p> <p>Horizontal scaling increases performance by extending the database operations to additional nodes. You can choose this option if you need to scale beyond the capacity of a single DB instance. An advantage of horizontally scaling in Amazon RDS is that AWS handles the infrastructure management, provisioning, and configuration of additional nodes. You can easily create additional nodes from the Amazon RDS console or API.</p> <p>To scale your read operations, you horizontally scale your database through read replicas. When you create a read replica, Amazon RDS creates read-only copies of your database and manages the asynchronous replication from the primary database. Amazon RDS DB engines such as MySQL, MariaDB, Oracle, PostgreSQL, and SQL Server all have the read replica feature.</p> <p></p> <p>You can use read replicas to increase performance for read options, for example redirecting the read traffic for business reporting and read queries from applications. Another use case is for disaster recovery\u2014you can promote the read replica to the primary database if it becomes unavailable. However, it's important to note that read replicas are not a replacement for the high availability and automatic failover capabilities that Multi-AZ provides.</p> <p>Amazon RDS</p> <p>Add redundancy by using the multi-AZ feature of RDS.When you use Multi-AZ, RDS will create a standby database instance in a different AZ and replicate data to it synchronously. This synchronous data replication makes your failover experience seamless.</p> <p>You can scale the database tier with Amazon RDS Read Replicas. Read Replicas are available if you are using MySQL, PostgresSQL, or Amazon Aurora. RDS MySQL and RDS PostgresSQL allow up to five Read Replicas and leverage native replication capability of MySQL and PostgresSQL that are subject to replication lag</p> <p>Amazon Aurora / Amazon Document DB</p> <p>Allows up to 15 Replicas and experiences minimal replication lag because Aurora Replicas use the same underlying storage</p> <p>Aurora Auto Scaling dynamically adjusts the number of Aurora Replicas provisioned for an Aurora DB cluster using single-master replication. Aurora Auto Scaling is available for both Aurora MySQL and Aurora PostgreSQL. Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances.</p> <p>You define and apply a scaling policy to an Aurora DB cluster. The scaling policy defines the minimum and maximum number of Aurora Replicas that Aurora Auto Scaling can manage. Based on the policy, Aurora Auto Scaling adjusts the number of Aurora Replicas up or down in response to actual workloads, determined by using Amazon CloudWatch metrics and target values.</p> <p>You can use the AWS Management Console to apply a scaling policy based on a predefined metric. Alternatively, you can use either the AWS CLI or Aurora Auto Scaling API to apply a scaling policy based on a predefined or custom metric.</p>"},{"location":"blueprints/scalability/database-scaling.html#dynamodb-auto-scaling","title":"DynamoDB auto scaling","text":"<p>When you create a DynamoDB table, auto scaling is the default capacity setting, but you can also enable auto scaling on any table that does not have it active. Behind the scenes, as illustrated in the following diagram, DynamoDB auto scaling uses a scaling policy in Application Auto Scaling. To configure auto scaling in DynamoDB, you set the minimum and maximum levels of read and write capacity in addition to the target utilization percentage. Auto scaling uses Amazon CloudWatch to monitor a table's read and write capacity metrics. To do so, it creates CloudWatch alarms that track consumed capacity.</p> <p></p> <p>NoSQL databases</p> <p>Horizontal Scaling - The real advantage of NoSQL is horizontal scaling, aka shading. Considering NoSQL 'documents' are sort of a 'self-contained' object, objects can be on different servers without worrying about joining rows from multiple servers, as is the case with the relational model.</p>"},{"location":"blueprints/scalability/infrastructure-scaling.html","title":"Infrastructure Scaling","text":"<p>Amazon Elastic Cloud Compute (EC2) Service</p> <p>Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. Use Auto Scaling groups and set the minimum size to fit current needs and set the maximum size by applying growth % over next 5 years.</p> <p>An EC2 instance is equivalent to a virtual machine. Instances are grouped into families with different ratios of compute, storage, and network resources to optimally serve your unique workload needs.</p> <p>Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. Use Auto Scaling groups and set the minimum size to fit current needs and set the maximum size by applying growth % over next 5 years.</p> <p></p> <p>Load balance incoming traffic across EC2 instances using Elastic Load Balancing (ELB). ELB is a redundant, horizontally scaled service that requires zero administration.</p> <p>[Amazon Virtual Private Cloud (VPC)](https://aws.amazon.com/vpc/)</p> <p>Amazon VPC lets you provision a logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define. You have complete control over IP address range, subnets, and routing rules.</p> <p>VPC subnet</p> <p>A subnet is a slice of the overall VPC IP address range and tied to an Availability Zone. Virtual machines deployed into a public subnet can have randomly assigned public IPs and are visible to other Internet hosts. To make a subnet public, you need to create a routing rule for the subnet and direct internet-bound traffic to egress through an internet gateway, which is a horizontally scaled, redundant, and highly available VPC component that allow communication between instances in VPC and the internet.</p> <p>Amazon Route 53</p> <p>Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to one another.</p> <p>Amazon Simple Storage Service (Amazon S3).</p> <p>S3 is a highly durable and available object storage solution that is great for storing videos, log files, and image files. S3 achieves 11 9s of durability by making multiple copies of data across a region. Objects stored in S3 are widely accessible through RESTful APIs. S3 can serve as a central static asset repository that knows to partition storage automatically to increase performance as the number of requests increases over time.</p> <p>Amazon CloudFront</p> <p>When you use S3 as a central repository for assets, you can leverage a content distribution network (CDN) service on AWS to scale further by caching and serving your frequently requested content from XXX+ edge locations around the globe. Amazon CloudFront is capable of serving both static and dynamic content, supports SSL certificates, and costs zero dollars for data transferred from an origin (EC2 or S3) to CloudFront.</p> <p>Amazon DynamoDB</p> <p>DynamoDB help you scale by storing session or state data. You can store relational data in RDS or Aurora and offload session data to DynamoDB. It's a managed NoSQL database that lets you fine tune performance with provisioned throughput. It is fast, predictable, and highly secure. If you have key-value format session data, using DynamoDB can let you offload work from a primary database and increase overall system performance.</p> <p>Amazon ElastiCache</p> <p>Use Amazon ElastiCache to hold frequently accessed data and further decrease load from a primary database. ElastiCache is protocol compliant with Redis and Memcached. This means if you are using Redis or Memcached today, you don't need to refactor code to start using ElastiCache. A benefit of using ElastiCache is that it is a managed service. This means AWS will detect and replace unhealthy nodes for you. If you use ElastiCache Redis, you also have the option to stand up replicas in a different AZ to increase availability.</p>"},{"location":"blueprints/security/index.html","title":"Security Blueprint","text":"<p>Security is the protection of customer data and systems against unauthorized access, use, disclosure, disruption, modification, or destruction.</p>"},{"location":"blueprints/sustainability/index.html","title":"Sustainability Blueprint","text":"<p>Sustainability is understanding the impacts of the compute services used by a solution, quantifying impacts through the entire workload lifecycle, and applying design principles and best practices to reduce these impacts.</p>"},{"location":"blueprints/sustainability/guidance.html","title":"Sustainability Guidance","text":"<p>Organizations today are in search of vetted solutions and architectural guidance to rapidly solve business challenges. Prescriptive architectural diagrams, sample code, and technical content that provides guidnace on building sustainability solutions. </p>"},{"location":"blueprints/sustainability/guidance.html#optimizing-data-architecture-for-sustainability-on-aws","title":"Optimizing Data Architecture for Sustainability on AWS","text":"<p>This Guidance demonstrates how you can optimize a data architecture for sustainability on AWS that maximizes efficiency and reduces waste. Included are curated data services and best practices that help you identify the right solution for your workloads, so you can build an efficient, end-to-end modern data architecture in the cloud. With a comprehensive set of data and analytics capabilities, this Guidance helps you design a data strategy that grows with your business.</p> <p>Explore Guidance for Optimizing Data Architecture for Sustainability on AWS</p>"},{"location":"blueprints/sustainability/guidance.html#product-carbon-footprinting-on-aws","title":"Product Carbon Footprinting on AWS","text":"<p>This Guidance helps customers scale product carbon footprint (PCF) tracking, reduce the manual effort involved with data collection and calculation, and provide transparent and auditable PCFs for reporting. The architecture pairs Internet of Things (IoT) sensor data from a manufacturing facility with product information and emission factors. An interactive dashboard uses this data to track product-level energy and carbon footprint in addition to benchmarking environmental performance across equipment and sites. With this Guidance, customers can identify hotspots and best practices to lower their PCF and manufacturing costs.</p> <p>Explore Guidance for Product Carbon Footprinting on AWS</p>"},{"location":"blueprints/sustainability/guidance.html#geospatial-insights-for-sustainability-on-aws","title":"Geospatial Insights for Sustainability on AWS","text":"<p>This Guidance helps customers observe land use changes using geospatial data to support supply chain best practices. Customers can monitor changes in forest density using an Amazon SageMaker geospatial capability that simplifies the process of analyzing satellite images for changes in vegetation. Results are stored, cataloged, and observable on Amazon QuickSight for customers to review.</p> <p>Explore Guidance for Geospatial Insights for Sustainability on AWS</p>"},{"location":"blueprints/sustainability/guidance.html#optimizing-deep-learning-workloads-for-sustainability-on-aws","title":"Optimizing Deep Learning Workloads for Sustainability on AWS","text":"<p>This guidance applies principles and best practices from the Sustainability Pillar of the AWS Well-Architected Framework to reduce the carbon footprint of your deep learning workloads. From data processing to model building, training and inference, this guidance demonstrates how to maximize utilization and minimize the total resources needed to support your workloads.</p> <p>Explore Guidance for Optimizing Deep Learning Workloads for Sustainability on AWS</p>"},{"location":"blueprints/sustainability/guidance.html#electric-vehicle-battery-health-prediction-on-aws","title":"Electric Vehicle Battery Health Prediction on AWS","text":"<p>This Guidance demonstrates how to use historical battery health data with artificial intelligence and machine learning (AI/ML) algorithms to improve the accuracy of battery State of Health (SoH) and Remaining Useful Life (RUL) estimations. Currently, these estimations largely rely on a static formula-based approach, which can provide near-term battery health information. Using this Guidance, automotive original equipment manufacturers (OEMs) can predict battery SoH and RUL into the future with easy-to-train AI/ML models built using historical data stored in the Cloud. </p> <p>Explore Electric Vehicle Battery Health Prediction on AWS</p>"},{"location":"enablement/workshops.html","title":"Workshops","text":"<p>Workshops are interactive and hands-on learning sessions that focus on specific technical skills or technologies. These workshops are designed to provide practical knowledge and training in a specific area of technology or a particular tool or software. Participants typically engage in activities, exercises, and demonstrations to enhance their understanding and proficiency in the subject matter.</p> <p>A catalog of over 100 workshops is available at AWS Workshops. Search by topic, AWS services, industry, or technology to get started.</p>"},{"location":"getting_started/index.html","title":"Getting Started","text":"<p>To get started building a solution using Solution Building Enablement:</p> <ul> <li> <p> Explore our Solution Building Asset Repository for curated collection of open source technical resources to help you build solutions faster using AWS-vetted build assets and components. To familiarize yourself with the types of resources available here, review the Glossary and then search for resources within Horizontal Technologies, Industries, Tools, and Enablement.</p> </li> <li> <p> Join the AWS Partner Network to innovate, expand your customer reach, and grow your business with technical, marketing, and funding resources from AWS.</p> </li> <li> <p> Use our Solution Building Enablement guide to kick-start your AWS Partner Journey and expedite the delivery of industry-focused solutions with business outcome.</p> </li> </ul> <p>Note: Solution Building Enablement and the Asset Repository are intended to be used as needed on your schedule. Some solutions may not be published to AWS Marketplace and therefore may not require you to be an AWS Partner. You are still welcome to use any of the resources in this Repository to facilitate your solution development.</p>"},{"location":"getting_started/index.html#aws-partner-network","title":"AWS Partner Network","text":"<p>The AWS Partner Network is a global community of partners that leverages programs, expertise, and resources to build, market, and sell customer offerings. This diverse network features 100,000 partners from more than 150 countries. AWS Partners are central to how AWS delivers innovation to customers at scale. As an AWS Partner, you are uniquely positioned to help customers take full advantage of all that AWS has to offer and accelerate their journey to the cloud. Together, partners and AWS can provide innovative solutions, solve technical challenges, win deals, and deliver value to our mutual customers.</p>"},{"location":"getting_started/ADF/index.html","title":"Application Design Framework (ADF)","text":"<p>Align business with technology, minimize rework, ease evolution.</p>"},{"location":"getting_started/ADF/index.html#context","title":"Context","text":"<p>Companies build, maintain and evolve applications over time. They need to do so safely (reduce blast radius) and effectively (build on existing knowledge) to reduce risk and increase velocity. ADF aims to help companies achieve these goals.</p>"},{"location":"getting_started/ADF/index.html#definitions","title":"Definitions","text":"<p>Application <sup>[1]</sup>:</p> <ul> <li>A body of code that's seen by developers as a single unit</li> <li>A group of functionality that business customers see as a single unit</li> <li>An initiative that those with the money see as a single budget</li> </ul> <p>Component <sup>[2][3]</sup>:</p> <ul> <li>Software components are things that are independently replaceable and upgradeable.</li> <li>A component is the code, configuration, and resources that together deliver against a requirement. A component is decoupled from other components.</li> </ul> <p>Architecture <sup>[3]</sup>:</p> <ul> <li>An architecture describes how components work together. How components communicate and interact is often the focus of architecture diagrams.</li> </ul>"},{"location":"getting_started/ADF/index.html#mindset","title":"Mindset","text":"<p>Application is an ownership boundary. It includes metadata, toolchain, and components.</p> <p>Application boundary should evolve with organizational and software changes.</p>"},{"location":"getting_started/ADF/index.html#recommendations","title":"Recommendations","text":"<p>ADF introduces the following recommendations along with related personas:</p> <ol> <li>Work backwards [Sales, Marketing, Product] to clarify problem and solution. Capture use cases. Consider writing press release/frequently asked questions (PR/FAQ) or pitch.</li> <li>Write stories and flows [Product] to start conversation about requirements.</li> <li>Define requirements [Product, Engineering] to guide architectural decisions. Consider functional and non-functional requirements.</li> <li>Define architecture [Product, Engineering] to address requirements. Describe stories and flows on architecture level. Identify applications and their components. Validate application boundaries using the following \u201cfracture planes\u201d <sup>[4]</sup>: 1/ Profit and Loss Group 2/ Business Domain Bounded Context 3/ Regulatory Compliance 4/ Change Cadence 5/ Team Location 6/ Risk 7/ Performance Isolation 8/ Technology 9/ User Personas.</li> <li>Write Architectural Decision Records (ADRs) [Engineering] to capture the design including \u201cwhy\u201d behind decisions, options considered, trade-offs, and consequences. ADRs allow to evolve the architecture starting from known design considerations. Consider building proof of concept (POC) to validate the decisions.</li> <li>Create one or more repositories and a single pipeline per application [Engineering] to reduce blast radius and increase delivery performance.</li> <li>Group infrastructure and runtime code by application components [Engineering] to align architecture with implementation. Consider grouping each of the infrastructure and runtime code by logical units. That should ease evolution and maintainability.</li> </ol>"},{"location":"getting_started/ADF/index.html#examples","title":"Examples","text":"<ul> <li>IAM Session Broker</li> <li>End-to-end design process and application boundary evolution</li> <li>Deployment strategy and application boundary evolution</li> </ul>"},{"location":"getting_started/ADF/index.html#related-frameworks","title":"Related frameworks","text":"<ul> <li>AWS Well-Architected Framework - apply architectural best practices for designing and operating reliable, secure, efficient, cost-effective, and sustainable systems.</li> <li>Operational Readiness Review - ensure a consistent review of operational readiness, with a specific focus on eliminating known, common causes of impact</li> </ul>"},{"location":"getting_started/ADF/index.html#related-guidance","title":"Related guidance","text":"<ul> <li>Awesome Architecture - concepts and foundations, followed by jobs-to-be-done</li> </ul>"},{"location":"getting_started/ADF/index.html#references","title":"References","text":"<ol> <li>Martin Fowler - ApplicationBoundary</li> <li>Martin Fowler - SoftwareComponent</li> <li>AWS Well-Architected Framework - Definitions</li> <li>Matthew Skelton - Designing organizations for responsiveness</li> </ol>"},{"location":"getting_started/ADF/application_evolution_1.html","title":"End-to-end design process and application boundary evolution","text":"<p>This example describes design phases in high-level and how application boundary changes as part of company growth. It doesn't go into details of stories, requirements, architecture and architectural decision records.</p>"},{"location":"getting_started/ADF/application_evolution_1.html#initial-design","title":"Initial design","text":""},{"location":"getting_started/ADF/application_evolution_1.html#end-to-end-process","title":"End-to-end process","text":""},{"location":"getting_started/ADF/application_evolution_1.html#working-backwards","title":"Working backwards","text":"<p>AnyCompany product team wants to build a DocuStar product that allows enterprises to store, view, edit, and share documents. Product team identified two main personas within customers - end-users and application security engineers. Working backwards from these personas, product team identified 3 main use cases: 1/ documents storage (end-users) 2/ user management (application security engineers) 3/ audit trail (application security engineers).</p>"},{"location":"getting_started/ADF/application_evolution_1.html#stories","title":"Stories","text":"<p>Product team captured stories and flows for each use case. </p>"},{"location":"getting_started/ADF/application_evolution_1.html#requirements","title":"Requirements","text":"<p>Product and engineering teams worked together to define the functional and non-functional requirements to satisfy the stories.</p>"},{"location":"getting_started/ADF/application_evolution_1.html#architecture","title":"Architecture","text":"<p>Engineering team captured flows for each story on architecture level (e.g. sign in, get upload link, upload document, log activity) to identify applications and their components. </p> <p>Engineering team decided to create the following applications: 1/ Soteria (documents storage) 2/ Aegis (user management) 3/ Athena (audit trail). The engineering team then identified components for each application and their interactions.</p> <p>Note: Aegis and Athena details are left out for brevity through the rest of the document.</p> <p>Soteria</p> <ul> <li>UI - Authenticates users and allows them to interact with the application</li> <li>Service - Generates protected links for upload and download</li> <li>Repository - Stores the documents</li> <li>Monitoring - Displays business and operational metrics</li> </ul> <p>Soteria interacts with Aegis and Athena to sign in users and log user activity respectively.</p>"},{"location":"getting_started/ADF/application_evolution_1.html#architectural-decision-records","title":"Architectural decision records","text":"<p>Engineering team captured the design in architectural decision records (ADRs).</p> <p>Soteria</p> <ul> <li>UI infrastructure includes Route 53, CloudFront, S3, CloudWatch, X-Ray, and Service Catalog services. UI runtime includes React application.</li> <li>Service infrastructure includes Route 53, Certificate Manager, WAF, EC2, Elastic Load Balancing, ECS, DynamoDB, CloudWatch, X-Ray, and Service Catalog services. Service runtime includes Express application.</li> <li>Repository infrastructure includes Route 53, S3, CloudWatch, X-Ray, and Service Catalog services.</li> <li>Monitoring infrastructure includes Route 53, CloudWatch, X-Ray, and Service Catalog services.</li> </ul>"},{"location":"getting_started/ADF/application_evolution_1.html#code-structure","title":"Code structure","text":"<p>Soteria</p> <p>Engineering team decided to create a single code repository for the Soteria application. The implementation includes components (UI, service, repository, and monitoring), toolchain (deployment pipeline and pull request build), and metadata (resources and attributes). Components, toolchain, and metadata deploy as a stack each. Components and toolchain resources are associated with the metadata.</p> <p>Engineering team decided to create the following environments: 1/ per-builder sandbox environment 2/ management environment (toolchain and metadata) 3/ alpha, beta, gamma, and production environments (components). Toolchain deploys components to the alpha, beta, gamma and production environments.</p> <pre><code>Repo: soteria.git\n\ncomponents/\n  ui/\n    runtime/\n      &lt;React application&gt;\n    infrastructure.py\n      class UI:\n        route53.HostedZone\n        route53.CNAMERecord\n        certificatemanager.Certificate\n        cloudfront.Distribution\n        s3.Bucket\n        s3_assets.Asset\n        cloudwatch.Metric\n        cloudwatch.Alarm\n        xray.Group\n        servicecatalogappregistry.AttributeGroup\n        servicecatalogappregistry.AttributeGroupAssociation\n  service/\n    runtime/ \n      &lt;Express application + Dockerfile&gt;\n    infrastructure.py\n      class Service:\n        route53.HostedZone\n        route53.CNAMERecord\n        certificatemanager.Certificate\n        waf.WebACL\n        ec2.VPC\n        ec2.Subnet\n        ec2.RouteTable\n        ec2.SecurityGroup\n        elasticloadbalancingv2.NetworkLoadBalancer\n        ecs.Cluster\n        ecs.Service\n        ecs.TaskDefinition\n        ecr_assets.DockerImageAsset\n        dynamodb.Table\n        cloudwatch.Metric\n        cloudwatch.Alarm\n        xray.Group\n        servicecatalogappregistry.AttributeGroup\n        servicecatalogappregistry.AttributeGroupAssociation\n  monitoring/\n    infrastructure.py\n      class Monitoring:\n        route53.HostedZone\n        route53.CNAMERecord\n        cloudwatch.Dashboard\n        xray.Group\n        servicecatalogappregistry.AttributeGroup\n        servicecatalogappregistry.AttributeGroupAssociation\n  repository/\n    infrastructure.py\n      class Repository:\n        route53.HostedZone\n        route53.CNAMERecord\n        certificatemanager.Certificate\n        s3.Bucket\n        cloudwatch.Metric\n        cloudwatch.Alarm\n        xray.Group\n        servicecatalogappregistry.AttributeGroup\n        servicecatalogappregistry.AttributeGroupAssociation\n  infrastructure.py \n    class Components:\n      UI\n      Service\n      Repository\n      Monitoring\n      servicecatalogappregistry.ResourceAssociation\nmetadata/\n  infrastructure.py\n    class Metadata:\n      servicecatalogappregistry.Application\ntoolchain/\n  infrastructure.py\n    class Toolchain:\n      pipelines.CodePipeline\n      codebuild.Project\n      servicecatalogappregistry.ResourceAssociation\napp.py\n  Metadata(\"Soteria-Metadata-Sandbox\")\n  Components(\"Soteria-Components-Sandbox\")\n  Toolchain(\"Soteria-Toolchain-Sandbox\")\n\n  Metadata(\"Soteria-Metadata-Management\")\n  Toolchain(\"Soteria-Toolchain-Management\")\n</code></pre>"},{"location":"getting_started/ADF/application_evolution_1.html#design-evolution","title":"Design evolution","text":""},{"location":"getting_started/ADF/application_evolution_1.html#architecture_1","title":"Architecture","text":"<p>As the functionality and the team grew, Soteria (documents storage) maintenance and evolution became harder and slower. The engineering team decided to split the Soteria application into smaller applications and keep Aegis (user management) and Athena (audit trail) as is. Soteria became 3 applications: 1/ Aphrodite (documents UI) 2/ Themis (documents service) 3/ Olympus (documents repository). The engineering team then moved existing components to the new applications and created dedicated components for each application (e.g. monitoring).</p> <p>Aphrodite</p> <ul> <li>UI - Authenticates users and allows them to interact with the application</li> <li>Monitoring - Displays business and operational metrics</li> </ul> <p>Themis</p> <ul> <li>Service - Generates protected links for upload and download</li> <li>Monitoring - Displays business and operational metrics</li> </ul> <p>Olympus</p> <ul> <li>Repository - Stores the documents</li> <li>Monitoring - Displays business and operational metrics</li> </ul> <p>Aphrodite and Themis interact with Aegis and Athena to sign in users and log user activity respectively.</p>"},{"location":"getting_started/ADF/application_evolution_1.html#architectural-decision-records_1","title":"Architectural decision records","text":"<p>Engineering team captured the design in architectural decision records (ADRs).</p> <p>Aphrodite</p> <ul> <li>UI infrastructure includes Route 53, CloudFront, S3, CloudWatch, X-Ray, and Service Catalog services. UI runtime includes React application.</li> <li>Monitoring infrastructure includes Route 53, CloudWatch, X-Ray, and Service Catalog services.</li> </ul> <p>Themis</p> <ul> <li>Service infrastructure includes Route 53, Certificate Manager, WAF, EC2, Elastic Load Balancing, ECS, DynamoDB, CloudWatch, X-Ray, and Service Catalog services. Service runtime includes Express application.</li> <li>Monitoring infrastructure includes Route 53, CloudWatch, X-Ray, and Service Catalog services.</li> </ul> <p>Olympus</p> <ul> <li>Repository infrastructure includes Route 53, S3, CloudWatch, X-Ray, and Service Catalog services.</li> <li>Monitoring infrastructure includes Route 53, CloudWatch, X-Ray, and Service Catalog services.</li> </ul>"},{"location":"getting_started/ADF/application_evolution_1.html#code-structure_1","title":"Code structure","text":"<p>Aphrodite</p> <p>Engineering team decided to create a single code repository for the Aphrodite application. The implementation includes components (UI and monitoring), toolchain (deployment pipeline and pull request build), and metadata (resources and attributes). Components, toolchain, and metadata deploy as a stack each. Components and toolchain resources are associated with the metadata.</p> <p>Engineering team decided to create the following environments: 1/ per-builder sandbox environment 2/ management environment (toolchain and metadata) 3/ alpha, beta, gamma, and production environments (components). Toolchain deploys components to the alpha, beta, gamma and production environments.</p> <pre><code>Repo: aphrodite.git\n\ncomponents/\n  ui/\n    runtime/\n      &lt;React application&gt;\n    infrastructure.py\n      class UI:\n        route53.HostedZone\n        route53.CNAMERecord\n        certificatemanager.Certificate\n        cloudfront.Distribution\n        s3.Bucket\n        s3_assets.Asset\n        cloudwatch.Metric\n        cloudwatch.Alarm\n        xray.Group\n        servicecatalogappregistry.AttributeGroup\n        servicecatalogappregistry.AttributeGroupAssociation\n  monitoring/\n    infrastructure.py\n      class Monitoring:\n        route53.HostedZone\n        route53.CNAMERecord\n        cloudwatch.Dashboard\n        xray.Group\n        servicecatalogappregistry.AttributeGroup\n        servicecatalogappregistry.AttributeGroupAssociation\n  infrastructure.py \n    class Components:\n      UI\n      Monitoring\n      servicecatalogappregistry.ResourceAssociation\nmetadata/\n  infrastructure.py\n    class Metadata:\n      servicecatalogappregistry.Application\ntoolchain/\n  infrastructure.py\n    class Toolchain:\n      pipelines.CodePipeline\n      codebuild.Project\n      servicecatalogappregistry.ResourceAssociation\napp.py\n  Metadata(\"Aphrodite-Metadata-Sandbox\")\n  Components(\"Aphrodite-Components-Sandbox\")\n  Toolchain(\"Aphrodite-Toolchain-Sandbox\")\n\n  Metadata(\"Aphrodite-Metadata-Management\")\n  Toolchain(\"Aphrodite-Toolchain-Management\")\n</code></pre> <p>Themis and Olympus follow a similar approach to Aphrodite, so left out for brevity.</p>"},{"location":"getting_started/ADF/application_evolution_2.html","title":"Deployment strategy and application boundary evolution","text":"<p>This example describes changes in deployment and ownership as part of company growth. It doesn't go into details of stories, requirements, architecture, architectural decision records and code structure.</p>"},{"location":"getting_started/ADF/application_evolution_2.html#initial-design","title":"Initial design","text":"<p>AnyCompany product team wants to build a DocuStar product that allows enterprises to store, view, edit, and share documents. Engineering team decided to create a single Soteria application with 4 components: 1/ documents UI 2/ documents service 3/ user management service 4/ IAM session broker service. Soteria application also includes toolchain (deployment pipeline and pull request build) and metadata (resources and attributes). Components, toolchain, and metadata deploy as a stack each. Components and toolchain resources are associated with the metadata.</p>"},{"location":"getting_started/ADF/application_evolution_2.html#architecture","title":"Architecture","text":""},{"location":"getting_started/ADF/application_evolution_2.html#deployment","title":"Deployment","text":""},{"location":"getting_started/ADF/application_evolution_2.html#deployment-evolution","title":"Deployment evolution","text":"<p>There were several occasions where code changes inadvertently deleted data resources (e.g. documents repository). The team had snapshots, but it took time to restore. The engineering team decided to split the components into discrete stacks and add termination protection for the sensitive resources to decrease blast radius.</p>"},{"location":"getting_started/ADF/application_evolution_2.html#architecture_1","title":"Architecture","text":""},{"location":"getting_started/ADF/application_evolution_2.html#deployment_1","title":"Deployment","text":""},{"location":"getting_started/ADF/application_evolution_2.html#ownership-evolution","title":"Ownership evolution","text":"<p>As the company grew, it established more teams and also decided to create an internal platform (DocuGuard) for the shared user management and IAM session broker services. There was now a dedicated team for each of the components (documents UI, documents service, user management service, IAM session broker service). The company decided to create a dedicated application for each of these components and assign ownership to the respective teams.</p>"},{"location":"getting_started/ADF/application_evolution_2.html#architecture_2","title":"Architecture","text":""},{"location":"getting_started/ADF/application_evolution_2.html#deployment_2","title":"Deployment","text":""},{"location":"getting_started/ADF/iam_session_broker.html","title":"Iam session broker","text":""},{"location":"getting_started/ADF/iam_session_broker.html#use-cases","title":"Use cases","text":"<p>A multi-tenant SaaS composes multiple applications. Applications use a shared IAM Session Broker library to scope user access to their tenant\u2019s boundary. SaaS provider wants to build a shared IAM Session Broker application instead to reduce operational overhead and improve security posture. The application should initially support the ABAC authorization strategy.</p>"},{"location":"getting_started/ADF/iam_session_broker.html#stories-and-flows","title":"Stories and flows","text":"<ul> <li>Scope the user access to their tenant\u2019s boundary</li> <li>SaaS admin registers users for Yellow and Blue tenants</li> <li>Yellow user authenticates</li> <li>Yellow user downloads Yellow data</li> <li>Yellow user gets access denied when trying to download Blue data</li> </ul>"},{"location":"getting_started/ADF/iam_session_broker.html#requirements","title":"Requirements","text":""},{"location":"getting_started/ADF/iam_session_broker.html#functional","title":"Functional","text":"<ul> <li>Require the applications to use ABAC with <code>${aws:PrincipalTag/</code><code>key</code><code>}</code> variable in policies for scoping access</li> <li>Require the applications to register and provide the following access metadata:</li> <li>Access role name (e.g. <code>DocumentsAPIDataAccess</code>)</li> <li>Session tag key (e.g. <code>TenantID</code>)</li> <li>JWT claim name (e.g. <code>custom:tenant_id</code>)</li> <li>JSON Web Key (JWK) Set URL (e.g. <code>https://cognito-idp.&lt;Region&gt;.amazonaws.com/&lt;userPoolId&gt;/.well-known/jwks.json</code>)</li> <li>Organize the access metadata by application name</li> <li>Use the assumed-role session principal (service) role name for application name during registration</li> <li>Authorize the registration requests for assumed-role session principals in the same account</li> <li>Authorize the temporary security credentials requests for a registered application and a valid JWT</li> <li>Validate the JWT by public keys in the registered application JWK Set URL</li> <li>Return the scoped temporary security credentials by assuming the access role. Tag the session with the registered session tag key and JWT claim name value as the session tag key value.</li> </ul>"},{"location":"getting_started/ADF/iam_session_broker.html#non-functional","title":"Non-functional","text":"<ul> <li>Generate SDKs for multiple programming languages from service specification</li> <li>Collect audit logs for security and compliance</li> <li>Use AWS STS to request temporary security credentials</li> <li>Cache the returned temporary security credentials to reduce latency</li> <li>Throttle applications on tenant-level to protect against noisy-neighbor</li> </ul>"},{"location":"getting_started/ADF/iam_session_broker.html#architecture","title":"Architecture","text":""},{"location":"getting_started/ADF/iam_session_broker.html#applications","title":"Applications","text":"<p>Context</p> <p>We need to identify applications to build by describing stories and flows on architecture level.</p> <p>Scope the user access to their tenant\u2019s boundary:</p> <p></p> <ol> <li>Yellow user authenticates using Identity Provider and gets a JWT</li> <li>Yellow user accesses the Application with JWT to download Yellow data</li> <li>Application calls IAM Session Broker to acquire Yellow-scoped temporary security credentials</li> <li>IAM Session Broker verifies the JWT and returns Yellow-scoped temporary security credentials</li> <li>Application returns Yellow data to the user</li> </ol> <p>Decision</p> <p>Create IAM Session Broker application that returns tenant-scoped temporary security credentials based on JWT claims for registered applications. IAM Session Broker should have a dedicated Git repository and a pipeline to reduce blast radius and increase delivery performance. Identity Provider application is out of scope for this document.</p> <p>Consequences</p> <p>IAM Session Broker is on the critical path for upstream applications. Hence, it should maintain the agreed upon service level objectives (SLOs). Users drive the requests volume to IAM Session Broker, because the user-agent provides the JWT. Hence, IAM Session Broker performance characteristics should take interactive flows as the baseline.</p>"},{"location":"getting_started/ADF/iam_session_broker.html#iam-session-broker-components","title":"IAM Session Broker components","text":"<p>Context</p> <p>We need to identify IAM Session Broker components.</p> <p>Decision</p> <p>Create API component with the following logical units:</p> <p></p> <p>Gateway should authorize requests and throttle if needed to prevent the \u201cnoisy neighbor\u201d problem. Gateway should proxy all authorized and non-throttled requests to Credentials Manager. Credentials Manager should 1/ fetch Access Metadata 2/ call Temporary Security Credentials Provider to assume the Service Role 3/ call Temporary Security Credentials Provider using the Service Role credentials to assume the access role 4/ return the scoped temporary security credentials.</p> <p>Gateway</p> <p>Use Amazon API Gateway HTTP API with IAM authorization. Use proxy integration for Credentials Manager. Leverage usage plans to prevent the \u201cnoisy neighbor\u201d problem. Use the Lambda authorizer as the API key source (example) and application name as the API key.</p> <p>Credentials Manager</p> <p>Use Lambda function and Lambda Powertools for Python. Use Lambda provisioned concurrency to reduce latency. Cache applications scoped temporary security credentials to further reduce latency.</p> Request Description Request body Response POST /applications Register an application {\u2003\"AccessRoleName\": \"...\",\u2003\"SessionTagKey\": \"...\",\u2003\"JWTClaimName\": \"...\",\u2003\"JWKSetURL\": \"...\"} GET /credentials?jwt=X Return scoped temporary security credentials {\u2003\"AccessKeyId\":\"...\",\u2003\"SecretAccessKey\":\"...\",\u2003\"SessionToken\":\"...\"} <p>Access Metadata</p> <p>Use Amazon DynamoDB. Create an <code>AccessMetadata</code> DynamoDB table. The table should store the registered access metadata.</p> <p>Data model (designed using NoSQL WorkBench for DynamoDB):</p> <p></p> <p>Temporary Security Credentials Provider</p> <p>Use AWS Session Token Service (AWS STS). Other options include AWS IAM Roles Anywhere and AWS IoT Core credential provider. Choose AWS STS because there is currently no requirement to support on-premises applications and/or certificate-based authentication use cases.</p> <p>Service Role</p> <p>Create <code>IAMSessionBroker</code> IAM role. Creating a dedicated Service Role enables flexibility for the IAM Session Broker architecture. Applications should trust the Service Role to assume their access role. The service role doesn\u2019t have any permissions. Per requirements, applications and IAM Session Broker should be in the same account. The applications access role\u2019s trust policy acts as an IAM resource-based policy. When a resource-based policy grants access to a principal in the same account, no additional identity-based policy is required (documentation).</p> <p>Note: Applications access role trust policy uses the <code>IAMSessionBroker</code> role ID once saved. Deleting or altering the <code>IAMSessionBroker</code> role will require applications to update their access role\u2019s trust policy to apply the new <code>IAMSessionBroker</code> role ID.</p> <p>Consequences</p> <p>To support cross-account scenarios, would need to replace API Gateway HTTP API by API Gateway REST API, because HTTP API doesn\u2019t support resource policies at this time.</p> <p>Credentials Manager uses role chaining: 1/ assume Service Role 2/ assume application access role. Role chaining limits the role session to a maximum of one hour. In the worst case scenario, the Credentials Manager will need to call Temporary Security Credentials Provider (AWS STS) every hour for a specific application.</p>"},{"location":"getting_started/ADF/iam_session_broker.html#service-discovery","title":"Service discovery","text":"<p>Context</p> <p>We need to decide on a service discovery strategy to allow applications discover IAM Session Broker API endpoint without managing configuration files.</p> <p>Decision</p> <p>Use DNS for service discovery. Use the following naming convention for domain hierarchy:</p> <pre><code>&lt;application&gt;.&lt;region&gt;.&lt;environment&gt;.&lt;product&gt;.&lt;top-level domain&gt;\n</code></pre> <p>Example:</p> <pre><code>iam-session-broker.eu-west-1.gamma.saas-platform.example.com\n</code></pre> <p>Delegate the product sub-domain to a dedicated AWS account so that each product team can manage DNS zones for their applications. Using the above approach, applications can construct the IAM Session Broker API endpoint at deployment time.</p> <p>Consequences</p> <p>This approach supports cross-environment (account and Region) use cases by relying on naming convention.</p>"},{"location":"getting_started/ADF/iam_session_broker.html#implementation","title":"Implementation","text":""},{"location":"getting_started/ADF/iam_session_broker.html#toolchain","title":"Toolchain","text":"<p>Python: <code>3.9.11</code></p> <p>AWS CDK Toolkit (CLI) and AWS CDK Construct Library: <code>2.69.0</code></p> <p>Project template: https://github.com/aws-samples/aws-cdk-project-structure-python</p>"},{"location":"getting_started/ADF/iam_session_broker.html#git-repositories","title":"Git repositories","text":"<p>IAM Session Broker: <code>iam-session-broker</code></p>"},{"location":"getting_started/ADF/iam_session_broker.html#code-structure","title":"Code structure","text":"<p>IAM Session Broker <pre><code>components/\n  api/\n    runtime/\n      &lt;AWS Lambda Powertools for Python application&gt;\n    infrastructure.py\n      class API:\n        Gateway\n        CredentialsManager\n        AccessMetadata\n        ServiceRole\n      class Gateway:\n        apigatewayv2.Api\n        apigatewayv2.Model\n        apigatewayv2.Route\n        apigatewayv2.Stage\n        apigatewayv2.Authorizer\n        apigatewayv2.Deployment\n      class CredentialsManager:\n        lambda.Function\n        lambda.Alias\n        lambda.Version\n      class AccessMetadata:\n        dynamodb.Table\n      class ServiceRole:\n        iam.Role\n  infrastructure.py \n    class Components:\n      API\ntoolchain/\n  infrastructure.py\n    class Toolchain:\n      pipelines.CodePipeline\n      codebuild.Project\napp.py\n  Components(\"IAMSessionBroker-Components-Sandbox\")\n  Toolchain(\"IAMSessionBroker-Toolchain-Sandbox\")\n\n  Toolchain(\"IAMSessionBroker-Toolchain-Production\")\n</code></pre></p>"},{"location":"getting_started/ADF/iam_session_broker.html#backlog","title":"Backlog","text":""},{"location":"getting_started/ADF/iam_session_broker.html#appendix","title":"Appendix","text":""},{"location":"getting_started/ADF/iam_session_broker.html#api-gateway-http-api-with-iam-authorizer-lambda-proxy-integration-payload-example","title":"API Gateway HTTP API with IAM authorizer Lambda proxy integration payload example","text":"<pre><code>{\n    \"version\": \"2.0\",\n    \"routeKey\": \"$default\",\n    \"rawPath\": \"/applications\",\n    \"rawQueryString\": \"\",\n    \"headers\": {\n        \"accept\": \"application/xml\",\n        \"accept-encoding\": \"gzip, deflate\",\n        \"authorization\": \"AWS4-HMAC-SHA256 Credential=ASIA3YC54HEJWX32ILMG/20230317/eu-west-1/execute-api/aws4_request, SignedHeaders=host;x-amz-date;x-amz-security-token, Signature=1b0063ba301f7668d5c7c982e505609db52ad83c3a879c311047acf6205cb760\",\n        \"content-length\": \"0\",\n        \"content-type\": \"application/json\",\n        \"host\": \"d0cfuu2ujg.execute-api.eu-west-1.amazonaws.com\",\n        \"user-agent\": \"python-requests/2.28.2\",\n        \"x-amz-content-sha256\": \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\",\n        \"x-amz-date\": \"20230317T170215Z\",\n        \"x-amz-security-token\": \"&lt;redacted&gt;\",\n        \"x-amzn-trace-id\": \"Root=1-64149d18-046d89152552d1da34770913\",\n        \"x-forwarded-for\": \"85.250.125.159\",\n        \"x-forwarded-port\": \"443\",\n        \"x-forwarded-proto\": \"https\"\n    },\n    \"requestContext\": {\n        \"accountId\": \"111111111111\",\n        \"apiId\": \"d0cfuu2ujg\",\n        \"authorizer\": {\n            \"iam\": {\n                \"accessKey\": \"ASIA3YC54HEJWX32ILMG\",\n                \"accountId\": \"111111111111\",\n                \"callerId\": \"AROA3YC54HEJ7ZID2KGLH:user@example.com\",\n                \"cognitoIdentity\": null,\n                \"principalOrgId\": \"aws:PrincipalOrgID\",\n                \"userArn\": \"arn:aws:sts::111111111111:assumed-role/AWSReservedSSO_DeveloperAccess_073t3cf358b80610/user@example.com\",\n                \"userId\": \"AROA3YC54HEJ7ZID2KGLH:user@example.com\"\n            }\n        },\n        \"domainName\": \"d0cfuu2ujg.execute-api.eu-west-1.amazonaws.com\",\n        \"domainPrefix\": \"d0cfuu2ujg\",\n        \"http\": {\n            \"method\": \"POST\",\n            \"path\": \"/applications\",\n            \"protocol\": \"HTTP/1.1\",\n            \"sourceIp\": \"85.250.125.159\",\n            \"userAgent\": \"python-requests/2.28.2\"\n        },\n        \"requestId\": \"B7170h_-DoEEPzA=\",\n        \"routeKey\": \"$default\",\n        \"stage\": \"$default\",\n        \"time\": \"17/Mar/2023:17:02:16 +0000\",\n        \"timeEpoch\": 1679072536104\n    },\n    \"isBase64Encoded\": false\n}\n</code></pre>"},{"location":"getting_started/playbook/index.html","title":"SBG Playbook","text":""},{"location":"getting_started/playbook/index.html#who-should-use-this-guidance","title":"Who should use this Guidance?","text":"<p>Solution Building Guidance is intended to help all APN partners build and launch.</p>"},{"location":"getting_started/playbook/index.html#what-is-the-purpose-of-this-guidance","title":"What is the purpose of this Guidance?","text":"<p>This content guides a user through ideating, building, going to market (GTM), selling, and managing the solution lifecycle for Line of Business (LoB) and C-suite buyer-focused outcome solutions. Solution Building Guidance creates a consistent and repeatable solution journey that accelerates solution building.</p>"},{"location":"getting_started/playbook/index.html#how-to-use-this-guidance","title":"How to use this Guidance?","text":"<p>This guidance is designed to help all partners through AWS solution building.</p> <p>Each section of the Guidance aligns to one of the four solution journey Hubs:</p> <ul> <li> <p>Ideate Hub</p> </li> <li> <p>Design and Build Hub</p> </li> <li> <p>Go-to-Market and Sales Hub</p> </li> <li> <p>Solution Lifecycle Management Hub</p> </li> </ul>"},{"location":"getting_started/playbook/design_and_build_hub/index.html","title":"Design and Build Hub","text":"<p>The Build Hub provides AWS Partners and AWS technical resources access to build tools, Open Source code, guidance, and testing mechanisms to help accelerate partner solution builds while meeting consistent quality bars. The Partner Solution Technology Team is responsible for the curation and onboarding assets, accelerators, guidance, and tooling.</p> <p>It provides everything needed to get started building, enhancing, and maintaining solutions. This would consist of the following components:</p> <ol> <li>Architectural patterns and best practices</li> <li>Templates and code examples, such as CloudFormation templates and CDK (Cloud Development Kit) examples</li> <li>Source code and solution assets for existing published solutions</li> <li>SDKs and developer toolchains, plugins, and more for quick integration with AWS services, including hardware-based solutions</li> <li>Technical guidance and documentation for solution development</li> </ol> <p></p> <p> The outcome of successfully completing the Design and Build Hub is a validated solution ready for launch and Go-to-Market support. There are five phases in the Design and Build Hub, which are designed to be completed sequentially:</p> <ul> <li> <p>Phase 0 is Build Engagement and Initiation</p> </li> <li> <p>Phase 1 is Design</p> </li> <li> <p>Phase 2 is Build</p> </li> <li> <p>Phase 3 is Validation</p> </li> <li> <p>Phase 4 is Publishing</p> </li> </ul>"},{"location":"getting_started/playbook/design_and_build_hub/index.html#design-and-build-hub-assets","title":"Design and Build Hub Assets","text":"<p>Solution Building Guidance is designed to allow partners to develop solution offerings on their own schedule. The time required to complete each phase will vary depending upon the partner, solution scope and complexity, funding, and business priorities.</p> Phase Description Assets 0 Build Engagement and Initiation N/A 1 Design Design Workshop example agenda  Cloudscape Figma Component Library  2 Build Partner Solution Technology Team (PSTT) Asset Repository   draw.io 3 Validation Foundational Technical Review  Well-Architected Tool 4 Publishing / Delivery Guidance on publishing to AWS Marketplace (if applicable)"},{"location":"getting_started/playbook/design_and_build_hub/phase_0.html","title":"Phase 0: Build Engagement and Initiation","text":"<p>Phase 0 is the starting point for the Build Engagement and Initiation Hub. The purpose of this phase is to connect the business team to the technical team supporting the solution build and to align on the current vision for the solution.</p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_0.html#to-do","title":"To do","text":"<ul> <li> Schedule meeting with build team to review PR/FAQ and discuss expectations, meeting cadence, and timelines</li> </ul> <p>Once you have completed Phase 0, you may proceed to Phase 1.</p> <p></p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_1.html","title":"Phase 1: Design","text":"<p>Phase 1 is dedicated to designing a proposed solution architecture diagram and technical resources (i.e., AWS services and others) for build.</p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_1.html#to-do","title":"To do","text":"<ul> <li> Conduct a Design Workshop</li> <li> Design solution wireframes</li> <li> Review related resources in Partner Solution Technology Team (PSTT) Build Repository</li> <li> Design a proposed solution architecture diagram</li> </ul>"},{"location":"getting_started/playbook/design_and_build_hub/phase_1.html#assets","title":"Assets","text":""},{"location":"getting_started/playbook/design_and_build_hub/phase_1.html#design-workshop-example-agenda","title":"Design Workshop example agenda","text":"<p>Download the Design Workshop example agenda here.</p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_1.html#cloudscape-figma-component-library","title":"Cloudscape Figma Component Library","text":"<p>Access the Cloudscape Figma Component Library here.</p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_1.html#pstt-build-repository","title":"PSTT Build Repository","text":"<p>Review Glossary of available resources here and then browse or search the repo by industry, use case, or technology.</p> <p></p> <p>Once you have completed Phase 1, you may proceed to Phase 2.</p> <p></p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_2.html","title":"Phase 2: Build","text":"<p>Phase 2 is dedicated to finalizing the solution architecture diagram, building the solution, and developing a solution user guide (including solution deployment approach).</p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_2.html#to-do","title":"To do","text":"<ul> <li> Finalize solution architecture diagram</li> <li> Complete solution build</li> <li> Run solution demo with immediate business and technical teams and executive sponsors</li> <li> Develop solution user guide (including deployment approach)</li> </ul>"},{"location":"getting_started/playbook/design_and_build_hub/phase_2.html#assets","title":"Assets","text":""},{"location":"getting_started/playbook/design_and_build_hub/phase_2.html#pstt-build-asset-repository","title":"PSTT Build Asset Repository","text":"<p>Identify and use relevant PSTT resources here for your solution.</p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_2.html#drawio","title":"draw.io","text":"<p>draw.io is one of many tools you can use for developing a reference architecture diagram. It includes a library of AWS service icons. Access the browser version here or download the Desktop app here.</p> <p></p> <p>Once you have completed Phase 2, you may proceed to Phase 3.</p> <p></p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_3.html","title":"Phase 3: Validation","text":"<p>Phase 3 is designed to ensure that a solution and its components meet a quality bar that delivers a consistent customer experience. At the end of this phase, the solution and its components have completed all required validations. This enables publishing in the discovery tools by meeting the required quality bars.</p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_3.html#foundational-technical-review-ftr","title":"Foundational Technical Review (FTR)","text":"<p>FTRs help identify and mitigate technical risks for AWS Partners and the AWS Partner Network on behalf of AWS customers. The FTR focuses on a subset of AWS Well-Architected best practices and defines each requirement's objective pass/fail criteria. This helps AWS Partners prioritize implementing the controls critical to customer success and creates a standard bar that can be applied to thousands of partners consistently.</p> <p>The FTR process evaluates a partner offering against a Validation Checklist (VCL). If issues are identified, a report is sent detailing the necessary remediations. Once there are no outstanding issues, the FTR is approved. For more details on the review, please see the FTR process page.</p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_3.html#ftr-vs-well-architected-framework-wafr","title":"FTR vs. Well-Architected Framework (WAFR)","text":"<p>The FTR differs from the WAFR in two key ways:</p> <ul> <li>Its scope is significantly narrower, focusing on a subset of best practices from the Security, Reliability, and Operational Excellence pillars</li> <li>It provides a much more specific and objective standard for evaluation</li> </ul> <p>This helps scale the FTR so we can provide a consistent validation experience for thousands or even tens of thousands of Partner offerings through a self-service modality. The FTR is not intended to be a replacement for a full WAFR. We require partners to conduct a self-review across the entire Well-Architected Framework as part of completing a partner-hosted FTR.</p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_3.html#aws-solution-quality-bar","title":"AWS Solution Quality Bar","text":"<p>Managed by WWSO, the AWS Solution Library is the central external repository for solutions with &gt;200 solutions spanning a range of technology and industry use cases. A solution addresses a customer use case on AWS through a combination of technical assets (e.g., application code, managed software, reference architecture) supported by consulting assets and services (for example, design, build, and support). Solutions automate and reduce the number of decisions customers need to make to address their business and technical use cases on AWS, given the breadth and depth of services and partner offerings and various ways these can be put together.</p> <p>The AWS Solution Library requires that AWS Partner Solutions meet the AWS Solution Quality Bar.</p> <p>Note: AWS Solution Library currently only supports the listing of Software based solutions. The listing of Solutions delivered by an SI/GSI as a lead partner is currently not supported.</p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_3.html#to-do","title":"To do","text":"<ul> <li> Complete a self-service AWS Foundational Technical Review (FTR)</li> <li> Evaluate the solution again AWS's Well-Architected Framework using the self-service Well-Architected Tool</li> </ul>"},{"location":"getting_started/playbook/design_and_build_hub/phase_3.html#assets","title":"Assets","text":""},{"location":"getting_started/playbook/design_and_build_hub/phase_3.html#foundational-technical-review","title":"Foundational Technical Review","text":"<p>Complete a self-service AWS Foundational Technical Review (FTR) here.</p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_3.html#well-architected-tool","title":"Well-Architected Tool","text":"<p>Evaluate the solution again AWS's Well-Architected Framework using the self-service Well-Architected Tool here.</p> <p></p> <p>Once you have completed Phase 3, you may proceed to Phase 4.</p> <p></p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_4.html","title":"Phase 4: Publishing / Delivery","text":"<p>Publishing triggers the publication of the Solution in the AWS Partner Discovery Tools, AWS Solution Library, and AWS Marketplace (optional) to allow customers to discover and purchase Solutions. Publishing will be initiated through the GTM and Sales Hub based on solution launch readiness.</p> <p>Depending on the composition of the Solution, it will be published as a Software listing (SaaS) or as a Services listing. Solutions that are composed of one or multiple offerings from one or multiple partners will be published as a Services listing with a designated lead partner (typically GSI/SI) to provide customers with a single point of contact/ownership for the Solution.</p> <p></p> <p>At the end of this phase, the Solution has been published in the AWS Partner Discovery Tools, AWS Solution Library, and/or AWS Marketplace (optional).</p>"},{"location":"getting_started/playbook/design_and_build_hub/phase_4.html#to-do","title":"To do","text":"<ul> <li> Publish solution or deliver to customer</li> </ul>"},{"location":"getting_started/playbook/design_and_build_hub/phase_4.html#assets","title":"Assets","text":""},{"location":"getting_started/playbook/design_and_build_hub/phase_4.html#guidance-on-publishing-to-aws-marketplace","title":"Guidance on publishing to AWS Marketplace","text":"<p>Learn how to publishing your solution to AWS Marketplace here.</p> <p></p> <p>Once you have completed Phase 4, you may proceed to the Design and Build Hub.</p> <p></p>"},{"location":"getting_started/playbook/gtm_and_sales_hub/index.html","title":"Go-to-Market and Sales Hub","text":"<p>This GTM and Sales Hub, Playbook, and the supporting checklist, map out activities along the GTM life cycle of a solution. Each section includes steps and instructions to maximize your GTM and Sales efforts. </p> <p>A GTM strategy includes all the activities to introduce a new solution into the market. The GTM and Sales Hub motions include all the activities executed to create customer demand and sales opportunities.</p> <p>The outcome of successfully completing the GTM and Sales Hub is a launched solution that is being marketed, sold, and tracked for performance. There are three phases in the GTM and Sales Hub, which are designed to be completed sequentially:</p> <ul> <li> <p>Pre-Launch includes all the work to get your solution ready for market</p> </li> <li> <p>Launch means you are in market! Celebrate the milestone with a big marketing push to make sure potential customers can learn about the solution offering.</p> </li> <li> <p>Post-Launch is for all the motions to grow once your solution has launched</p> </li> </ul> <p>The steps in the Hub span three main focus areas:</p> <ol> <li>Marketing: Content planning and preparation for solution launch, including market/customer identification, messaging, asset creation (content, blogs, case studies, FCD, webinar), marketing and launch plan, announcement, goal and KPI setting and tracking, and sales enablement.</li> <li>Go-to-Market: Execution of the marketing and launch plans, demand generation activities, customer success, account-based marketing, and events.</li> <li>Selling: Execution of Account plans with jointly identified customers, nurturing of PSQLs, outreach to key customer profiles, and opportunity creation along with performance tracking.</li> </ol>"},{"location":"getting_started/playbook/gtm_and_sales_hub/index.html#gtm-and-sales-hub-assets","title":"GTM and Sales Hub Assets","text":"Phase Description Assets 1 Pre-Launch GTM Packages and Sales Enablement guidance  Field Ready Kits  Press Release/Blog template 2 Launch Solution demos, webinars, and events examples 3 Post-Launch QBRs template  Customer Feedback collection template"},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_1.html","title":"Phase 1: Pre-Launch","text":"<p>Pre-Launch is the starting point for the GTM and Sales Hub. This phase is dedicated to completing all the work required to get your solution ready for market.</p>"},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_1.html#to-do","title":"To do","text":"<ul> <li> Complete Solution Readiness activities (see table below)</li> <li> Complete Planning activities (see table below)</li> <li> Complete Asset Creation activities (see table below)</li> <li> Complete Sales Enablement activities (see table below)</li> <li> Complete Account Mapping activities (see table below)</li> </ul>"},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_1.html#solution-readiness","title":"Solution Readiness","text":"Task Description Instructions GTM Kickoff Meeting The GTM Kickoff acts as the launching point for defining the GTM strategy and the GTM planning to align with your business development, sales, and marketing teams. This meeting will focus on how you currently drive GTM and sales for other products or solutions. Key objectives of the GTM kick-off meeting are:  1. Establish understanding of the business, goals, and priorities.  2. Set goal targets.  3. Identify key stakeholders to be involved in the creation and execution of a GTM plan.  4. Review the GTM plan outline and set expectations for the required inputs and expected outputs.  5. Set the timeline for plan creation. Determine dates and participants for a GTM Workshop. GTM Workshop The GTM Workshop results in the creation of the GTM plan. The GTM workshop can be 1 session or multiple, based on availability. The workshop should include stakeholders and marketing teams, as appropriate. The workshop will cover:  1. Current GTM model of the partner: this includes target market, buyer persona, sales team structure and incentives, and sales process/cycle.  2. Sales and Marketing Collateral: existing materials, and gap analysis.  3. GTM Focus Areas and Campaigns: planned campaigns, sales motions, and marketing plan.  4. Governance and Tracking: tracking mechanisms, meeting cadences. Target Customer Personas and Market Segments Define Customer Personas and Market Segments (if not already completed in Ideate Hub). Determine the addressable market, target customer, and industries, geographies, and specific accounts for piloting the product. Define Product Position and Value Prop Product positioning and use case best practices. What problem does the solution address, and what are the use cases? Define Product Position and Value Prop Define sales personas and channels What is the value prop and how will it be enabled?"},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_1.html#planning","title":"Planning","text":"<p>Proper planning helps set our solution goals, ensures expectations are aligned, and assigns clear owners for each marketing and sales task. These plans are developed based on the GTM workshop and Kickoff output. Plans are living documents that should be referred to often and updated as we gather more customer data, as market conditions evolve, and as we expand offerings into new markets.</p> Task Description Instructions Marketing Plan Create marketing plan based on marketing kickoff, customer, market segment, etc., that align to KPIs set out for solution. Determine the GTM team, engagement model. Solution Discovery Plan Describe how product/solution will be discovered. Sales/product collateral repositories for internal and external information. GTM Objectives and Stakeholders At a glance overview of objectives, stakeholders, and tracking. With the GTM plan identify top objectives, stakeholders, and role and responsibilities. GTM Plan Map out holistic GTM approach. For each sales persona, create an enablement plan including how will they made aware of the product and sales strategy. This plan is created as a result of the GTM workshop. Message testing with pilot/MLP customers Message testing: messaging workshop all GTM stakeholders. Customers can help us understand the strength of our messaging. This outreach to pilot customers allows us to adapt messaging before reaching a broader audience at launch. Schedule messaging survey discussions with pilot customers, as available."},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_1.html#asset-creation","title":"Asset Creation","text":"<p>Assets are used to engage with customers/prospects, sellers, and prospective partners that might be able to collaborate on solution enhancements. The partner may have existing assets that should be cataloged and reviewed for reuse. This is not an exhaustive list, yet not all assets are required. Assets can also be added over time; this is especially valuable as new customer successes are released.</p> <p>To ensure AWS and partner sellers have the tools to take a solution to market, the Field Ready Kit (FRK) should be built for each solution.</p> <p>The FRK includes:</p> <ol> <li>Sales Brief: Two-page document for AWS account teams to quickly understand the partner's core value proposition to AWS customers and AWS.</li> <li>Solution Brief: Two-page customer-facing resource that is the partner's tool to motivate and inspire customers to purchase or seek more information regarding their offering on AWS.</li> <li>Sales Deck: Presentation designed to educate the AWS account teams on the partner solution's core value proposition to AWS customers and AWS, common sales scenarios and use cases, target customer market segment(s) and buyer roles, sales cycle, customer success, deployment options, and call-to-action content and resources.</li> <li>First Call Deck: Presentation used to articulate the value AWS customers can derive by leveraging your solution.</li> </ol> <p></p> Task Description Instructions First Call Deck Presentation used to articulate the value AWS customers can derive by leveraging the partner\u2019s solution. It includes information to introduce the partner's offerings on AWS, including a solution overview, industry trends, customers\u2019 business needs, core business scenarios, AWS + partner alignment, case studies, and calls-to-action, including a technical addendum with more technical depth, including reference architecture, if applicable. It can be used as a quick reference by the AWS account teams or as a tool to help guide them through a first call with a potential customer. Create customer-facing version. Sales Brief Two-page document for AWS account teams to quickly understand the partner's core value proposition to AWS customers and AWS, common sales scenarios and use cases, target customer market segment(s) and buyer roles, similar solutions in the market, customer success, strategic partner benefits, deployment options, and available resources to use when they've identified a potential co-selling opportunity. Create 2 pager summarizing the customer use cases, solution value prop, and commercial model to be used by APO and Global Services sellers. Solution Brief A two-page customer-facing resource that is a partner's tool to motivate and inspire customers to purchase or seek more information regarding their offering on AWS. It should remind the customer of what was discussed or presented to them. This is not a data sheet highlighting the technical aspects or features that make the solution work. This is the \u201cwhy,\u201d with only a cursory mention of \u201chow.\u201d They will likely need deeper content, which they can follow up with later. Create leave-behind for the customer highlighting the product features. APN Blog Blog to highlight solution launch. Write blog with SM or other approved AWS blogger. Social Media Organic or paid social media promotion. Partner and PDM/PMM to create social media assets announcing solution. Video for APN TV Partner and customer video content to promote, if possible. Marketing Team (vPMM) to assist on APN TV requirements and production. Webinar live or on demand Live or pre-recorded webinar to promote solution. Record or schedule live webinar broadcast to drive demand and awareness of LoB solution. Solution Demo Virtual or physical demo to showcase solution attributes. Create demo. eBook eBook highlighting solution and value prop/key messages. Partner (and/or AWS) to create eBook on solution. Pilot case studies or references Customer successes Complete case studies with end customers as a part of the sales pitches."},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_1.html#sales-enablement","title":"Sales Enablement","text":"<p>It is important to discuss enablement and add any steps that might accelerate understanding of the solution and the value of being built with Solution Building Guidance and on AWS.</p> Task Description Instructions Demo videos Pre-recorded demos of the solution for Sellers. Create demo videos for inclusion at events, website, etc. Coordinate Sales and Delivery Training Structured learning paths based on audience. Pre-requisite: Use case is \"published\" and prioritized for field GTM through Alchemy, and solution messaging has been reviewed/approved by your IMM/PMM. Launch Announcement Launch Announcement for AWS Sellers. Partner and PDM to create Launch announcement including one minute webinar, customer story, key messages."},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_1.html#account-mapping","title":"Account Mapping","text":"<p>Co-selling, or sharing and collaborating on leads and opportunities, increases the number of leads/opportunities shared between AWS and a partner. Co-selling fosters collaboration across teams, communicating the combined value to the customer, providing better customer outcomes, and assuring mutual commitment from AWS and Partners. Co-selling accelerates when there is a qualified opportunity for AWS and the partner to work together. AWS and partner combine demand generation efforts to create leads that qualify into opportunities.</p> <p>Once appropriate AWS Sellers are identified (by industry, region, or service family), a PDM can organize Account Mapping with the AWS Seller team. This will help coordinate outreach across a targeted list of accounts, create a plan of outreach between the partner and AWS account teams (PSM/ISM/AMs), and build a pipeline that is reviewed regularly to provide governance, visibility, coaching, alignment, remove blockers, allocate ownership, track GTM activities, build a new pipeline, and define next steps together as a team.</p> Task Description Instructions Account Mapping Plan Identify your account map with AWS Sellers. The following steps should be followed for account mapping:  1. Set up weekly or bi-weekly pipeline call.  2. Get targeted list of accounts from the partner.  3. Leverage data to break down the partner footprint within each of these identified accounts.  4. Map the partner account teams to the AWS sales team.  5. Identify the partner solutions that will be propositioned within each account.  6. Identify 10-30 accounts to target first.  7. Build out broader campaigns (by footprint) to help start the conversations within the list of target accounts.  8. Tag opportunities with a campaign code. Partner Account Plan Build out Partner Account Plan for PO opportunity growth. PDM to map out Partner Account Plan with partner. Pipeline Review Regularly scheduled review of pipeline. A pipeline review call aims to provide governance, visibility, coaching, alignment, remove blockers, allocate ownership, track GTM activities, build a new pipeline, and define the next steps with a partner.  This call should be held weekly or bi-weekly with the partner and PDM. Sales and alliance resources should be invited to these discussions to attend as possible."},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_1.html#pre-launch-outcomes","title":"Pre-Launch Outcomes","text":"<p>Once this Phase of work has been completed, you are ready to announce the availability of your Solution Building Guidance solution. Sellers are already driving opportunity creation with the targeted list of accounts. Your partner has a holistic marketing and GTM strategy for the solution. A comprehensive set of marketing assets supports this to drive interest in the solution.</p> <p></p> <p>Once you have completed Phase 1, you may proceed to Phase 2.</p> <p></p>"},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_2.html","title":"Phase 2: Launch","text":"<p>Launch is a time for us to celebrate all the work we have done so far and get in front of customers and prospects publicly to share the transformation potential of the partner solution. Execution is the key at this Phase. Assuming our content, messaging, and strategy in the Pre-Launch phase were correct, this is where we make fine-tuned adjustments based on feedback from a broader set of customers. These steps in this phase focus on the announcement itself, execution of plans, and a 60-day milestone to confirm that our assumptions from Pre-Launch are playing out as expected.</p>"},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_2.html#to-do","title":"To do","text":"<ul> <li> Complete Listing activities (see table below)</li> <li> Complete Announcement activities (see table below)</li> <li> Complete Marketing Execution activities (see table below)</li> <li> Complete Sales Execution activities (see table below)</li> </ul>"},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_2.html#listing","title":"Listing","text":"<p>Every solution will have a different listing path based on how to get the solution into customers\u2019 hands best. This could be on a partner\u2019s discovery channels, the AWS Marketplace, or through a distributor channel. Below are the AWS listing sites:</p> <ul> <li>AWS Marketplace, which requires Marketplace validation for listing</li> <li>AWS Solution Library, which requires a FTR and the creation of a GTM package (at a minimum, industry requirements vary)</li> <li>Partner Solution Finder, which lists solutions from partners above Registered status</li> </ul> <p></p> Task Description Instructions Marketplace Listing AWS Marketplace is a curated digital catalog for customers to find, buy, deploy, and manage third-party software, data, and services to build solutions and run their businesses. AWS Marketplace includes thousands of software listings from popular categories such as security, business applications, machine learning, and data products across specific industries, such as healthcare, financial services, and telecommunications. Customers can quickly launch preconfigured software and choose software solutions in AMIs, SaaS, and other formats. If you want your industry solution listed on this page for effective discovery, you can fill out this MP website update request form. Library Listing An AWS website providing an improved solution content discovery experience for customers - both builder and buyer personas. The AWS Solutions Library is the central external repository for solutions, with &gt;200 solutions spanning a range of technology and industry use cases. Note that this is a curated list of solutions, where entry to the catalog is managed by various internal stakeholders across different solution domains and areas, who define the prioritized use cases and need for solutions across the different solution response types. PSM and SM to connect with WWSO counterpart to help with publishing this to the AWS Solutions Library. Partner Solution Finder Partners can list validated offerings in Partner Solution Finder. This section will outline the path to publication for partners and their offerings within Partner Solution Finder. All steps are in Partner Central: User Guide for APN Partners. Partners Requirements: Partners must meet either one of the listed criteria to be published to Solution Building Guidance:  - Select + Tier Status  - Confirmed in ISV Partner Pathways  Offerings Prerequisites: Partners must first have their profile published to Solution Building Guidance for any offerings to be published. If a partner does not meet the criteria outlined in the \"Partners\" section above, their respective offerings will not be published."},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_2.html#announcement","title":"Announcement","text":"<p>Launch is supported by outbound announcements to draw attention to and drive momentum for a Solution Building Guidance solution. Marketing should be aligned to a multi-media announcement approach for maximum impact. Below are the suggested announcement vehicles that can complement launch events or stand alone to drive demand for a newly launched solution.</p> Task Description Instructions Press Release Announcement of Solution (Partner led only) PDM to submit partner led press releases for AWS review and approval before submission. Launch Blog Blog announcement of solution PDM and SM to work with partner on Blog announcement on AWS or partner channels. Launch Webinar Internal and/or external webinar Team to determine whether a webinar is part of the marketing plan. AWS Launch Announcement Internal announcement to relevant teams PDM and SM to create an internal announcement of the solution launch."},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_2.html#marketing-execution","title":"Marketing Execution","text":"<p>Once you have worked with your partner to launch their Solution Building Guidance solution, it\u2019s important to put that story to the test for the customers to see and hear through planned events, campaigns, and activities. The marketing plan built in Pre-Launch should include all activities that are carried out to achieve awareness, generate demand and leads and fill the pipeline with opportunities. This Phase executes the marketing plan to ensure the steps are accomplishing the plan.</p> Task Description Instructions Summits and Events Participation in summit and events, as appropriate. PDM and SM to identify key 1P and 3P events to participate in. Execute Marketing Campaign Execution of campaign components mapped out in marketing plan. Team to execute agreed upon plan steps."},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_2.html#sales-execution","title":"Sales Execution","text":"<p>All of the activities leading up to deal close/launch, which include lead generation, lead nurturing, AWS and partner seller outreach, account-based marketing, and more, fall into sales execution.</p> Task Description Instructions ACE opportunity submission ACE opportunity tracking is our key metric for sales success. PDM to ensure the partner submits every op in ace with proper tagging and responds within 2 weeks SLA for proper reporting. Demand generation campaign Partner-led demand generation campaigns. Partner to work with PDM to create DG campaign with AWS support. Joint Account Based marketing plan execution Marketing Plan steps aligned to plan to be executed. PDM, SM, and partner to execute on agreed upon plan and track progress regularly. 60-day perfor-mance milestone Milestone check to ensure the solution is selling/getting awareness as expected. PDM and SM to track and report out on KPIs in QBR."},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_2.html#launch-outcomes","title":"Launch Outcomes","text":"<p>Once this Phase has been completed, the stakeholders should regroup to discuss a 60-day post-launch review. A key piece of Solution Building Guidance is that we want to build solutions that don\u2019t sit on the shelf. If solutions are not driving the expected customer interest, this is an excellent time to reassess and reevaluate our progress. If improvements are needed, pause ongoing Post-Launch work, make course corrections, and retest progress.</p> <p></p> <p>Once you have completed Phase 2, you may proceed to Phase 3.</p> <p></p>"},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_3.html","title":"Phase 3: Post-Launch","text":"<p>Post-Launch, we will continue expanding assets, adding customer references, and submitting to industry campaigns to grow awareness of the solution.</p>"},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_3.html#to-do","title":"To do","text":"<ul> <li> Complete Ongoing Marketing activities (see table below)</li> <li> Complete Reporting activities (see table below)</li> <li> Complete Customer Success activities (see table below)</li> <li> Complete Selling activities (see table below)</li> </ul>"},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_3.html#ongoing-marketing","title":"Ongoing Marketing","text":"<p>Post-launch, we will continue expanding assets, adding customer references, and submitting to industry campaigns to grow awareness of the solution.  </p> Task Description Instructions Customer Events - Webinars Work with marketing team to setup webinars and marketing events. Partner, PDM, and SM to execute on paid, partner-led, or AWS-led customer-facing webinars, per marketing plan. Support Partner Campaigns Resources available to Partners to create marketing campaigns. Partner, PDM, and SM to execute on paid, partner-led, or AWS-led customer-facing campaigns, per marketing plan."},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_3.html#reporting","title":"Reporting","text":"<p>To ensure solutions are used, we will report quarterly and adjust based on challenges and blockers escalated in those meetings amongst AWS and partner stakeholders. We will track opportunities against all our Solution Building Guidance output goals and the key performance indicators important to the partner, industry teams, or AWS Sellers that will help grow the business over the life of the solution.</p> Task Description Instructions Opportunity Report SFDC Opportunity Report tied to Solution Building Guidance Campaign Code. Partner to work with PDx to create Opportunity Report in SFDC. KPIs Key Performance indicators identified in planning to track to ensure solutions do not sit on the shelf. Partner to track and report out on KPIs in QBR."},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_3.html#customer-success","title":"Customer Success","text":"<p>Line-of-Business customers will want to hear from existing customers, and AWS Sellers want to have confidence that other customers are using a solution and that it works as expected. Customers are the ideal marketing asset for campaigns. Below are instructions on creating customer case studies, and success stories.</p> Task Description Instructions Case Studies Having a customer story that speaks to the scenario, problem, solution, benefits to the customer, and any cost savings is critical in validating the solution and earning trust with our AWS Sellers. Find details on how to submit a formal partner case study to be published on the AWS Partner Solution Finder can be found on the APN Case Study Wiki."},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_3.html#selling","title":"Selling","text":"<p>Sales alignment will continue to meet the needs of customers throughout the life of a solution. The PDM and partner should continue to reach new targets based on continued customer feedback.</p> Task Description Instructions Continued Sales Alignment Developing sales campaigns helps us work better with our partners to create new opportunities and build pipelines. It focuses the sales activities on a defined scope and a concrete goal and provides supporting instruction and collateral. Leveraging all the AWS resources available from an industry, vertical, or workload perspective is essential when developing a sales play or campaign. Sales plays/campaigns will differ between Consulting, Technology, Distribution, and ISV partners, and the AWS teams that should be engaged will also be different. Working with the AWS Strategic BD team will be imperative when creating an industry-specific campaign. In addition to working with our AWS industry and workload specialist teams, alignment with segment teams (i.e., SMB Partner Team or Global Segment ISV Team) will be important to incorporate into a campaign depending upon the type of partner that you manage (Technology, Consulting, or Distribution Partner). Alignment between the technical, specialist, segment, marketing, and business-minded teams is essential to the success of any campaign."},{"location":"getting_started/playbook/gtm_and_sales_hub/phase_3.html#post-launch-outcomes","title":"Post-Launch Outcomes","text":"<p>The outcome of this Phase is the end of life or relaunching of a solution, as GTM and Selling will continue throughout the life of a solution. Post Launch motions will overlap with Solution Lifecycle Management (SLM) and we can adjust our marketing and GTM efforts according to the feedback through our reporting mechanisms in the SLM Hub.</p> <p></p> <p>Once you have completed Phase 3, you may proceed to the Solution Lifecycle Management Hub.</p> <p></p>"},{"location":"getting_started/playbook/ideate_hub/index.html","title":"Ideate Hub Overview","text":"<p>The Ideate Hub is the first of four solution development Hubs. In this Hub, we gather and evaluate customer insights to better understand customer needs and innovate ideas to solve customer challenges. Through the ideation process, we create the vision for the solution and map the end to end customer experience. In the final stages of ideation, the proposed solution is tested, refined and the definition and metrics of success are developed.</p> <p></p> <p>The goal of the Ideate Hub is to obtain the right information needed at the beginning of the solution development process in order to make the most optimized decisions across the solution development stages. Our process of Working Backwards from the customer helps to create the foundation for innovative solution ideation and is Amazon\u2019s process of discovery and invention that delivers successful customer business outcomes. The Ideate Hub reduces costly rework and saves time in subsequent solution development stages because we design with the customer in mind from the beginning, creating a solid foundation on which Solution Building Guidance builds a business outcome solution reducing the time to value and time to market for our customers. Ideate Hub Self-Service is designed to allow partners to develop solution offerings on their own schedule. The time required to complete each phase will vary depending upon the partner, solution scope and complexity, funding, and business priorities.</p>"},{"location":"getting_started/playbook/ideate_hub/index.html#the-ideate-hub-experience","title":"The Ideate Hub Experience","text":"<p>In the Ideate Hub, partners will be introduced to a foundation of successful solution ideation, including how customer obsession drives innovation and actual use case application. After completing the foundational learning series, partners will experience \u201cWorking Backwards from your Customer\u201d to begin the process of solution ideation by answering the 5 Customer Questions and completing the solution Press Release and Frequently Asked Questions document (PR FAQ).  The working backwards exercise can be completed on your own or with your team. Partners who complete the Working Backwards exercise can also sign up to be invited to an AWS Partner Matchmaking event and review solution funding options and resources.</p> <p></p>"},{"location":"getting_started/playbook/ideate_hub/index.html#milestones","title":"Milestones","text":"<ul> <li>WB - 5 Customer Questions</li> <li>PR/FAQ</li> <li>Funding</li> </ul>"},{"location":"getting_started/playbook/ideate_hub/index.html#unlocked-benefits","title":"Unlocked Benefits","text":"<ul> <li>Sandbox and POC credits</li> <li>Partner Matchmaking</li> <li>Funding Resources</li> </ul>"},{"location":"getting_started/playbook/ideate_hub/index.html#ideate-hub-learning-path","title":"Ideate Hub Learning Path","text":"<ol> <li>Solution Innovation</li> <li>Intro Working Backwards</li> <li>Working Backwards and 5Q</li> <li>PR FAQ</li> <li>Partner Matchmaking</li> <li>Funding</li> </ol>"},{"location":"getting_started/playbook/ideate_hub/index.html#explore","title":"Explore","text":"<ul> <li> <p>Explore Industries &amp; Use Cases</p> </li> <li> <p>Explore AWS Specialized Partners</p> </li> <li> <p>Explore AWS Partner Device Catalog</p> </li> <li> <p>Explore Solution Funding Options</p> </li> </ul>"},{"location":"getting_started/playbook/ideate_hub/index.html#ideate-assets-resources","title":"Ideate Assets &amp; Resources","text":"<ul> <li>Working Backwards Exercise Template</li> <li>PR FAQ Template</li> <li>Customer Journey Map</li> <li>Customer Vignette Template</li> <li>Funding Options</li> </ul> <p>Next in Series: Digital Innovation.</p> <p></p>"},{"location":"getting_started/playbook/ideate_hub/phase_0.html","title":"Ideate Hub: Digital Innovation","text":""},{"location":"getting_started/playbook/ideate_hub/phase_0.html#to-do","title":"To Do","text":"<ol> <li> <p>Learn - AWS Culture of Innovation</p> <p></p> <p>Video: AWS Culture of Innovation  (23 mins)</p> <p>Amazon\u2019s approach to innovation - start with the customer and work backwards.\u00a0In this video, be introduced to Amazon\u2019s peculiar culture and how we innovate through four distinct yet interdependent elements: Culture, Mechanisms, Architecture, and Organization.</p> <p></p> </li> <li> <p>Learn - Introduction to Working Backwards from the Customer</p> <p></p> <p>Video: Introduction to Working Backwards (17 mins)</p> <p>Answer the 5 Customer Questions to develop a customer-obsessed idea and kick-start your Press Release &amp; Frequently Asked Questions,(PR/FAQs) using Amazon's Working Backwards methodology.</p> <p></p> </li> <li> <p>Download the Working Backwards Exercise Template</p> </li> </ol> <p></p> <p>Next in Series: Working Backwards and 5 Customer Questions</p> <p></p>"},{"location":"getting_started/playbook/ideate_hub/phase_1.html","title":"Ideate Hub - Working Backwards and 5 Customer Questions","text":""},{"location":"getting_started/playbook/ideate_hub/phase_1.html#working-backwards-5-stages-aligned-to-5-customer-questions","title":"Working Backwards: 5 Stages Aligned to 5 Customer Questions","text":"<p>Working Backwards is Amazon\u2019s process of discovery and invention that delivers customer delight. Working Backwards is how we stay customer obsessed \u2014 we work backwards from the moment that the product, service or experience is in a customer\u2019s hands. The Working Backwards framework can be used for big and small inventions alike, and for external or internal customers.</p> <p>Working Backwards involves five stages: Listen, Define, Invent, Refine, and Test &amp; Iterate. Each Working Backwards stage aligns to the 5 Customer Questions and a set of recommended actions. Discover the definitions, importance, and what to do in each stage below.</p> <p>Download the Working Backwards Exercise Template to get started.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Next in Series: PR FAQ</p> <p></p>"},{"location":"getting_started/playbook/ideate_hub/phase_2.html","title":"Ideate Hub - Press Release and Frequently Asked Questions (PR FAQ)","text":"<p>Writing a Press Release and FAQ</p> <p>With the Press Release, you leap into the future and imagine how you want a customer to feel and what you want them to say when they experience the product, feature or service you want to build.  When you write your document, imagine that your customer is going to read it. It\u2019s a one-page narrative explaining your vision using customer-centric language.</p> <ol> <li> <p>Download the PR FAQ Template to get started</p> </li> <li> <p>Below are the topics covered in the PR FAQ Template </p> </li> </ol> Topic Page Tips for Writing the Press Release 1 Tips for the Writing Process and Collaboration 1 - 2 Quick Start Press Release Outline 3 Quick Start FAQ Outline 4 Press Release Document Template 5 <p></p> <p>Next in Series: Partner Matchmaking</p> <p></p>"},{"location":"getting_started/playbook/ideate_hub/phase_3.html","title":"Ideate Hub - Partner Matchmaking","text":""},{"location":"getting_started/playbook/ideate_hub/phase_3.html#aws-partner-matchmaking","title":"AWS Partner Matchmaking","text":"<p>Sign up for an invitation to join a Solution Ideation and Partner Matchmaking Workshop presented by the AWS Solution Building Guidance team.</p> <p>Partners can expect to learn more about joint solution development innovation and best practices from AWS and our solution partners and will have an exclusive opportunity to network with members of the AWS partner community and engage in a solution development and partner matchmaking workshop with different partner types.</p> <p>You'll also learn about how AWS Solution Building Guidance provides an end to end repeatable framework and solution development playbook for joint solution building best practices to create packaged solution offerings that deliver business outcomes for buyer customers.</p> <ul> <li> Sign up for  AWS Partner Matchmaking - network with AWS solution partners and participate in a joint solution development workshop</li> </ul> <p></p> <p>Next in Series: Explore Funding Options</p> <p></p>"},{"location":"getting_started/playbook/ideate_hub/phase_4.html","title":"Ideate Hub - Funding","text":"<p>In progress...</p> <p></p> <p>Next in Series: Explore Funding Options</p> <p></p>"},{"location":"getting_started/playbook/slm_hub/index.html","title":"Solution Lifecycle Management Hub","text":"<p>Solution lifecycle management spans across Build and GTM/Sales Hubs and is active for the entire lifespan of a solution. This ensures that the solution stays relevant through its shelf life. It provides essential opportunities to re-evaluate the impact of prior Post Launch activities and assess new activities. A very important aspect is an active feedback loop from customers, partners, and AWS to drive continuous improvement as it matures in the market. It includes decisions to retire solutions that no longer drive value for and adoption by new customers.</p>"},{"location":"getting_started/playbook/slm_hub/index.html#build","title":"Build","text":"<p>The Build component of lifecycle management is an event-driven process managed through automated and manual mechanisms such as validation status and feedback received. For example, FTRs for a Solution and its components expire every 24 months. At that time, they will go through the FTR process for revalidation again. This ensures that potentially new risks can be identified and remediated, as necessary. AWS Partners can track the validation status of their offerings and solutions in their Partner Scorecard in AWS Partner Central, including notification of upcoming expirations.</p> <p></p> <p>Note: The required revalidation cycle of a solution can also be leveraged as a force function for a lifecycle checkpoint by regrouping the partner, PDM, and SM to evaluate the performance of a solution against its customer-driven KPIs.</p>"},{"location":"getting_started/playbook/slm_hub/index.html#gtm-and-sales","title":"GTM and Sales","text":"<p>The GTM/Sales component of the solution lifecycle management consists of the ongoing Post Launch activities (reporting, ongoing marketing, customer success, selling) throughout the lifespan of the solution.</p> <p></p>"},{"location":"getting_started/playbook/slm_hub/index.html#feedback","title":"Feedback","text":"Phase Description Assets Feedback Provide solution feedback to drive solution success PDM, SM, and other AWS stakeholders submit feedback through SIM intake."},{"location":"industries/index.html","title":"Technical Assets for Industries","text":"<p>Technical Assets for Industries is a curated collection of industry-specific building blocks. Each asset in this collection is tailored to the needs, challenges, and goals of a particular industry. They can be used independently or in conjunction with one another to build a new solution or to raise the quality bar of an existing solution.</p> <p>As technical assets are developed, they will be added here with documentation, examples, and tutorials.</p>"},{"location":"industries/automotive/index.html","title":"Automotive","text":""},{"location":"industries/automotive/index.html#pioneering-automotive-solutions","title":"Pioneering Automotive Solutions","text":"<p>At AWS, we are dedicated to providing purpose-built solutions tailored for the automotive industry. We've developed an extensive portfolio of automotive-specific services and solutions, encompassing software-defined vehicles, connected mobility, autonomous mobility, digital customer engagement, manufacturing, supply chain, and product engineering.</p>"},{"location":"industries/automotive/index.html#unparalleled-industry-expertise","title":"Unparalleled Industry Expertise","text":"<p>Our commitment to innovation has positioned AWS as the top choice in ABI Research's Connected Car Cloud Platform rankings. We empower automotive companies, from cutting-edge startups to industry-leading global OEMs, to harness the potential of data and create value across their operations.</p>"},{"location":"industries/automotive/index.html#expansive-automotive-ecosystem","title":"Expansive Automotive Ecosystem","text":"<p>We facilitate the automotive industry's digital transformation through an extensive suite of capabilities, including AI/ML, IoT, high-performance computing (HPC), and data lakes. Furthermore, we maintain a global network of Amazon and AWS Partners with profound expertise in the automotive sector, ensuring comprehensive support for your unique needs.</p> <p>Explore Automotive solutions in the AWS Solutions Library</p>"},{"location":"industries/automotive/accelerators/addf.html","title":"Autonomous Driving Data Framework (ADDF)","text":"<p>The Autonomous Driving Data Framework (ADDF) is a comprehensive and open-source solution designed to provide modular code artifacts for automotive teams seeking to implement common  tasks for advanced driver-assistance systems (ADAS).</p> <p>ADDF offers solutions for tasks like configuring centralized data storage, data processing pipelines, visualization mechanisms, search interfaces, simulation workloads, analytics interfaces, and prebuilt dashboards. Users can easily leverage, customize, or create modules to streamline deployment efforts.</p> <p>Architecture:  </p>"},{"location":"industries/automotive/accelerators/addf.html#how-to-use","title":"How to use","text":"<p>Source code : GitHub</p> <p>Documentation: PDF</p>"},{"location":"industries/automotive/accelerators/evbm.html","title":"Building an Electric Vehicle Battery Monitoring Solution with AWS IoT FleetWise","text":"<p>This solution makes it easier for automakers to collect, transform, and transfer vehicle data to the cloud specially for EV batteries. Once transferred, you can use the breadth and depth of AWS analytics and machine learning services to extract value from the vehicle data. If you have no previous experience with AWS IoT FleetWise, please consider starting with the blog Generating insights from vehicle data with AWS IoT FleetWise. You will learn about use cases, technical capabilities, and a logical architecture for AWS IoT FleetWise. In the same blog, an Electric Vehicle (EV) battery monitoring solution is introduced.</p>"},{"location":"industries/automotive/accelerators/evbm.html#use-cases-for-near-real-time-vehicle-data-processing","title":"Use Cases for Near-Real Time Vehicle Data Processing","text":"<ul> <li> <p>Vehicle Issue Prevention: Real-time vehicle data enables automakers and fleet operators to enhance driving experiences and improve vehicle quality. For instance, monitoring electric vehicle battery temperature helps detect rate of discharge, better route energy planning, fixing issues such as overheating, enabling quick analysis and root cause assessment, leading to immediate corrective actions.</p> </li> <li> <p>Optimization Loop for Advanced Driver Assistance Systems (ADAS): ADAS systems require continuous optimization and retraining of machine learning models. Access to real-time vehicle data enables this optimization process, reducing unknowns and enhancing safety.</p> </li> </ul> <p>For the most up-to-date information and resources, please refer to AWS IoT FleetWise Documentation.</p>"},{"location":"industries/automotive/accelerators/evbm.html#use-case","title":"Use Case","text":"<p>Lithium-ion batteries (Li-ion) are widely used as energy storage systems for EVs. When operating EV fleets, continuous monitoring and protection of battery cells is an important consideration.</p> <p>For EV manufacturers and fleet operators, the ability to detect and predict battery performance and issues such as overcurrent, overcharge, or overheating is crucial. This can improve the efficiency and safety of fleet operations by enabling timely planning for battery replacement. It also allows EV manufacturers to collaborate with battery suppliers on battery improvement initiatives by evaluating the conditions of the battery in different scenarios.</p> <p>The purpose of this accelerator is to assiss you get up and running with EV telematics jurney and will help you on how you can use AWS IoT FleetWise to collect and transfer Battery Management System (BMS) parameters to the cloud. For this purpose, we will use an example of overcurrent detection use-case. After the data is transferred, it will be stored in a database, ready to be used for monitoring, alarming, and ML model training.</p>"},{"location":"industries/automotive/accelerators/evbm.html#solution-architecture","title":"Solution Architecture","text":"<p>The following diagram shows the architecture of the solution</p> <p></p> <p>The following diagram shows the logical data flow of the solution </p>"},{"location":"industries/automotive/accelerators/evbm.html#how-to-use","title":"How to use","text":"<p>Source code: GitHub</p> <p>Blogs: Part 1, Part 2</p>"},{"location":"industries/automotive/accelerators/kvsvc.html","title":"Cloud-Based Driver Monitoring System (DMS)","text":"<p>In response to the growing concern over driver drowsiness and distraction, many countries mandated that vehicles sold within their jurisdiction must be equipped with Driver Monitoring Systems (DMS). To advance the development of cloud-based DMS solutions, a workshop has been organized that will introduce a reference solution cloud architecture and utilize Amazon Web Services (AWS).</p> <p></p>"},{"location":"industries/automotive/accelerators/kvsvc.html#workshop-highlights","title":"Workshop Highlights","text":"<ul> <li> <p>This workshop utilizes Amazon Kinesis Video Streams, enabling the building of low-latency video solutions for various applications, including smart surveillance cameras, robotics applications, and connected vehicles.</p> </li> <li> <p>Key Learning Objectives:</p> </li> <li>Live face recognition.</li> <li>Near real-time analysis using Amazon Rekognition Video.</li> <li> <p>Integration of connected cameras into AWS IoT service.</p> </li> <li> <p>Workflow Overview: In the workshop, a virtual camera mounted in the car's cabin records live video of both the driver (DMS) and passengers (OMS). This video feed is transmitted in real-time to a Kinesis Video Stream. The Rekognition service actively processes the video stream to identify faces, including that of the driver, and provides associated confidence scores. Additionally, it analyzes pitch, yaw, and roll angles to gauge the driver's attentiveness to the road (Gaze). In the event signs of drowsiness are detected, an alert is immediately activated, prompting the driver to consider taking a break or safely pulling over.</p> </li> <li> <p>Architecture: The workshop's architecture focuses on using Kinesis Video Stream and Rekognition to enhance driver safety by monitoring attention levels and detecting signs of drowsiness in real-time. In real deployments, an edge ML solution can handle immediate ML, and video streams are sent to AWS for verification and compliance.</p> </li> </ul>"},{"location":"industries/automotive/accelerators/kvsvc.html#workshop-environment","title":"Workshop Environment","text":"<ul> <li> <p>Cloud 9 Simulation: The workshop simulates a camera in a Cloud9 environment, sending data upstream to AWS cloud.</p> </li> <li> <p>Practical applications: A streamer is loaded onto a single-board computer, capable of transmitting video, such as a Raspberry Pi with an Intel RealSense camera.</p> </li> </ul>"},{"location":"industries/automotive/accelerators/kvsvc.html#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Set up your AWS Cloud9 environment to upload video to Amazon Kinesis Video Streams.</li> <li>Install the Amazon Kinesis Video Streams Producer SDK and run a sample app.</li> <li>Utilize the AWS management console for video analysis.</li> </ul>"},{"location":"industries/automotive/accelerators/kvsvc.html#how-to-use","title":"How to use","text":"<p>Workshop and source code link: DMS Workshop</p>"},{"location":"industries/automotive/assets/ecu2ec2.html","title":"ECU to EC2","text":""},{"location":"industries/automotive/assets/ecu2ec2.html#overview-of-a-big-loop","title":"Overview of a Big Loop","text":"<p>This asset contains instructions and code to create environmental parity between a virtual ECU (on EC2) and an actual ECU (ARM Processor Platform) It deploys AWS IoT FleetWise edge on EWAOL it guides you through the process of running the AWS IoT FleetWise Edge on the Edge Workload Abstraction and Orchestration Layer (EWAOL) using containerization and orchestration with k3s. </p> <p>Containers are great! you can easily move them between cloud and physical environments, (provided you are usibng ARM v8 cores).  Aside from containers you need an orchestrator, here we used cdk8s framework to streamline development, you can easily replace it with other orchestrators of your liking. </p> <p></p> <ul> <li>Detailed set of instructions to get started. please use Cloud9 environment in one of the supported regions.</li> <li>The CloudFormation stack is provided, which takes approximately 3 minutes.</li> <li>After stack creation, you are guided to run scripts to deploy cloud resources via the AWS Cloud Development Kit (CDK).</li> <li>It also offers an option to run AWS IoT FleetWise Edge on the build host, using k3s </li> </ul>"},{"location":"industries/automotive/assets/ecu2ec2.html#how-to-use","title":"How to use","text":"<p>Source Code: Github</p>"},{"location":"industries/automotive/assets/ewaol.html","title":"The meta-aws-ewaol Repository","text":"<p>The meta-aws-ewaol asset provides the example code and instructions for building a customized Edge Workload Abstraction and Orchestration Layer (EWAOL) distribution in the form of an Amazon Machine Image (AMI). The end result is a Yocto based AMI which can be a great help in testing your application in automotive envirnment as a virtual ECU. </p> <p></p>"},{"location":"industries/automotive/assets/ewaol.html#how-to-use","title":"How to Use","text":"<p>Source code: GitHub </p>"},{"location":"industries/automotive/reference_architectures/cmp.html","title":"Connected Mobility","text":"<p>This reference architecture outlines the key components of a connected mobility platform, addressing three core elements: in-vehicle, external infrastructure, and backend services hosted in the cloud. The adoption of a serverless architecture is a pivotal strategy, significantly reducing operational overhead for the connected mobility platform.</p> <p>Connected Mobility Platform Diagram</p> <p></p> <p>Reference architecture diagram illustrating the utilization of AWS services within your connected mobility platform.</p> <p>To build this architecture, AWS IoT FleetWise Edge Agent and AWS IoT Core are used to facilitate bidirectional data communication with the cloud. AWS IoT Greengrass serves as the host for edge components and supports machine learning (ML) at the edge.</p> <p>AWS IoT Device Management plays a critical role in onboarding vehicles and managing their lifecycles. It also enables remote operations through the AWS IoT Device Shadow service.</p> <p>Raw telemetry data, along with selected values from Amazon Timestream, is stored in an Amazon Simple Storage Service (Amazon S3) bucket. Aggregated data is maintained in Amazon DynamoDB, and the entire data lake is governed by AWS Lake Formation.</p> <p>For the exposure of service and data APIs, AWS Data Exchange and Amazon API Gateway are leveraged, catering to both internal and external requirements.</p> <p>Real-time alert generation and insights are driven by Amazon Managed Service for Apache Flink.</p> <p>The architecture supports the personalization of use cases through Amazon Rekognition and facilitates the development of custom models using Amazon SageMaker, particularly for preventive or predictive scenarios.</p> <p>Additionally, a fleet management portal is established for fleet operators, enabling real-time monitoring of vehicles, with Amazon Location Service offering key geospatial capabilities.</p> <p>Finally, the call center is empowered by Amazon Connect and Amazon Chime, while call insights can be visualized through Amazon QuickSight.</p>"},{"location":"industries/automotive/reference_architectures/cmp.html#download-the-editable-diagram","title":"Download The Editable Diagram","text":"<p>For complete customization of this reference architecture diagram, tailored to your business requirements, download the ZIP file that includes an editable PowerPoint here</p>"},{"location":"industries/eu/index.html","title":"Energy &amp; Utilities","text":"<p>AWS enables customers to transform, innovate, and accelerate the energy transition with solutions for businesses across the energy value chain, including Oil &amp; Gas, Power &amp; Utilities, Renewables, Decarbonization, and more.</p>"},{"location":"industries/eu/index.html#assets-and-accelerators","title":"Assets and Accelerators","text":"<p>As energy-specific assets and accelerators and developed and published, they will be added here with documentation, examples, and tutorials.</p>"},{"location":"industries/eu/index.html#energy-utilities-solution-library","title":"Energy &amp; Utilities Solution Library","text":"<p>Explore energy and utilities solutions in the AWS Solution Library</p>"},{"location":"industries/eu/accelerators/energy_monitoring.html","title":"Energy Monitoring","text":""},{"location":"industries/eu/accelerators/energy_monitoring.html#energy-monitoring-for-digital-twins","title":"Energy Monitoring for Digital Twins","text":"<p>Energy Monitoring for Digital Twins is an IoT accelerator for creating an energy monitoring solution using digital twins with AWS IoT. This accelerator consists of a AWS Cloud Formation template which creates and configures the AWS services required to deploy data ingestion, storage and 3D visualization components that are required for a energy monitoring solution in a digital twin.</p>"},{"location":"industries/eu/accelerators/energy_monitoring.html#prerequisites","title":"Prerequisites:","text":"<pre><code>On site (field) device which can publish the data read from energy meters to an MQTT topic in IoT Core\nAWS account with privileges to create the services used in this solution listed below.\n</code></pre>"},{"location":"industries/eu/accelerators/energy_monitoring.html#features","title":"Features:","text":"<pre><code>AWS IoT Core ingests the uplink energy meter readings from the field device via MQTT\nAmazon Timestream stores the timeseries data\nAWS IoT TwinMaker provides the capability to build 3D model and entity hierarchy of the digital twin and bind the data stored in Timestream to these entities.\nAmazon Managed Grafana provides a flexible, no-code builder for the user to build a custom dashboard.\n</code></pre>"},{"location":"industries/eu/accelerators/energy_monitoring.html#architecure","title":"Architecure","text":"<p>Click here to access the library and start designing your solution.</p>"},{"location":"industries/eu/accelerators/forecast_electricity_demand.html","title":"Forecast Electricity Demand","text":""},{"location":"industries/eu/accelerators/forecast_electricity_demand.html#forecast-electricity-demand-with-gluonts-and-sagemaker-custom-containers","title":"Forecast electricity demand with GluonTS and SageMaker custom containers","text":"<p>Electricity demand prediction accelerator that helps in training and evaluating time series models with Gluon TS library and Amazon SageMaker custom containers. This accelerator uses the datasets from UC Irvine Machine Learning Repository - Individual household electric power consumption.</p>"},{"location":"industries/eu/accelerators/forecast_electricity_demand.html#aws-services","title":"AWS Services","text":"<pre><code>Amazon SageMaker\nAWS ECR\nGluon TS\n</code></pre>"},{"location":"industries/eu/accelerators/forecast_electricity_demand.html#architecure","title":"Architecure","text":"<p>Click here to access the library and start designing your solution.</p>"},{"location":"industries/eu/assets/aws_for_energy_ebook.html","title":"AWS For Energy","text":""},{"location":"industries/eu/assets/aws_for_energy_ebook.html#aws-for-energy-ebook","title":"AWS for Energy - eBook","text":"<p>With today\u2019s energy industry rapidly changing, the need for energy companies to innovate is critical. As customer behaviors evolve, energy demand increases, and the need to decarbonize accelerates, energy organizations will need to increase innovation through the latest cloud technologies such as AI, ML, edge computing, and many others. This ebook, focused on High Performance Computing is part of an ongoing series that will dive deep into the critical cloud technologies that are helping energy companies across the value chain reinvent the industry as they build the energy systems of the future.</p> <p>Click here to access the e-Book.</p>"},{"location":"industries/eu/assets/new_energy_playbook.html","title":"New Energy Playbook","text":""},{"location":"industries/eu/assets/new_energy_playbook.html#the-new-energy-playbook-ebook","title":"The New Energy Playbook - eBook","text":"<p>The current crisis in the energy sector is a reflection of two black swan events occurring at the same time. And that\u2019s exactly why we cannot simply mitigate these extremely negative and impossibly difficult-to-predict events using strategies we\u2019ve learned from previous energy crises.</p> <p>Click here to access the e-Book.</p>"},{"location":"industries/hcls/index.html","title":"Healthcare &amp; Life Sciences","text":"<p>AWS is the trusted technology infrastructure partner to healthcare and life sciences organizations. From modernizing the pharma value chain to improving the patient, provider, and payer experience, AWS and AWS Partners are transforming the way healthcare and life sciences organizations unlock the potential of their data, drive innovation, and improve clinical efficiency across the industry.</p>"},{"location":"industries/hcls/index.html#assets-and-accelerators","title":"Assets and Accelerators","text":"<p>As health-specific assets and accelerators and developed and published, they will be added here with documentation, examples, and tutorials.</p>"},{"location":"industries/hcls/index.html#solutions-library","title":"Solutions Library","text":"<p>Explore healthcare and life sciences solutions in the AWS Solutions Library</p>"},{"location":"industries/hcls/accelerators/fhir_hl7v2.html","title":"FHIR on AWS","text":""},{"location":"industries/hcls/accelerators/fhir_hl7v2.html#implementing-fhir-works-on-aws-using-hl7v2-transform","title":"Implementing FHIR works on aws using HL7V2 Transform","text":"<p>This AWS sample project demonstrates implementation of an Integration Transform designed to extend third-party integration capabilities of FHIR Works on AWS framework.</p>"},{"location":"industries/hcls/accelerators/fhir_hl7v2.html#architecture","title":"Architecture","text":"<p>This sample architecture consists of multiple AWS serverless services. The endpoint is hosted using API Gateway backed by a Lambda function. We chose to use SQS to pass messages from transform function to Fargate container implementing HL7v2 client (sender).</p> <p>We also implemented Test HL7 Server using NLB, Fargate, and S3.</p> <p>In production environment, you would need to establish secure encrypted connection from your VPC to the network where your HL7 endpoint is hosted. For illustration purposes, we show encrypted connection from your VPC to your corporate data center using VPN tunnel. You should consult your account team for prescriptive guidance regarding establishing secure and reliable network connection between your on-premises network and VPC.</p> <p></p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"industries/hcls/accelerators/hcls_notebook.html","title":"HCLS AI/ML Workshop","text":""},{"location":"industries/hcls/accelerators/hcls_notebook.html#aws-healthcare-life-sciences-hcls-artificial-intelligencemachine-learning-aiml-immersion-days","title":"AWS Healthcare Life Sciences (HCLS) Artificial Intelligence/Machine Learning (AI/ML) Immersion Days","text":"<p>offer an opportunity for AWS customers and those who wish to learn about AWS AI/ML services via a deep, hands on workshop experience. Customers can use Immersion Days to:</p> <p>Engage in hands on workshops to learn about AI/ML services. We will work in a hands-on fashion with data scientists, machine learning engineers, developers, analysts and anyone else to familiarize the customer with our services. These workshops are hands on -- workshop participants will be provided with temporary AWS account(s) from which they will execute AI/ML workloads in a step-by-step fashion with our HCLS AI/ML Solutions Architects. Please see the Workshops section for available workshops.</p> <p>Gain a deep understanding of AWS AI/ML Services. We will discuss what our AI/ML services are, how they can be easily brought to bear on numerous workloads, and help enable the customer to approach their own business problems in the context of AI/ML. These conversations can be overviews of AWS services, or technical deep dives into specific components that to enable well-architected AI/ML applications for HCLS business.</p> <p>Understand best practices with AI/ML in the context of HCLS. We will discuss what are the best practices and procedures for using AI/ML intelligently in HCLS applications. This includes basics of training and testing, MLOps and deployment practices, software development life cycle in the context of AI/ML and many other components.</p> <p>The Immersion Day workshops may be used by in the context of AWS Instructure-Led Labs or self-paced labs. Please see here for more information.</p> <p>Click here to access the workshop content and start learning.</p>"},{"location":"industries/hcls/accelerators/multimodel-analytics-health.html","title":"Data Analysis with AWS Health","text":""},{"location":"industries/hcls/accelerators/multimodel-analytics-health.html#multi-modal-data-analysis-with-aws-health-and-ml-services","title":"Multi-Modal Data Analysis with AWS Health and ML Services","text":"<p>This Guidance demonstrates how to set up an end-to-end framework to analyze multimodal healthcare and life sciences (HCLS) data. It analyzes this data using purpose-built health care and life sciences services (such as AWS HealthOmics, AWS HealthLake, AWS HealthImaging) and machine learning (ML) and analytics services (such as Amazon SageMaker, Amazon Athena, and Amazon QuickSight). It ingests raw HCLS data formats like variant call format (VCF), Fast Healthcare Interoperability Resources (FHIR), and Digital Imaging and Communications in Medicine (DICOM), and provides a zero-extract, transform, load (ETL) architecture to customers who want to run their data analysis at scale on AWS.</p>"},{"location":"industries/hcls/accelerators/multimodel-analytics-health.html#architecture","title":"Architecture","text":"<p>The Multi-Modal Data Analysis with AWS Health and ML Services provides guidance and demonstrates how to set up an end-to-end framework to analyze multimodal healthcare and life sciences (HCLS) data.</p>"},{"location":"industries/hcls/assets/analyze_medical_notes.html","title":"Multilingual Medical Analysis","text":""},{"location":"industries/hcls/assets/analyze_medical_notes.html#amazon-translate-with-amazon-comprehend-medical","title":"Amazon Translate with Amazon Comprehend Medical","text":"<p>This project contains sample code for using Amazon Translate in conjunction with Amazon Comprehend Medical to analyze medical notes in multiple languages. This repository contains the AWS Lambda function code for using these two services together, an AWS CloudFormation custom resource for creating an Amazon Athena table to evaluate the results, and the AWS CloudFormation template to deploy it all.</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"industries/hcls/assets/medicaltranscript.html","title":"Medical Transcription","text":""},{"location":"industries/hcls/assets/medicaltranscript.html#medical-transcription-analysis-mta","title":"Medical Transcription Analysis (MTA)","text":"<p>is a simple solution that leverages the powers of Amazon Transcribe Medical and Amazon Comprehend Medical to provide medical notes transcription and comprehension. The solution opens a WebSocket between the client (browser) and Amazon Transcribe Medical. This WebSocket is used to send the audio from the client to Amazon Transcribe Medical and retrieve real time transcription which is then rendered on the UI. The transcribed results are then sent to Amazon Comprehend Medical which returns an analysis of the transcription.</p> <p>Checkout our AWS Blog here : https://aws.amazon.com/blogs/machine-learning/performing-medical-transcription-analysis-with-amazon-transcribe-medical-and-amazon-comprehend-medical/</p> <p>Click here to access the asset and source code.</p>"},{"location":"industries/media_ent/index.html","title":"Media &amp; Entertainment","text":"<p>Media and entertainment customers face industry-wide transformation, with companies reinventing how they create content, optimize media supply chains, and compete for audience attention across streaming, broadcast, and direct-to-consumer platforms. AWS aligns the most purpose-built media and entertainment capabilities of any cloud against six solution areas to help customers transform the industry: Content Production; Media Supply Chain &amp; Archive; Broadcast; Direct-to-Consumer &amp; Streaming; Data Science &amp; Analytics; and Monetization. With AWS, you can select the right tools and partners for your workloads to accelerate production launches, and see faster time to value.</p> <p>Explore Media &amp; Entertainment solutions in the AWS Solutions Library</p>"},{"location":"industries/media_ent/accelerators/livechat-sample-app.html","title":"Add live chat to your video streams with Amazon Interactive Video Service","text":"<p>This accelerator provides a sample React application and guide for integrating live chat using Amazon Interactive Video Service (Amazon IVS).</p> <p>Amazon Interactive Video Service (Amazon IVS) enables developers to build interactive experiences designed to scale alongside Amazon IVS live streams. This accelerator provides a walkthrough for deploying a sample chat app using a serverless backend to handle auth tokens and a React frontend to display video and chat. The chat room and identity services integrate with AWS tools like Lambda, CloudFormation, and Cognito. It also describes potential enhancements like automated moderation with Comprehend and connecting to a user database.</p> <p>You can deploy and customize this accelerator for your use case to integrate live chat into your products to create new and exciting interactive experiences.</p> <p>Access this accelerator here</p>"},{"location":"industries/media_ent/accelerators/medialive-channel-orchestrator.html","title":"AWS MediaLive Channel Orchestrator","text":"<p>This accelerator contains sample code to deploy a web app that can be used to simplify the management of AWS MediaLive Channels</p> <p>AWS Elemental MediaLive is a real-time video service that lets you create live outputs for broadcast and streaming delivery. You use MediaLive to transform live video content from one format and package into other formats and packages. You typically need to transform the content in order to provide a format and package that a playback device can handle. Playback devices include smartphones and set-top boxes attached to televisions.</p> <p>AWS MediaLive Channel Orchestrator allows you to deploy a sample application via a CloudFormation template to simplify management of AWS MediaLive Channels.</p> <p>Supported functionality:</p> <ul> <li>Start/Stop Channels</li> <li>Input Switching</li> <li>Motion Graphics Overlays</li> <li>Channel Status</li> <li>Output confidence monitoring</li> <li>Media Package output autodiscovery</li> <li>Input alerting</li> <li>Static Image Overlays</li> <li>Channel Scheduling</li> </ul> <p>Access this accelerator here</p>"},{"location":"industries/media_ent/accelerators/visual-narrations.html","title":"Social Media Stories and Visual Narrations with Amazon Polly and AWS Elemental MediaConvert","text":"<p>This accelerator deploys a sample stack to illustrate how you can turn articles into videos by using a fully automated editorial workflow built using AWS services</p> <p>This accelerator uses AWS CDK, Amazon Polly, AWS Elemental MediaConvert, and Amazon Comprehend to provide an overview of how you can turn articles into videos by using a fully automated editorial workflow built using AWS services.</p> <p>Read a blogpost that guides the implementation of this accelerator here and then clone the associated repository here</p>"},{"location":"industries/media_ent/assets/livestreaming-cfn.html","title":"Livestreaming CloudFormation template","text":"<p>This asset provides an explanation of how to use CloudFormation for automating live streaming and provides a sample template and documentation references for those new to CloudFormation.</p> <p>To build an end-to-end, live video streaming workflow with AWS, you typically begin by setting up each of the required AWS Media Services through the AWS Management Console. The console is a great tool to explore all the features each Media Service offers, and the settings you can enable, disable, or tweak as you put all the pieces together. However, once you set up the resources exactly as you want, you may find it tedious to use the console to replicate your setup in other Regions, or recreating a previous workflow for another event. Automating this process, and only using the console to make minor changes to resources generated by your automation, may be the best way to work.</p> <p>This asset is an article explaining how to automate live video streaming workflows on AWS using CloudFormation templates, which allow you to define and replicate Media Services like MediaLive and MediaStore across regions. It analyzes a sample CloudFormation template that sets up a basic live streaming workflow with a MediaStore container, MediaLive HLS input, and single-pipeline MediaLive channel. The template resources correspond to those you would configure in the AWS console, so by automating with CloudFormation you can avoid repetitive manual setup. </p> <p>Access this asset here</p>"},{"location":"industries/media_ent/assets/media-intelligence.html","title":"Media Intelligence Analysis in AWS","text":"<p>This asset enables you to identify specific elements in video content</p> <p>This asset is a Media Intelligence Video Analysis solution that can identify specific elements in video content using a CloudFormation template. This layer is the basis for identifying and indexing video analysis elements that in the future can be used for finding specific scenes based on a set of rules.</p> <p>As possible add-ons, customers can use this basis layer for:</p> <ul> <li>Ads Slots identification and insertion as in Smart Ad Breaks</li> <li>Digital Product Placement for branding solutions</li> <li>Media content moderation</li> <li>Media content classification</li> </ul> <p>Access the asset here</p>"},{"location":"industries/media_ent/assets/mediasearch.html","title":"MediaSearch","text":"<p>This asset makes your audio and video files searchable using Amazon Transcribe and Amazon Kendra</p> <p>This asset includes a blogpost and GitHub repository for a new open-source solution, MediaSearch, built to make your media files searchable, and consumable in search results. It uses Amazon Transcribe to convert media audio tracks to text, and Amazon Kendra to provide intelligent search. Your users can find the content they\u2019re looking for, even when it\u2019s embedded in the sound track of your audio or video files. The solution also provides an enhanced Amazon Kendra query application that lets users play the relevant section of original media files or YouTube video, directly from the search results page.</p> <p>Read a blogpost that guides the implementation of this asset here and then clone the associated repository here</p>"},{"location":"industries/mfg_ind/index.html","title":"Manufacturing and Industrial","text":"<p>Extract insights from machine data using AWS cloud technology to optimize productivity, quality, and sustainability.</p> <p>AWS helps leading manufacturers transform their operations with the most advanced set of cloud solutions, including Machine Learning (ML), IoT, Robotics, and Analytics. AWS allows you to focus your resources on optimizing production, creating new smart products, and improving operational efficiencies across the value chain, not on the infrastructure to make it happen.</p>"},{"location":"industries/mfg_ind/index.html#assets-and-accelerators","title":"Assets and Accelerators","text":"<p>As manufacturing-specific assets and accelerators and developed and published, they will be added here with documentation, examples, and tutorials.</p>"},{"location":"industries/mfg_ind/index.html#solutions-library","title":"Solutions Library","text":"<p>Explore manufacturing solutions in the AWS Solutions Library</p>"},{"location":"industries/mfg_ind/accelerators/industrial_asset_monitoring.html","title":"Industrial Asset Monitoring","text":""},{"location":"industries/mfg_ind/accelerators/industrial_asset_monitoring.html#energy-monitoring-for-digital-twins","title":"Energy Monitoring for Digital Twins","text":"<p>Energy Monitoring for Digital Twins is an IoT accelerator for creating an energy monitoring solution using digital twins with AWS IoT. This accelerator consists of a AWS Cloud Formation template which creates and configures the AWS services required to deploy data ingestion, storage and 3D visualization components that are required for a energy monitoring solution in a digital twin.</p>"},{"location":"industries/mfg_ind/accelerators/industrial_asset_monitoring.html#prerequisites","title":"Prerequisites:","text":"<pre><code>On site (field) device which can publish the data read from energy meters to an MQTT topic in IoT Core\nAWS account with privileges to create the services used in this solution listed below.\n</code></pre>"},{"location":"industries/mfg_ind/accelerators/industrial_asset_monitoring.html#features","title":"Features:","text":"<pre><code>AWS IoT Core ingests the uplink energy meter readings from the field device via MQTT\nAmazon Timestream stores the timeseries data\nAWS IoT TwinMaker provides the capability to build 3D model and entity hierarchy of the digital twin and bind the data stored in Timestream to these entities.\nAmazon Managed Grafana provides a flexible, no-code builder for the user to build a custom dashboard.\n</code></pre>"},{"location":"industries/mfg_ind/accelerators/industrial_asset_monitoring.html#architecure","title":"Architecure","text":"<p>Click here to access the library and start designing your solution.</p>"},{"location":"industries/mfg_ind/accelerators/industrial_equipment_anomaly.html","title":"Industrial Equipment Anomaly","text":""},{"location":"industries/mfg_ind/accelerators/industrial_equipment_anomaly.html#anomaly-detection-on-industrial-equipment-using-audio-signals","title":"Anomaly Detection on Industrial Equipment using Audio Signals","text":"<p>Industrial companies have been collecting a massive amount of time series data about their operating processes, manufacturing production lines, industrial equipment... They sometime store years of data in historian systems. Whereas they are looking to prevent equipment breakdown that would stop a production line, avoid catastrophic failures in a power generation facility or improving their end product quality by adjusting their process parameters, having the ability to process time series data is a challenge that modern cloud technologies are up to. In this post, we are going to focus on preventing machine breakdown from happening.</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"industries/mfg_ind/accelerators/manufacturing_defects_detection.html","title":"Manufacturing Defects Detection","text":""},{"location":"industries/mfg_ind/accelerators/manufacturing_defects_detection.html#detect-manufacturing-defects-in-real-time-using-amazon-lookout-for-vision","title":"Detect manufacturing defects in real time using Amazon Lookout for Vision","text":"<p>Amazon Lookout for Vision is a machine learning (ML) service that spots defects and anomalies in visual representations using computer vision (CV). With Amazon Lookout for Vision, manufacturing companies can increase quality and reduce operational costs by quickly identifying differences in images of objects at scale. For example, Amazon Lookout for Vision can be used to identify missing components in products, damage to vehicles or structures, irregularities in production lines, miniscule defects in silicon wafers, and other similar problems. Amazon Lookout for Vision uses ML to see and understand images from any camera as a person would, but with an even higher degree of accuracy and at a much larger scale. Amazon Lookout for Vision allows customers to eliminate the need for costly and inconsistent manual inspection, while improving quality control, defect and damage assessment, and compliance. In minutes, you can begin using Amazon Lookout for Vision to automate inspection of images and objects\u2013with no machine learning expertise required.</p>"},{"location":"industries/mfg_ind/accelerators/manufacturing_defects_detection.html#features","title":"Features:","text":"<pre><code>Image upload to S3 using API Gateway and S3 signed URL\nAnomaly Detection using Amazon Lookout For Vision\nInference results storage using Amazon DynamoDB\nNotifications using Amazon SNS\nMonitoring &amp; Alerting using Amazon CloudWatch\n</code></pre>"},{"location":"industries/mfg_ind/accelerators/manufacturing_defects_detection.html#architecure","title":"Architecure","text":"<p>Click here to access the library and start designing your solution.</p>"},{"location":"industries/mfg_ind/assets/guidance_connected_products.html","title":"Connected Products","text":""},{"location":"industries/mfg_ind/assets/guidance_connected_products.html#guidance-for-connected-products-with-simpleiot-on-aws","title":"Guidance for Connected Products with SimpleIOT on AWS","text":"<p>This Guidance helps you implement SimpleIOT, an intuitive system for creating cloud-connected Internet of Things (IoT) devices from the ground up. SimpleIOT helps you rapidly set up a fully functional serverless Internet of Things (IoT) framework in an AWS account. It supports rapid creation of complete, secure, connected devices that send and receive data to the cloud in near real-time.</p>"},{"location":"industries/mfg_ind/assets/guidance_connected_products.html#architecure","title":"Architecure","text":"<p>Click here to access the library and start designing your solution.</p>"},{"location":"industries/mfg_ind/assets/ot_assets_data_ingestion.html","title":"OT Assets Data Ingestion","text":""},{"location":"industries/mfg_ind/assets/ot_assets_data_ingestion.html#guidance-for-connecting-on-premises-operational-technology-assets-to-aws-cloud-for-robust-data-ingestion","title":"Guidance for Connecting On-Premises Operational Technology Assets to AWS Cloud for Robust Data Ingestion","text":"<p>This AWS Solution provides operational technology (OT) managers with secure machine and industrial equipment connectivity to the AWS Cloud. This solution automatically deploys and configures AWS IoT Greengrass and provides integration with AWS IoT SiteWise. You can then publish machine and industrial asset telemetry data to AWS IoT SiteWise and Amazon Simple Storage Service (Amazon S3), populating an industrial data lake with machine telemetry to support insights through visualization, analytics, and machine learning. This solution supports both the OPC Data Access (OPC-DA) and OPC Unified Architecture (OPC-UA) protocols, and supports streaming time series data through the OSIsoft PI Web API.</p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"industries/mfg_ind/assets/scale_out_computing.html","title":"Scale Out Computing","text":""},{"location":"industries/mfg_ind/assets/scale_out_computing.html#multiuser-environment-for-computationally-intensive-workflows-such-as-computer-aided-engineering-cae","title":"multiuser environment for computationally intensive workflows, such as Computer-Aided Engineering (CAE)","text":"<p>The Scale-Out Computing on AWS solution helps customers deploy and operate a multiuser environment for computationally intensive workflows, such as computer-aided engineering (CAE). The solution features a large selection of compute resources, a fast network backbone, unlimited storage, and budget and cost management directly integrated within AWS.  </p> <p>Click here to access the library and start designing your solution.</p>"},{"location":"industries/mfg_ind/reference_architectures/industrial_predictive_maintenance.html","title":"Industrial Predictive Maintenance","text":""},{"location":"industries/mfg_ind/reference_architectures/industrial_predictive_maintenance.html#anomaly-detection-for-industrial-workloads","title":"Anomaly detection for industrial workloads","text":"<p>This reference architecture provides the design for creating a Predictive Maintenance (PdM) Machine Learning (ML) model using Amazon SageMaker with AWS IoT Core and an anomaly detection application using Amazon Kinesis Data Analytics.</p>"},{"location":"industries/mfg_ind/reference_architectures/industrial_predictive_maintenance.html#architecure","title":"Architecure","text":"<ol> <li> <p>Configure AWS IoT Greengrass using Greengrass Connectors to communicate with factory machines.</p> </li> <li> <p>Configure rules within AWS IoT Core to trigger events based on MQTT topics for the factory machines.</p> </li> <li> <p>Create an Amazon Kinesis Data Firehouse delivery stream to store the factory machines data in the data lake on Amazon Simple Storage Service (Amazon S3).</p> </li> <li> <p>Build your Predictive Maintenance (PdM) Machine Learning (ML) model with Amazon SageMaker.</p> </li> <li> <p>Deploy your Machine Learning model onto your AWS IoT Greengrass Edge Gateway.</p> </li> <li> <p>Build your data queries in Amazon Athena against your AWS Glue Data Catalog of the data lake on Amazon S3.</p> </li> <li> <p>Visualize your analysis using Amazon QuickSight on the Amazon Athena data source.</p> </li> <li> <p>Create an anomaly detection application in Amazon Kinesis Data Analytics.</p> </li> <li> <p>Configure AWS Lambda as an output of Amazon Kinesis Data Analytics application to send anomaly detections notifications to an Amazon Simple Notification Service (Amazon SNS) topic.</p> </li> </ol>"},{"location":"industries/mfg_ind/reference_architectures/industrial_workloads_anomaly_detection.html","title":"Industrial Workloads Anomaly","text":""},{"location":"industries/mfg_ind/reference_architectures/industrial_workloads_anomaly_detection.html#anomaly-detection-for-industrial-workloads","title":"Anomaly detection for industrial workloads","text":"<p>This reference architecture uses IoT, analytics, and machine learning services to design a solution that can inform operational technology teams of performance anomalies.</p>"},{"location":"industries/mfg_ind/reference_architectures/industrial_workloads_anomaly_detection.html#architecure","title":"Architecure","text":"<ol> <li> <p>Telemetry from industrial assets is ingested by connectors in an AWS IoT Greengrass edge solution. Hot anomaly detection originates at the edge with stream analytics and machine learning inference.</p> </li> <li> <p>Edge-to-cloud interfaces AWS IoT Core and AWS IoT SiteWise ingest telemetry. </p> </li> <li> <p>Amazon Kinesis Data Analytics runs queries to determine anomalistic behavior in datasets on the warm path.</p> </li> <li> <p>An Amazon Simple Storage Service (S3) data lake architecture stores raw and processed device telemetry, trained machine learning (ML) models, and ML inference results.</p> </li> <li> <p>ML models are trained and used for batch inference on the cold anomaly detection path with Amazon SageMaker, or any of the application-level AI services such as Amazon Lookout for Equipment.</p> </li> <li> <p>Code running in AWS Lambda functions analyzes the results of batch inference produced by ML models. </p> </li> <li> <p>Operational technology teams consume alerts from SNS as emails, text messages, or integration into ticketing systems.</p> </li> <li> <p>No-code dashboards assess real-time and historical machine performance.</p> </li> <li> <p>Amazon QuickSight to evaluates history of anomalies, fleet performance, and other scaled analysis.</p> </li> </ol>"},{"location":"industries/mfg_ind/reference_architectures/manufacturing_on_aws.html","title":"Manufacturing On AWS","text":""},{"location":"industries/mfg_ind/reference_architectures/manufacturing_on_aws.html#architectural-blueprints-for-manufacturing-on-aws","title":"Architectural Blueprints for Manufacturing on AWS","text":"<p>This architectural blueprint shows how the AWS Cloud can enable digital transformation for manufacturers, and highlights how a smart factory, smart products, enterprise applications, and engineering and design workloads can be integrated using a data lake.</p>"},{"location":"industries/mfg_ind/reference_architectures/manufacturing_on_aws.html#blueprints","title":"Blueprints","text":"<ol> <li>Data Ingestion</li> <li>Data Lake and Analytics</li> <li>Applicaion Hosting</li> <li>Smart Products</li> </ol> <p>Click here for the architecture diagram and the data flow details for each Blueprint.</p>"},{"location":"industries/telecom/index.html","title":"Telecommunications","text":"<p>AWS empowers IT and business leaders to transform infrastructure and processes, strengthen security posture, and integrate advanced technologies. Use our robust, mature, proven platform to digitize industries, transform telcos, and reimagine the consumer experience.</p>"},{"location":"industries/telecom/index.html#assets-and-accelerators","title":"Assets and Accelerators","text":"<p>As telecommunications-specific assets and accelerators and developed and published, they will be added here with documentation, examples, and tutorials.</p>"},{"location":"industries/telecom/index.html#solutions-library","title":"Solutions Library","text":"<p>Explore telecommunications solutions in the AWS Solutions Library</p>"},{"location":"industries/telecom/assets/5g-edge-discovery.html","title":"Deploying dynamic 5G Edge Discovery architectures with AWS Wavelength","text":"<p>With the rapid expansion of AWS edge computing infrastructure, developers face new challenges in deploying applications to optimal locations and discovering the best endpoints to service requests based on factors like latency and network topology. Effective solutions involve geo-distributed, automated deployments across multiple zones and granular, accurate discovery methods beyond DNS that consider the requesting device's precise location within the network rather than just resolver IP.</p> <p>To address this developer feedback, this asset (in the form of a blogpost) positions a reference pattern using Telecommunication provider-developed Edge Discovery Service (EDS) APIs and how they can be directly integrated into powerful event-driven architectures. As an example, this post demonstrates how to use the Verizon Edge Discovery Service to provide a dynamic workflow for mobile clients in highly distributed edge compute environments.</p> <p></p> <p>Access the blogpost here</p>"},{"location":"industries/telecom/assets/telco-workload.html","title":"Telco Workload Placement and Configuration Updates with Lambda and inotify","text":"<p>With users and applications demanding high bandwidth along with low latencies, mobile service providers and application developers are working hard to meet demanding performance requirements. To deliver high bandwidth with low latencies, operators often need to deploy mobile Container Network Functions (CNFs) to deliver network connectivity services closer to the traffic location. Since a CNF needs a Kubernetes cluster in which it is deployed, this results in having multiple Kubernetes clusters in a Communication Service Providers (CSP) network.</p> <p>This asset shows one way to implement an agile workload placement method that provides a simple abstraction to business applications by using AWS container and serverless constructs. The use of serverless technologies for this task provides an inherent resiliency in this solution implementation. These constructs are flexible and can be adopted to meet CSPs and network function providers automation requirements. This approach is straightforward yet extendible.</p> <p></p> <p>Read the blogpost here and then clone the associated repository here to start leveraging this asset.</p>"},{"location":"reference_architectures/index.html","title":"Reference Architecture","text":"<p>Reference architecture is a holistic set of guidelines for building and deploying SBE solutions using AWS services. These are used to help design, implement, and manage SBE solutions that align with AWS recommended practices.</p> <p>As reference architectures are added to the SBE Design &amp; Build Hub, they will appear here with documentation, examples, and tutorials. This section will contain general reference architectures, applicable across a variety of verticals, and additional industry-specific reference architecture will be available within the Industries section of this site.</p>"},{"location":"reference_architectures/data_lake_renewable_energy.html","title":"Data Lake Architecture for Renewable Energy","text":"<p>This architecture enables you to build a renewable energy data lake that includes telemetry data from IoT devices, and business application data for near real-time monitoring. It also enables you to visualize data and make predictions with machine learning (ML) models.</p>"},{"location":"reference_architectures/data_lake_renewable_energy.html#data-lake-architecture-for-renewable-energy-diagram","title":"Data Lake Architecture for Renewable Energy Diagram","text":"<ol> <li> <p>Renewable energy data is ingested into AWS IoT Core with MQ Telemetry Transport (MQTT) protocol.</p> </li> <li> <p>Using AWS IoT rules engine within AWS IoT Core, telemetry data is routed to Amazon Timestream and Amazon Simple Storage Service (Amazon S3) through Amazon Kinesis Data Streams. Use Amazon Managed Service for Apache Flink to transform and analyze streaming data in near real-time.</p> </li> <li> <p>The schemas for the on-premise databases are discovered and converted by the AWS Schema Conversion Tool (AWS SCT). The data is moved by AWS Database Migration Service (AWS DMS) to Amazon S3 and Amazon Redshift.</p> </li> <li> <p>Data stored in Amazon S3 is crawled by AWS Glue crawler. The schemas are discovered and the AWS Glue Data Catalog is populated with this metadata.</p> </li> <li> <p>AWS Glue extract, transform, load (ETL) jobs process, transform, and enrich the raw data, and output it in an Amazon S3 processed bucket.</p> </li> <li> <p>Schemas and tables are then created in Amazon Redshift. Using the COPY command, data is loaded into Amazon Redshift tables. Business logic data transformations can then be performed by stored procedures.</p> </li> <li> <p>The AWS Glue Data Catalog, AWS Lake Formation, and AWS Identity and Access Management (IAM) are used to provide centralized security and governance.</p> </li> <li> <p>Amazon Athena, Amazon QuickSight, and Amazon Managed Grafana visualize data and build dashboards and reporting.</p> </li> <li> <p>Use raw datasets with Amazon SageMaker to train and deploy machine learning models.</p> </li> </ol>"},{"location":"reference_architectures/hcls.html","title":"Healthcare &amp; Life Sciences Reference Architecture","text":"<p>As health-specific reference architectures are selected and developed for SBE, they will be added here.</p> <p>Explore reference architecture diagrams for healthcare and life sciences solutions</p>"},{"location":"reference_architectures/manufacturing.html","title":"Manufacturing &amp; Industrial Reference Architecture","text":"<p>As manufacturing-specific reference architectures are selected and developed for SBE, they will be added here.</p> <p>Explore reference architecture diagrams for manufacturing solutions</p>"},{"location":"reference_architectures/oil_and_gas_ml_predictive_maintenence.html","title":"Machine Learning Enabled Predictive Maintenance","text":"<p>Sucker rod pumping systems are the most widely applied artificial lift equipment in the oil and gas industry. Condition- based maintenance with near real-time inference results and notifications are necessary for the best maintenance of applications and equipment.</p>"},{"location":"reference_architectures/oil_and_gas_ml_predictive_maintenence.html#predictive-maintenance-architecture-diagram","title":"Predictive Maintenance Architecture Diagram","text":"<ol> <li> <p>Use the MQTT protocol in an AWS IoT device SDK to ingest data to AWS IoT Core from sucker rod pumps.</p> </li> <li> <p>Configure an IoT rule in AWS IoT Core to store data in Amazon Timestream as time series data.</p> </li> <li> <p>Visualize and monitor sensor data by using Amazon Managed Grafana in a near real time interactive dashboard from an Amazon Timestream database.</p> </li> <li> <p>Configure an IoT rule in AWS IoT Core to capture, transform, and deliver data to Amazon Simple Storage Service (Amazon S3) using Amazon Kinesis Data Firehose.</p> </li> <li> <p>Amazon Lookout for Equipment analyzes data from Amazon S3 and automatically trains a unique machine learning (ML) model to detect equipment abnormalities in near real-time without managing infrastructure. The inference results are stored in Amazon S3.</p> </li> <li> <p>Use AWS Glue Data Catalog to categorize the data. The metadata catalog is integrated by Amazon Athena to perform one-time or on-demand queries on the data.</p> </li> <li> <p>Display inference results from Amazon Athena by using Amazon Managed Grafana.</p> </li> </ol>"},{"location":"reference_architectures/eu/index.html","title":"Energy &amp; Utilities Reference Architecture","text":"<p>Together, we are accelerating the energy transition through practical innovations for today and tomorrow that deliver energy efficiently, reliably, sustainably, and responsibly. AWS brings the most advanced and secure cloud services and deep industry expertise across energy, utilities, and sustainable energy sectors. With the broadest energy partner ecosystem, AWS empowers energy leaders to improve performance, accelerate innovation, transform the customer experience, maximize safety and security, and minimize their carbon footprint.</p> <p>As energy-specific reference architectures are selected and developed for SBE, they will be added here.</p>"},{"location":"reference_architectures/media_ent/media-cdk-ref-arch.html","title":"AWS CDK MediaServices Reference Architectures","text":"<p>AWS CDK MediaServices Reference Architectures is a GitHub repository of reference architecture examples designed to provide a broader understanding of the media services as well as how to implement them to build media supply chain, media infrastructure and media application workflow.</p> <p>This project will provide you with deployment samples for the following AWS services:</p> <ul> <li>AWS Elemental Connect (EMX)</li> <li>AWS Elemental MediaLive (EML)</li> <li>AWS Elemental MediaLive Statmux (EML-STX)</li> <li>AWS Elemental MediaLink (EMK)</li> <li>AWS Elemental MediaConvert (EMC)</li> <li>AWS Elemental MediaPackage (EMP)</li> <li>AWS Elemental MediaStore (EMS)</li> <li>AWS Elemental MediaTailor (EMT)</li> <li>AWS Elemental Channel Assembly (ECA)</li> <li>AWS Elemental Secure Packager and Encoder Key Exchange (SPEKE)</li> <li>Amazon Interactive Video Service (IVS)</li> </ul> <p>Some examples will also demonstrate the integration of MediaServices with other AWS services through detailed use cases.</p> <p>Access the reference architectures here</p>"},{"location":"reference_architectures/telecom/index.html","title":"Telecommunications Reference Architecture","text":"<p>As telecommunications-specific reference architectures are selected and developed for SBE, they will be added here.</p> <p>Explore reference architecture diagrams for telecommunications solutions</p>"},{"location":"reference_architectures/telecom/deploying-5g-core.html","title":"Deploying 5G Core on AWS: Distribute your 5G Core to on-premises data centers","text":"<p>This reference architecture explains how 5G Core can be distributed between on-premises data centers and AWS Regions.</p> <p></p>"},{"location":"reference_architectures/telecom/digital-messaging.html","title":"Enable Digital Messaging Channels in Amazon Connect","text":"<p>Add support for digital messaging channels (like Facebook Messenger and Slack) to your contact center using Amazon Connect Chat\u2019s Message Streaming APIs and Serverless Services on AWS.</p> <p></p>"},{"location":"reference_architectures/telecom/two-way-sms.html","title":"Enable two-way SMS as a customer service channel in Amazon Connect","text":"<p>Add support for SMS to your contact center using Amazon Connect Chat\u2019s Message Streaming APIs and Amazon Pinpoint two-way SMS functionality.</p> <p></p>"},{"location":"tools/abp.html","title":"AWS Builder Platform","text":"<p>The AWS Builder Platform is a self-service API and UI that provides code repositories with built-in automation for best practice and security validation. The Platform surfaces machine-driven feedback to the builder at each step of the process via a language agnostic, extensible code analysis framework. Tools and automated test ensure that security and coding standards are continuously maintained as the builder creates, iterates, and makes the asset available on AWS. ABP allows AWS teams and partner builders to collaborate in a context that is familiar and consistent to both. The self-service nature of the Builder Platforn ensures that Partners and AWS Tech Field Teams (SAs, TAMs, etc) can spend their time engaging on high-value activities such as architectural design rather than editing code and performing manual processes to move assets through the publication pipeline.</p> <p>Tools and solutions managed via ABP are available in this GitHub repo but partners may build and manage solutions via ABP without publishing to a public repo.</p>"},{"location":"tools/abp.html#key-concepts","title":"Key Concepts","text":""},{"location":"tools/abp.html#project","title":"Project","text":"<p>A project is any solution that SBG supports partners in building.</p>"},{"location":"tools/abp.html#project-type","title":"Project Type","text":"<p>A project type is a template and set of automated steps used for testing, provisioning, and publishing a particular category of projects (in the case of SBG, the category will be SBG plus the language family of the solution - CloudFormation, Terraform, Python, etc.)</p>"},{"location":"tools/abp.html#projects-vs-project-types","title":"Projects vs. Project Types","text":"<p>A project type is analogous to a class, while a project is analogous to an instance of a class</p> <p>Each project type belongs to a language family - such as CFN, Terraform, Python - plus SBG-specific provisioning and testing definitions.</p> <p></p> <p>Because of this inheritance structure, an individual project instance will inherit all testing, provisioning, and publishing definitions contained in its parent project type - i.e., every time you create a project that belongs to a specific project type, those definitions will apply to your individual project instance. As a result, you don\u2019t have to waste time manually writing the same tests for every single solution you build.</p> <p></p>"},{"location":"tools/abp.html#project-type-specification","title":"Project Type Specification","text":"<p>The specification defines the criteria that must be met in order to consider a solution validated. SBG has standard definitions for testing, provisioning, and publication for all of its project types, but these may be modified for a given solution to accommodate the use case and objective. For example, a partner may customize the definitions for:</p> <ul> <li>Static tests: Simple data validations (e.g., if it\u2019s a CloudFormation project, it might run a linter to validate syntax or check that the security groups are not open to public)</li> <li>Functional tests: Create resources in an AWS account to ensure the functionality of the solution.</li> <li>Publication: Successful validation of all automated tests defined in the project type. However, a 'published' solution is not necessarily publicly available; in some cases, solutions are  published to AWS Marketplace or Terraform Hashicorp Registry, while in other cases they will remain private.</li> </ul>"},{"location":"tools/abp.html#proposing-a-solution","title":"Proposing a solution","text":"<p>Partners can contact their designated Partner Development Manager or Partner Development Specialist to discuss and propose a SBG solution. The PDx will then submit the proposal to AWS Builder Platform.</p> <p>Once the solution is approved, they will work with the SBG team on contributing to and tracking the solution.</p>"},{"location":"tools/abp.html#contributing-to-a-solution","title":"Contributing to a solution","text":"<p>Once the solution is approved in ABP:</p> <ol> <li>I&amp;A will automatically provision a GitHub repo</li> <li>Contributors (partner builders and/or AWS builders) will fork and add their code to the forked repo</li> <li>Builders will then submit a PR to merge it with the main branch</li> <li>When they submit a PR, the automated tests are triggered</li> <li>Once all tests pass and 2 reviewers approve the PR, it gets merged into the main branch</li> </ol> <p></p>"},{"location":"tools/abp.html#tracking-a-solution","title":"Tracking a solution","text":"<p>Additionally, when a solution is approved in ABP, a template for solution tracking will be generated in GitHub. The template, called a GitHub Project is attached to the GitHub repo created by ABP.</p> <p>The Project contains a set of Issues, which you can see in the <code>Todo</code> column in the example below:</p> <p></p> <p>Issues represent steps in the SBG Build Hub and the Project aggregates all of those Issues. This allows everyone (no matter what point they become involved in the solution) to understand what activities are required to validate the solution and how much progress has been made thus far. In addition, all artifacts and resources are consolidated in the project's GitHub repo.</p>"},{"location":"tools/partner-solutions.html","title":"AWS Partner Solutions","text":"<p>Partner Solutions (formerly known as Quick Starts) are automated reference deployments built by Amazon Web Services (AWS) solutions architects and AWS Partners. Partner Solutions help you deploy popular technologies to AWS according to AWS best practices. You can reduce hundreds of manual procedures to a few steps and start using your environment within minutes.</p> <p>Explore our catalog of AWS Partner Solutions here. </p>"},{"location":"tools/taskcat.html","title":"TaskCat","text":""},{"location":"tools/taskcat.html#what-is-taskcat","title":"What is TaskCat?","text":"<p>TaskCat is a tool that tests AWS CloudFormation templates. It deploys your AWS CloudFormation template in multiple AWS Regions and generates a report with a pass/fail grade for each region. You can specify the regions and number of Availability Zones you want to include in the test, and pass in parameter values from your AWS CloudFormation template. TaskCat is implemented as a Python class that you import, instantiate, and run.</p> <p>TaskCat was developed by the aws-ia team to test AWS CloudFormation templates that automatically deploy workloads on AWS. We\u2019re pleased to make the tool available to all developers who want to validate their custom AWS CloudFormation templates across AWS Regions</p> <p>See TaskCat documentation.</p>"},{"location":"tools/taskcat.html#support","title":"Support","text":""},{"location":"tools/taskcat.html#github","title":"GitHub","text":""},{"location":"tools/taskcat.html#pypi","title":"PyPi","text":""},{"location":"tools/quickstart/index.html","title":"AWS Quick Starts","text":"<p>AWS Quick Starts are automated reference deployments built by Amazon Web Services (AWS) solutions architects and AWS Partners. By using best practices and automating hundreds of manual procedures, Quick Starts can help you deploy popular technologies to AWS in minutes.</p>"},{"location":"tools/quickstart/cloudbeesCI.html","title":"cloudbeesCI","text":""},{"location":"tools/quickstart/cloudbeesCI.html#continous-integration-using-cloudbees-a-ci-solution-based-on-jenkins-and-integrated-with-amazon-eks","title":"Continous Integration using Cloudbees -  A CI solution based on Jenkins and integrated with Amazon EKS","text":"<p>This Partner Solution deploys CloudBees CI on the Amazon Web Services (AWS) Cloud to provide continuous integration (CI) that's based on Jenkins and integrated with Amazon Elastic Kubernetes Service (Amazon EKS).</p> <p>The Partner Solution sets up a CI environment that meets architectural best practices, including high availability, automatic scaling, segregation of agent workloads, and Kubernetes-native integration with Amazon Elastic Compute Cloud (Amazon EC2) Spot Instances.</p> <p>The Partner Solution is automated by AWS CloudFormation templates that deploy CloudBees CI in about 45 minutes in your AWS account. You can choose to install CloudBees CI into a new virtual private cloud (VPC) or into your existing VPC. After you deploy the Partner Solution, you can use Jenkins plugins to add functionality to your CI environment.</p> <p>This reference deployment uses the Amazon EKS Architecture Partner Solution as a foundation to provide a fully managed, highly available, and certified Kubernetes-conformant control plane for CloudBees CI.</p>"},{"location":"tools/quickstart/cloudbeesCI.html#architecture","title":"Architecture","text":"<p>The CloudBees CI on the AWS allows you to build CI using Jenkins and Amazon EKS.</p>"},{"location":"tools/quickstart/paloaltoonaws.html","title":"Paloaltoonaws","text":""},{"location":"tools/quickstart/paloaltoonaws.html#palo-alto-networks-vm-series-on-aws","title":"Palo Alto Networks VM-Series on AWS","text":"<p>This Terraform module deploys Palo Alto Networks VM-Series to the Amazon Web Services (AWS) Cloud.</p>"},{"location":"tools/quickstart/paloaltoonaws.html#architecture","title":"Architecture","text":"<p>The Palo-Alto on AWS terraform allows you to deploy Palo Alto VM on AWS using Terraform template.</p>"},{"location":"tools/quickstart/rdsonaws.html","title":"Rdsonaws","text":""},{"location":"tools/quickstart/rdsonaws.html#amazon-rds-on-aws","title":"Amazon RDS on AWS","text":"<p>This solution deploys Amazon Relational Database Service (Amazon RDS) for PostgreSQL, MySQL, or Microsoft SQL Server in the Amazon Web Services (AWS) Cloud. The solution deploys an Amazon RDS cluster to a private subnet with an Amazon RDS read replica in one Availability Zone and a write replica in another zone. Native SQL Server replication mechanisms synchronize the read replica with the source database. The deployment also includes access to Amazon CloudWatch metrics and logs, event notifications, and data encryption.</p>"},{"location":"tools/quickstart/rdsonaws.html#architecture","title":"Architecture","text":"<p>The Amazon RDS on AWS allows you to Operate, manage, and scale your Amazon RDS databases in the cloud</p>"},{"location":"tools/quickstart/solaceonaws.html","title":"Solaceonaws","text":""},{"location":"tools/quickstart/solaceonaws.html#solace-on-aws","title":"Solace on AWS","text":"<p>This Partner Solution deploys Solace PubSub+ to the Amazon Web Services (AWS) Cloud. Solace PubSub+ is a message broker that lets you establish event-driven interactions between applications and microservices regardless of location.</p> <p>This Partner Solution uses AWS CloudFormation templates to deploy Solace PubSub+ message brokers in high availability (HA) redundancy groups for fault tolerance. You can use this Partner Solution to build a new virtual private cloud (VPC) or deploy Solace PubSub+ message brokers into your existing VPC.</p>"},{"location":"tools/quickstart/solaceonaws.html#architecture","title":"Architecture","text":"<p>The Solace on AWS allows you to deploy this Partner Solution with default parameters builds the following PubSub+ event broker environment in the AWS Cloud.</p>"},{"location":"war/index.html","title":"Well Architected Lens","text":"<p>AWS Well-Architected Pillars help cloud architects build secure, high-performing, resilient, and efficient infrastructure for a variety of applications and workloads.</p> <p>The AWS Well-Architected Tool makes it easy to create Custom Lenses by providing a json template that you can use. The template outlines how the lens content must be defined, and it controls how the lens is presented within the AWS WA Tool.</p> <p>AWS Well-Architected Lenses extend the guidance offered by AWS Well-Architected to specific industry and technology domains, such as machine learning (ML), data analytics, serverless, high performance computing (HPC), IoT, SAP, streaming media, the games industry, hybrid networking, and financial services. To fully evaluate workloads, use applicable lenses together with the AWS Well-Architected Framework and its six pillars.</p>"}]}